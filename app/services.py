import os
import json
import re
import requests
import wikipedia

from datetime import datetime
from bs4 import BeautifulSoup
from langchain_ollama.llms import OllamaLLM
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain.prompts import PromptTemplate
from langchain_community.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

def get_ollama_models(ollama_base_url="http://localhost:11434"):
    try:
        response = requests.get(f"{ollama_base_url}/api/tags")
        response.raise_for_status()
        models_data = response.json().get("models", [])
        return [model["name"] for model in models_data]
    except requests.exceptions.ConnectionError:
        print(f"âŒ éŒ¯èª¤ï¼šç„¡æ³•é€£æ¥åˆ° Ollama æœå‹™ ({ollama_base_url})ã€‚è«‹ç¢ºèª Ollama æ­£åœ¨é‹è¡Œã€‚")
        return []
    except Exception as e:
        print(f"âŒ ç²å– Ollama æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return []

class ConversationalRAG:
    def __init__(self, persist_directory, embedding_model_name, llm_model, ollama_base_url, 
                 use_wikipedia=True, use_history=True, use_scraper=True, 
                 history_summary_threshold=2000):
        self.persist_directory = persist_directory
        self.use_wikipedia = use_wikipedia
        self.use_history = use_history
        self.use_scraper = use_scraper
        self.ollama_base_url = ollama_base_url
        self.history_summary_threshold = history_summary_threshold
        
        print("æ­£åœ¨åˆå§‹åŒ– Embedding æ¨¡å‹...")
        self.embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name, model_kwargs={'device': 'cpu'})

        print("æ­£åœ¨åˆå§‹åŒ–/è¼‰å…¥å‘é‡è³‡æ–™åº«...")
        if not os.path.exists(self.persist_directory):
            print("æ‰¾ä¸åˆ°ç¾æœ‰è³‡æ–™åº«ï¼Œå°‡å‰µå»ºä¸€å€‹æ–°çš„ã€‚")
            dummy_doc = Document(page_content="start", metadata={"source": "initialization"})
            self.vector_db = Chroma.from_documents([dummy_doc], self.embeddings, persist_directory=self.persist_directory)
        else:
            print("æ‰¾åˆ°ç¾æœ‰è³‡æ–™åº«ï¼Œæ­£åœ¨è¼‰å…¥...")
            self.vector_db = Chroma(persist_directory=self.persist_directory, embedding_function=self.embeddings)
        
        self.llm = None
        self.current_llm_model = None
        self.set_llm_model(llm_model)

        self.retriever = self.vector_db.as_retriever(search_kwargs={'k': 3})

        self.main_prompt = PromptTemplate(
            template="""ä½ æ˜¯ä¸€ä½é ‚ç´šçš„ AI æŠ€è¡“æ–‡ä»¶åˆ†æå¸«ã€‚ä½ çš„ä»»å‹™æ˜¯åš´æ ¼åŸºæ–¼ä¸‹é¢æä¾›çš„ã€Œä¸Šä¸‹æ–‡è³‡æ–™ã€ï¼Œæ·±å…¥åœ°å›ç­”ã€Œä½¿ç”¨è€…ç•¶å‰å•é¡Œã€ã€‚

**åŸ·è¡Œæµç¨‹èˆ‡è¦å‰‡ï¼š**

1.  **æ·±å…¥åˆ†æä¸Šä¸‹æ–‡**ï¼šé¦–å…ˆï¼Œä»”ç´°é–±è®€ä¸¦å®Œå…¨ç†è§£ã€Œä¸Šä¸‹æ–‡è³‡æ–™ã€ä¸­èˆ‡ã€Œä½¿ç”¨è€…ç•¶å‰å•é¡Œã€ç›¸é—œçš„æ‰€æœ‰ç‰‡æ®µã€‚
2.  **çµ„ç¹”èˆ‡å›ç­”**ï¼š
    *   **å¦‚æœã€Œä¸Šä¸‹æ–‡è³‡æ–™ã€ä¸­åŒ…å«å›ç­”å•é¡Œæ‰€éœ€çš„è³‡è¨Š**ï¼Œä½ çš„ä»»å‹™æ˜¯ã€æˆæ¬Šä¸¦é¼“å‹µã€‘ä½ ä½¿ç”¨è‡ªå·±çš„èªè¨€èƒ½åŠ›ï¼Œå°é€™äº›ç¢ç‰‡åŒ–çš„è³‡è¨Šé€²è¡Œ**ç¸½çµã€æ¨ç†ã€å’Œé‡æ–°çµ„ç¹”**ï¼Œä»¥å½¢æˆä¸€å€‹é€£è²«ã€æ¸…æ™°ã€å°ˆæ¥­çš„å›ç­”ã€‚ä½ çš„å›ç­”æ‡‰è©²çœ‹èµ·ä¾†åƒæ˜¯è©²é ˜åŸŸå°ˆå®¶å¯«çš„ï¼Œè€Œä¸åƒ…åƒ…æ˜¯åŸæ–‡çš„è¤‡è£½ã€‚
    *   **å¦‚æœã€Œä¸Šä¸‹æ–‡è³‡æ–™ã€å®Œå…¨æ²’æœ‰æåŠå•é¡Œçš„æ ¸å¿ƒä¸»é¡Œ**ï¼Œé‚£éº¼ä½ ã€å¿…é ˆã€‘åªå›ç­”ï¼šã€Œæ ¹æ“šæˆ‘æ‰€æŒæ¡çš„è³‡æ–™ï¼Œæˆ‘æ‰¾ä¸åˆ°é—œæ–¼ '{question}' çš„ç¢ºåˆ‡è³‡è¨Šã€‚ã€
3.  **å®šç¾©ã€Œå¹»è¦ºã€ç¦å€**ï¼šä½ ã€çµ•å°ç¦æ­¢ã€‘å¼•å…¥ä»»ä½•**åœ¨ã€Œä¸Šä¸‹æ–‡è³‡æ–™ã€ä¸­å®Œå…¨ä¸å­˜åœ¨çš„ã€æ†‘ç©ºæé€ çš„äº‹å¯¦æˆ–æ•¸æ“š**ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä¸Šä¸‹æ–‡æ²’æœ‰æåˆ°ç‰ˆæœ¬è™Ÿï¼Œä½ å°±ä¸èƒ½è‡ªå·±ç·¨ä¸€å€‹ç‰ˆæœ¬è™Ÿã€‚ä½ çš„æ‰€æœ‰æ ¸å¿ƒè«–é»éƒ½å¿…é ˆæºæ–¼ä¸Šä¸‹æ–‡ã€‚
4.  **æ€è€ƒéç¨‹**ï¼šåœ¨æœ€çµ‚ç­”æ¡ˆå‰ï¼Œä½ å¯ä»¥ä½¿ç”¨ <think>...</think> æ¨™ç±¤ä¾†å¯«ä¸‹ä½ çš„åˆ†æã€æ¨ç†å’Œåˆ¤æ–·éç¨‹ã€‚

---
[ä¸Šä¸‹æ–‡è³‡æ–™]:
{context}
---

[ä½¿ç”¨è€…ç•¶å‰å•é¡Œ]: {question}

ä½ çš„å›ç­”:""",
            input_variables=["context", "question"]
        )
        
        self.summarizer_prompt = PromptTemplate(
            template="è«‹å°‡ä»¥ä¸‹æä¾›çš„æ–‡å­—å…§å®¹ç¸½çµæˆä¸€æ®µç°¡æ½”ã€æµæš¢çš„æ‘˜è¦ï¼Œä¿ç•™å…¶æ ¸å¿ƒè³‡è¨Šã€‚æ–‡å­—å…§å®¹å¦‚ä¸‹ï¼š\n\n---\n{text_to_summarize}\n---\n\næ‘˜è¦:",
            input_variables=["text_to_summarize"]
        )

        self.query_expansion_prompt = PromptTemplate(
            template="""ä½ æ˜¯ä¸€å€‹æŸ¥è©¢å„ªåŒ–åŠ©ç†ã€‚è«‹æ ¹æ“šä½¿ç”¨è€…æä¾›çš„åŸå§‹æŸ¥è©¢ï¼Œç”Ÿæˆä¸€å€‹æˆ–å¤šå€‹æ›´å…·é«”ã€æ›´å¯èƒ½åœ¨æŠ€è¡“æ–‡ä»¶ä¸­æ‰¾åˆ°ç›¸é—œå…§å®¹çš„æ“´å……æŸ¥è©¢ã€‚è«‹åªè¿”å›æ“´å……å¾Œçš„æŸ¥è©¢ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡‹ã€‚

[åŸå§‹æŸ¥è©¢]: {original_query}

[æ“´å……æŸ¥è©¢]:""",
            input_variables=["original_query"]
        )

    def set_llm_model(self, model_name: str):
        print(f"\nğŸ”„ æ­£åœ¨åˆ‡æ› LLM æ¨¡å‹è‡³: {model_name}")
        try:
            self.llm = OllamaLLM(model=model_name, base_url=self.ollama_base_url)
            self.llm.invoke("Hi", stop=["Hi"]) 
            self.current_llm_model = model_name
            print(f"âœ… LLM æ¨¡å‹æˆåŠŸåˆ‡æ›ç‚º: {self.current_llm_model}")
            return True
        except Exception as e:
            print(f"âŒ åˆ‡æ› LLM æ¨¡å‹å¤±æ•—: {e}")
            return False

    def set_history_retrieval(self, enabled: bool):
        print(f"ğŸ”„ å°‡æ­·å²å°è©±æª¢ç´¢è¨­å®šç‚º: {'å•Ÿç”¨' if enabled else 'åœç”¨'}")
        self.use_history = enabled
        return True

    def set_wikipedia_search(self, enabled: bool):
        print(f"ğŸ”„ å°‡ç¶­åŸºç™¾ç§‘æœå°‹è¨­å®šç‚º: {'å•Ÿç”¨' if enabled else 'åœç”¨'}")
        self.use_wikipedia = enabled
        return True
    
    def set_scraper_search(self, enabled: bool):
        print(f"ğŸ”„ å°‡ç¶²é çˆ¬èŸ²è¨­å®šç‚º: {'å•Ÿç”¨' if enabled else 'åœç”¨'}")
        self.use_scraper = enabled
        return True

    def add_document(self, file_path: str):
        print(f"ğŸ“„ æ­£åœ¨è™•ç†æ–°æ–‡ä»¶: {file_path}")
        loader = UnstructuredFileLoader(file_path)
        docs = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)
        self.vector_db.add_documents(splits)
        print(f"âœ… æ–‡ä»¶ '{os.path.basename(file_path)}' å·²æˆåŠŸåŠ å…¥è³‡æ–™åº«ã€‚")
        if os.path.exists(file_path):
            os.remove(file_path)

    def _scrape_webpage_text(self, url: str):
        print(f"ğŸ•¸ï¸ (çˆ¬èŸ²) æ­£åœ¨å˜—è©¦çˆ¬å–ç¶²å€: {url}")
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            for element in soup(['script', 'style', 'nav', 'footer', 'aside']):
                element.decompose()
            text = '\n'.join(line.strip() for line in soup.get_text().splitlines() if line.strip())
            print(f"âœ… (çˆ¬èŸ²) æˆåŠŸç²å–ç¶²é æ–‡å­—ï¼Œé•·åº¦: {len(text)} å­—å…ƒã€‚")
            return text[:4000]
        except Exception as e:
            print(f"âŒ (çˆ¬èŸ²) å¤±æ•—: {e}")
            return f"ç„¡æ³•çˆ¬å–ç¶²å€ï¼ŒéŒ¯èª¤: {e}"
            
    def _search_wikipedia(self, query: str):
        print(f"ğŸ” (å¤–éƒ¨) æ­£åœ¨å¾ç¶­åŸºç™¾ç§‘æœå°‹ '{query}'...")
        try:
            wikipedia.set_lang("zh-tw")
            suggestion = wikipedia.suggest(query)
            search_query = suggestion if suggestion else query
            summary = wikipedia.summary(search_query, sentences=5)
            print(f"âœ… (å¤–éƒ¨) æ‰¾åˆ°ç¶­åŸºç™¾ç§‘æ‘˜è¦ã€‚")
            return summary
        except wikipedia.exceptions.PageError:
            return "ç„¡ç›¸é—œè³‡æ–™"
        except Exception as e:
            print(f"âŒ (å¤–éƒ¨) ç¶­åŸºç™¾ç§‘æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return "æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤"

    def _summarize_text(self, text: str) -> str:
        print(f"ğŸ“ (å…§éƒ¨) æ­£åœ¨ç¸½çµæ–‡å­—ï¼ŒåŸå§‹é•·åº¦: {len(text)}")
        try:
            prompt_value = self.summarizer_prompt.format(text_to_summarize=text)
            summary = self.llm.invoke(prompt_value)
            print(f"âœ… (å…§éƒ¨) ç¸½çµå®Œæˆï¼Œæ–°é•·åº¦: {len(summary)}")
            return summary
        except Exception as e:
            print(f"âŒ (å…§éƒ¨) ç¸½çµæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return text[:self.history_summary_threshold]

    def ask(self, question: str, stream: bool = False):
        print(f"\nğŸ¤” æ”¶åˆ°è«‹æ±‚ï¼Œå•é¡Œ: '{question}'")

        print("ğŸ’¡ æ­£åœ¨é€²è¡ŒæŸ¥è©¢æ“´å±•...")
        try:
            expansion_prompt_value = self.query_expansion_prompt.format(original_query=question)
            expanded_query = self.llm.invoke(expansion_prompt_value).strip()
            print(f"   -> æ“´å±•å¾ŒæŸ¥è©¢: '{expanded_query}'")
            retrieval_query = f"{question}\n{expanded_query}"
        except Exception as e:
            print(f"   -> æŸ¥è©¢æ“´å±•å¤±æ•—: {e}ï¼Œå°‡ä½¿ç”¨åŸå§‹æŸ¥è©¢ã€‚")
            retrieval_query = question

        context_parts = []
        retrieved_docs = []

        # ç¶²é å…§å®¹
        if self.use_scraper:
            url_match = re.search(r'https?://[\S]+', question)
            if url_match:
                url = url_match.group(0)
                web_context = self._scrape_webpage_text(url)
                if "ç„¡æ³•çˆ¬å–" not in web_context:
                    context_parts.append(f"ä¾†æºï¼šç¶²é å…§å®¹ ({url})\nå…§å®¹ï¼š\n{web_context}")
        
        # ç¶­åŸºç™¾ç§‘å…§å®¹
        if self.use_wikipedia:
            wiki_context = self._search_wikipedia(retrieval_query)
            if wiki_context not in ["ç„¡ç›¸é—œè³‡æ–™", "æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤"]:
                context_parts.append(f"ä¾†æºï¼šç¶­åŸºç™¾ç§‘\nå…§å®¹ï¼š\n{wiki_context}")
        
        # æ­·å²å°è©±èˆ‡æ–‡ä»¶å…§å®¹
        if self.use_history:
            print(f"ğŸ” (å…§éƒ¨) æ­£åœ¨ä½¿ç”¨æŸ¥è©¢ '{retrieval_query[:50]}...' é€²è¡Œæª¢ç´¢...")
            docs_from_db = self.retriever.get_relevant_documents(retrieval_query)
            docs_from_db = [doc for doc in docs_from_db if doc.page_content != "start"]
            
            if docs_from_db:
                key_term_match = re.search(r'(_[A-Z0-9]{2,4}\b)', question)
                if key_term_match:
                    key_term = key_term_match.group(1)
                    print(f"   -> æ­£åœ¨éæ¿¾çµæœï¼Œè¦æ±‚å¿…é ˆåŒ…å«é—œéµè© '{key_term}'...")
                    filtered_docs = [doc for doc in docs_from_db if key_term in doc.page_content]
                    if filtered_docs:
                        docs_from_db = filtered_docs
                
                retrieved_docs = docs_from_db # æ›´æ–°ç”¨æ–¼é¡¯ç¤ºä¾†æºçš„è®Šæ•¸
                context_from_docs = "\n---\n".join([f"ä¾†æºï¼š{doc.metadata.get('source', 'æœªçŸ¥')}\nå…§å®¹ï¼š\n{doc.page_content}" for doc in docs_from_db])
                
                if len(context_from_docs) > self.history_summary_threshold:
                    print(f"â“˜ (å…§éƒ¨) ä¸Šä¸‹æ–‡éé•· ({len(context_from_docs)} å­—å…ƒ)ï¼Œæ­£åœ¨é€²è¡Œç¸½çµ...")
                    summarized_context = self._summarize_text(context_from_docs)
                    context_parts.append(f"[ç¸½çµå¾Œçš„ç›¸é—œè³‡æ–™]:\n{summarized_context}")
                else:
                    context_parts.append(f"[ç›¸é—œè³‡æ–™åº«å…§å®¹]:\n{context_from_docs}")
        
        # çµ„åˆæ‰€æœ‰ä¸Šä¸‹æ–‡
        final_context = "\n\n".join(context_parts)
        if not final_context:
            final_context = "æ²’æœ‰å¯ç”¨çš„ä¸Šä¸‹æ–‡è³‡æ–™ã€‚"

        print("ğŸ“ æ­£åœ¨çµ„åˆ Prompt...")
        formatted_prompt = self.main_prompt.format(
            context=final_context,
            question=question
        )
        
        print(f"ğŸ¤– æ­£åœ¨ä½¿ç”¨æ¨¡å‹ '{self.current_llm_model}' ç”Ÿæˆå›ç­”...")
        if stream:
            return self.stream_and_save(question, formatted_prompt, retrieved_docs)
        else:
            try:
                answer = self.llm.invoke(formatted_prompt)
                self.save_qa(question, answer)
                return answer
            except Exception as e:
                error_msg = f"æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
                print(f"âŒ åœ¨éä¸²æµç”Ÿæˆéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
                return error_msg

    def stream_and_save(self, question, prompt, source_documents):
        full_answer = ""
        try:
            if source_documents:
                source_data = [{"page_content": doc.page_content, "metadata": doc.metadata} for doc in source_documents]
                yield f"data: {json.dumps({'type': 'sources', 'data': source_data})}\n\n"

            for chunk in self.llm.stream(prompt):
                full_answer += chunk
                response_chunk = {"type": "content", "content": chunk, "error": None}
                yield f"data: {json.dumps(response_chunk)}\n\n"
            
            print("ğŸ’¾ æ­£åœ¨å„²å­˜æœ¬æ¬¡å•ç­”...")
            self.save_qa(question, full_answer)

        except Exception as e:
            error_msg = f"æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
            print(f"âŒ åœ¨ä¸²æµç”Ÿæˆéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            response_chunk = {"type": "error", "error": error_msg}
            yield f"data: {json.dumps(response_chunk)}\n\n"
        
        finally:
            yield f"data: [DONE]\n\n"

    def save_qa(self, question, answer):
        if not answer or answer.strip() == "":
            print("   -> åµæ¸¬åˆ°ç©ºå›ç­”ï¼Œè·³éå„²å­˜ã€‚")
            return
            
        qa_pair_content = f"å•é¡Œ: {question}\nå›ç­”: {answer}"
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        metadata = { "source": "conversation", "timestamp": current_time }
        new_doc = Document(page_content=qa_pair_content, metadata=metadata)
        self.vector_db.add_documents([new_doc])
        print("   -> å°è©±æ­·å²å„²å­˜å®Œç•¢ï¼")
