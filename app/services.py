import os
import json
import re
import requests
import wikipedia
from datetime import datetime
from bs4 import BeautifulSoup
import logging

# LangChain Imports
from langchain_ollama.llms import OllamaLLM
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain.prompts import PromptTemplate
from langchain_unstructured.document_loaders import UnstructuredLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain_core.output_parsers import StrOutputParser
from langchain_community.vectorstores.utils import filter_complex_metadata

# Configure logging
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')


def get_ollama_models(ollama_base_url="http://localhost:11434"):
    try:
        response = requests.get(f"{ollama_base_url}/api/tags")
        response.raise_for_status()
        return [model["name"] for model in response.json().get("models", [])]
    except Exception as e:
        logging.error(f"å­˜å– Ollama æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return []


class ConversationalRAG:
    def __init__(self, config: dict):
        self.config = config
        self.persist_directory = self.config['PERSIST_DIRECTORY']
        self.use_wikipedia = False
        self.use_history = True
        self.use_scraper = False
        self.ollama_base_url = self.config['OLLAMA_BASE_URL']
        self.history_summary_threshold = 2000

        self.all_docs_for_bm25 = []
        self.bm25_retriever = None

        logging.info("æ­£åœ¨åˆå§‹åŒ– Embedding æ¨¡å‹...")
        logging.info(f"   -> ä½¿ç”¨æ¨¡å‹: {self.config['EMBEDDING_MODEL_NAME']}")
        logging.info(f"   -> ä½¿ç”¨è¨­å‚™: {self.config['EMBEDDING_DEVICE']}")
        try:
            self.embeddings = HuggingFaceEmbeddings(
                model_name=self.config['EMBEDDING_MODEL_NAME'],
                model_kwargs={
                    'device': self.config['EMBEDDING_DEVICE'], 'trust_remote_code': True}
            )
        except Exception as e:
            logging.error(f"åˆå§‹åŒ– Embedding æ¨¡å‹å¤±æ•—: {e}")
            raise

        logging.info("æ­£åœ¨åˆå§‹åŒ–/è¼‰å…¥å‘é‡æ•¸æ“šåŒ–...")
        try:
            self.vector_db = Chroma(
                persist_directory=self.persist_directory,
                embedding_function=self.embeddings
            )
        except Exception as e:
            logging.error(f"åˆå§‹åŒ–å‘é‡æ•¸æ“šåº«å¤±æ•—: {e}")
            raise

        logging.info("æ­£åœ¨è¨­ç½® LLM...")
        try:
            self.llm = OllamaLLM(
                model=self.config['llm_model'], base_url=self.ollama_base_url)
            self.current_llm_model = self.config['llm_model']
        except Exception as e:
            logging.error(f"è¨­ç½® LLM å¤±æ•—: {e}")
            raise

        logging.info("æ­£åœ¨åˆå§‹åŒ–æ··åˆæª¢ç´¢å™¨...")
        try:
            self.vector_retriever = self.vector_db.as_retriever(
                search_kwargs={'k': self.config['VECTOR_SEARCH_K']}
            )
            self.ensemble_retriever = self.vector_retriever
            self.update_ensemble_retriever(full_rebuild=True)
        except Exception as e:
            logging.error(f"åˆå§‹åŒ–æ··åˆæª¢ç´¢å™¨å¤±æ•—: {e}")
            raise

        self._init_prompts()

    def _init_prompts(self):
        self.main_prompt = PromptTemplate.from_template(
            """ä½ æ˜¯ä¸€ä½é ‚ç´šçš„ AI æŠ€è¡“æª”åˆ†æå¸«ã€‚ä½ çš„ä»»å‹™æ˜¯åš´æ ¼åŸºæ–¼ä¸‹é¢æä¾›çš„ã€Œä¸Šä¸‹æ–‡è³‡æ–™ã€ï¼Œæ·±å…¥åœ°å›ç­”ã€Œä½¿ç”¨è€…ç•¶å‰å•é¡Œã€ã€‚

**åŸ·è¡Œæµç¨‹èˆ‡è¦å‰‡ï¼š**
1.  **æ·±å…¥åˆ†æä¸Šä¸‹æ–‡**ï¼šé¦–å…ˆï¼Œä»”ç´°é–±è®€ä¸¦å®Œå…¨ç†è§£ã€Œä¸Šä¸‹æ–‡è³‡æ–™ã€ä¸­èˆ‡ã€Œä½¿ç”¨è€…ç•¶å‰å•é¡Œã€ç›¸é—œçš„æ‰€æœ‰ç‰‡æ®µã€‚
2.  **çµ„ç¹”èˆ‡å›ç­”**ï¼š
    *   å¦‚æœã€Œä¸Šä¸‹æ–‡è³‡æ–™ã€ä¸­åŒ…å«å›ç­”å•é¡Œæ‰€éœ€çš„è³‡è¨Šï¼Œä½ çš„ä»»å‹™æ˜¯ã€æˆæ¬Šä¸¦é¼“å‹µã€‘ä½ ä½¿ç”¨è‡ªå·±çš„èªè¨€èƒ½åŠ›ï¼Œå°é€™äº›ç¢ç‰‡åŒ–çš„è³‡è¨Šé€²è¡Œ**ç¸½çµã€æ¨ç†ã€å’Œé‡æ–°çµ„ç¹”**ï¼Œä»¥å½¢æˆä¸€å€‹é€£è²«ã€æ¸…æ™°ã€å°ˆæ¥­çš„å›ç­”ã€‚
    *   å¦‚æœã€Œä¸Šä¸‹æ–‡è³‡æ–™ã€å®Œå…¨æ²’æœ‰æåŠå•é¡Œçš„æ ¸å¿ƒä¸»é¡Œï¼Œé‚£éº¼ä½ ã€å¿…é ˆã€‘åªå›ç­”ï¼šã€Œæ ¹æ“šæˆ‘æ‰€æŒæ¡çš„è³‡æ–™ï¼Œæˆ‘æ‰¾ä¸åˆ°é—œæ–¼ '{question}' çš„ç¢ºåˆ‡è³‡è¨Šã€‚ã€
3.  **å®šç¾©â€œå¹»è¦ºâ€ç¦å€**ï¼šä½ ã€çµ•å°ç¦æ­¢ã€‘å¼•å…¥ä»»ä½•**åœ¨ã€Œä¸Šä¸‹æ–‡è³‡æ–™ã€ä¸­å®Œå…¨ä¸å­˜åœ¨çš„ã€æ†‘ç©ºæé€ çš„äº‹å¯¦æˆ–è³‡æ–™**ã€‚
4.  **æ€è€ƒéç¨‹**ï¼šåœ¨æœ€çµ‚ç­”æ¡ˆå‰ï¼Œä½ å¯ä»¥ä½¿ç”¨ <think>...</think> æ¨™ç±¤ä¾†å¯«ä¸‹ä½ çš„åˆ†æã€æ¨ç†å’Œåˆ¤æ–·éç¨‹ã€‚
5.  **ã€æ ¼å¼è¦æ±‚ã€‘**: `<think>` å€å¡Šå¿…é ˆä»¥ `</think>` çµæŸï¼Œä¸” `</think>` æ¨™ç±¤ä¹‹å¾Œã€å¿…é ˆç«‹åˆ»ã€‘é–‹å§‹ä½ çš„æœ€çµ‚å›ç­”ï¼Œä¸­é–“ä¸èƒ½æœ‰ä»»ä½•æ›è¡Œæˆ–å¤šé¤˜çš„ç©ºæ ¼ã€‚

---
[ä¸Šä¸‹æ–‡è³‡æ–™]:
{context}
---
[ä½¿ç”¨è€…ç•¶å‰å•é¡Œ]: {question}
ä½ çš„å›ç­”:"""
        )
        self.query_expansion_prompt = PromptTemplate.from_template(
            "ä½ æ˜¯ä¸€å€‹æŸ¥è©¢å„ªåŒ–åŠ©ç†ã€‚è«‹æ ¹æ“šä½¿ç”¨è€…æä¾›çš„åŸå§‹æŸ¥è©¢ï¼Œç”Ÿæˆä¸€å€‹æ›´å…·é«”ã€æ›´å¯èƒ½åœ¨æŠ€è¡“æª”ä¸­æ‰¾åˆ°ç›¸é—œå…§å®¹çš„æ“´å±•æŸ¥è©¢ã€‚è«‹åªè¿”å›æ“´å±•å¾Œçš„æŸ¥è©¢ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡‹ã€‚\n\n[åŸå§‹æŸ¥è©¢]: {original_query}\n\n[æ“´å±•æŸ¥è©¢]:"
        )
        self.router_prompt = PromptTemplate.from_template(
            """ä½ æ˜¯ä¸€å€‹ä»»å‹™è·¯ç”±å™¨ã€‚æ ¹æ“šä½¿ç”¨è€…çš„å•é¡Œï¼Œåˆ¤æ–·å®ƒå±¬æ–¼å“ªä¸€ç¨®é¡å‹ã€‚è«‹åªå›ç­”ä»¥ä¸‹åˆ†é¡ä¸­çš„ä¸€å€‹ï¼š'rag_query' æˆ– 'general_conversation'ã€‚

- å¦‚æœå•é¡Œéœ€è¦æŸ¥æ‰¾ã€è§£é‡‹ã€æ¯”è¼ƒæˆ–å®šç¾©ç‰¹å®šè³‡è¨Šï¼Œç‰¹åˆ¥æ˜¯æŠ€è¡“è¡“èªï¼Œè«‹å›ç­” 'rag_query'ã€‚
- å¦‚æœå•é¡Œæ˜¯ç°¡å–®çš„å•å€™ã€é–’èŠæˆ–èˆ‡çŸ¥è­˜åº«ç„¡é—œçš„å°è©±ï¼Œè«‹å›ç­” 'general_conversation'ã€‚

[ä½¿ç”¨è€…å•é¡Œ]: {question}
[åˆ†é¡]:"""
        )

    def set_llm_model(self, model_name: str):
        logging.info(f"\nğŸ”„ æ­£åœ¨åˆ‡æ› LLM æ¨¡å‹è‡³: {model_name}")
        try:
            self.llm = OllamaLLM(
                model=model_name, base_url=self.ollama_base_url)
            self.llm.invoke("Hi", stop=["Hi"])
            self.current_llm_model = model_name
            logging.info(f"âœ… LLM æ¨¡å‹æˆåŠŸåˆ‡æ›ç‚º: {self.current_llm_model}")
            return True
        except Exception as e:
            logging.error(f"åˆ‡æ› LLM æ¨¡å‹å¤±æ•—: {e}")
            return False

    def set_history_retrieval(self, enabled: bool):
        logging.info(f"ğŸ”„ å°‡æ­·å²å°è©±æª¢ç´¢è¨­ç½®ç‚º: {'å•Ÿç”¨' if enabled else 'åœç”¨'}")
        self.use_history = enabled
        return True

    def set_wikipedia_search(self, enabled: bool):
        logging.info(f"ğŸ”„ å°‡ç¶­çªç™¾ç§‘æœç´¢è¨­ç½®ç‚º: {'å•Ÿç”¨' if enabled else 'åœç”¨'}")
        self.use_wikipedia = enabled
        return True

    def set_scraper_search(self, enabled: bool):
        logging.info(f"ğŸ”„ å°‡ç¶²é çˆ¬èŸ²è¨­ç½®ç‚º: {'å•Ÿç”¨' if enabled else 'åœç”¨'}")
        self.use_scraper = enabled
        return True

    def search_records(self, query: str = "", page: int = 1, per_page: int = 50):
        logging.info(f"ğŸ” æ­£åœ¨è³‡æ–™åº«ä¸­æœç´¢ '{query}'ï¼Œç¬¬ {page} é ...")
        offset = (page - 1) * per_page
        where_document_filter = {
            "$contains": query} if query and query.strip() else None

        try:
            all_matching_ids = self.vector_db._collection.get(
                where_document=where_document_filter,
                include=[]
            )['ids']
            total_records = len(all_matching_ids)

            results = self.vector_db.get(
                limit=per_page,
                offset=offset,
                where_document=where_document_filter,
                include=["metadatas", "documents"]
            )
            records = [{
                "id": results['ids'][i], "content": results['documents'][i], "metadata": results['metadatas'][i]
            } for i in range(len(results['ids']))]

            return {
                "records": sorted(records, key=lambda x: x['id'], reverse=True),
                "total_records": total_records, "current_page": page,
                "total_pages": (total_records + per_page - 1) // per_page
            }
        except Exception as e:
            logging.error(f"åœ¨è³‡æ–™åº«æœç´¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {"records": [], "total_records": 0, "current_page": 1, "total_pages": 0}

    def update_ensemble_retriever(self, new_docs: list = None, full_rebuild: bool = False):
        logging.info("ğŸ”„ æ­£åœ¨æ›´æ–° Ensemble Retriever...")
        if full_rebuild:
            logging.info("   -> åŸ·è¡Œå®Œæ•´é‡å»º...")
            try:
                all_data = self.vector_db.get(
                    include=["documents", "metadatas"])
                documents_content = all_data['documents']
                metadatas = all_data['metadatas']
                self.all_docs_for_bm25 = [
                    Document(
                        page_content=documents_content[i], metadata=metadatas[i])
                    for i in range(len(documents_content))
                    if documents_content[i] != "start"
                ]
                logging.info(
                    f"   -> å¾è³‡æ–™åº«è¼‰å…¥ {len(self.all_docs_for_bm25)} ä»½æª”é€²è¡Œç´¢å¼•ã€‚")
            except Exception as e:
                logging.error(f"å¾è³‡æ–™åº«ç²å–æ–‡æª”å¤±æ•—: {e}")
                self.all_docs_for_bm25 = []

        if new_docs:
            logging.info(f"   -> åŸ·è¡Œå¢é‡æ›´æ–°ï¼Œæ–°å¢ {len(new_docs)} ä»½æ–‡ä»¶...")
            self.all_docs_for_bm25.extend(new_docs)

        if not self.all_docs_for_bm25:
            logging.info("   -> è³‡æ–™åº«æ–‡æª”ä¸è¶³ï¼Œåƒ…ä½¿ç”¨å‘é‡æª¢ç´¢å™¨ã€‚")
            self.ensemble_retriever = self.vector_retriever
            self.bm25_retriever = None
            return

        try:
            logging.info(
                f"   -> æ­£åœ¨åŸºæ–¼ {len(self.all_docs_for_bm25)} ä»½æª”é‡å»º BM25 ç´¢å¼•...")
            self.bm25_retriever = BM25Retriever.from_documents(
                self.all_docs_for_bm25,
                k=self.config['BM25_SEARCH_K']
            )
            self.ensemble_retriever = EnsembleRetriever(
                retrievers=[self.bm25_retriever, self.vector_retriever],
                weights=self.config['ENSEMBLE_WEIGHTS']
            )
            logging.info("âœ… æ··åˆæª¢ç´¢å™¨å·²æˆåŠŸæ›´æ–°ã€‚")
        except Exception as e:
            logging.error(f"æ›´æ–°æ··åˆæª¢ç´¢å™¨å¤±æ•—: {e}ã€‚å°‡é€€å›è‡³åƒ…ä½¿ç”¨å‘é‡æª¢ç´¢å™¨ã€‚")
            self.ensemble_retriever = self.vector_retriever


    def add_document(self, file_path: str):
        logging.info(f"ğŸ“„ æ­£åœ¨è™•ç†æ–°æ–‡ä»¶: {file_path}")
        try:
            loader = UnstructuredLoader(
            file_path, mode="elements", strategy="fast")
            docs = loader.load()

            text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.config.get('CHUNK_SIZE', 1000),
            chunk_overlap=self.config.get('CHUNK_OVERLAP', 200)
        )
            splits = text_splitter.split_documents(docs)
            logging.info(f"   -> æ–‡ä»¶è¢«åˆ‡å‰²æˆ {len(splits)} å€‹ç‰‡æ®µã€‚")

            file_name = os.path.basename(file_path)
            for split in splits:
                split.metadata['source'] = file_name
            final_splits = filter_complex_metadata(splits)
            logging.info(f"   -> å·²æ¸…ç† {len(final_splits)} å€‹ç‰‡æ®µçš„å…ƒæ•¸æ“šã€‚")

            if not final_splits:
                logging.warning("   -> æ–‡ä»¶è™•ç†å¾Œæ²’æœ‰ç”Ÿæˆä»»ä½•å¯ç”¨çš„æ–‡å­—ç‰‡æ®µï¼Œè™•ç†ä¸­æ­¢ã€‚")
                return

            batch_size = 32  # è¼ƒå°çš„æ‰¹æ¬¡å¤§å° (32-128) é€šå¸¸æ›´ç©©å®š
            total_splits = len(final_splits)
            logging.info(f"   -> å°‡ä»¥æ¯æ‰¹ {batch_size} å€‹ç‰‡æ®µçš„å¤§å°ï¼Œåˆ†æ‰¹æ¬¡å­˜å…¥è³‡æ–™åº«...")
            for i in range(0, total_splits, batch_size):
                batch = final_splits[i:i + batch_size]
                self.vector_db.add_documents(batch)
                logging.info(
                f"      -> å·²å­˜å…¥ {min(i + batch_size, total_splits)} / {total_splits} å€‹ç‰‡æ®µ...")

            logging.info(f"âœ… æ–‡ä»¶ '{file_name}' å·²æˆåŠŸå­˜å…¥å‘é‡è³‡æ–™åº«ã€‚")

            self.update_ensemble_retriever(new_docs=final_splits)

        except Exception as e:
          logging.error(f"æ–‡ä»¶è™•ç†æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)
          raise
        finally:
           logging.info(f"ğŸ§¹ æ­£åœ¨æ¸…ç†æš«å­˜æ–‡ä»¶: {file_path}")
           if os.path.exists(file_path):
               try:
                   os.remove(file_path)
               except OSError as e:
                   logging.error(f"æ¸…ç†æš«å­˜æ–‡ä»¶ {file_path} å¤±æ•—: {e}")

    def _scrape_webpage_text(self, url: str):
        logging.info(f"ğŸ•¸ï¸ (çˆ¬èŸ²) æ­£åœ¨å˜—è©¦çˆ¬å–ç¶²å€: {url}")
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            for element in soup(['script', 'style', 'nav', 'footer', 'aside']):
                element.decompose()
            text = '\n'.join(line.strip()
                             for line in soup.get_text().splitlines() if line.strip())
            logging.info(f"âœ… (çˆ¬èŸ²) æˆåŠŸç²å–ç¶²é æ–‡å­—ï¼Œé•·åº¦: {len(text)} å­—å…ƒã€‚")
            return text[:4000]
        except Exception as e:
            logging.error(f"âŒ (çˆ¬èŸ²) å¤±æ•—: {e}")
            return f"ç„¡æ³•çˆ¬å–ç¶²å€ï¼ŒéŒ¯èª¤: {e}"

    def _search_wikipedia(self, query: str):
        logging.info(f"ğŸ” (ç¶­çªç™¾ç§‘) æ­£åœ¨æœç´¢: '{query[:50].strip()}...'")
        try:
            wikipedia.set_lang("zh-tw")
            summary = wikipedia.summary(query, sentences=5, auto_suggest=False)
            logging.info("âœ… (ç¶­çªç™¾ç§‘) æ‰¾åˆ°æ‘˜è¦ã€‚")
            return summary
        except wikipedia.exceptions.PageError:
            return "ç„¡ç›¸é—œè³‡æ–™"
        except Exception as e:
            logging.error(f"âŒ (ç¶­çªç™¾ç§‘) æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return "æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤"

    def _get_rag_context(self, question: str, retrieval_query: str):
        all_source_docs = []
        context_parts = []

        if self.use_scraper:
            url_match = re.search(r'https?://[\S]+', question)
            if url_match:
                url = url_match.group(0)
                web_content = self._scrape_webpage_text(url)
                if "ç„¡æ³•çˆ¬å–" not in web_content:
                    web_doc = Document(page_content=web_content, metadata={
                                       "source": f"ç¶²é : {url}"})
                    all_source_docs.append(web_doc)
                    context_parts.append(
                        f"ä¾†æºï¼š{web_doc.metadata['source']}\nå…§å®¹ï¼š\n{web_doc.page_content}")
        if self.use_wikipedia:
            short_query = question
            wiki_content = self._search_wikipedia(short_query)
            if wiki_content not in ["ç„¡ç›¸é—œè³‡æ–™", "æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤"]:
                wiki_doc = Document(page_content=wiki_content,
                                    metadata={"source": "ç¶­çªç™¾ç§‘"})
                all_source_docs.append(wiki_doc)
                context_parts.append(
                    f"ä¾†æºï¼š{wiki_doc.metadata['source']}\nå…§å®¹ï¼š\n{wiki_doc.page_content}")

        if self.use_history:
            logging.info(
                f"ğŸ” (æ··åˆæª¢ç´¢) æ­£åœ¨æª¢ç´¢: '{retrieval_query[:80].replace('\n', ' ')}...'")
            db_docs = self.ensemble_retriever.get_relevant_documents(
                retrieval_query)
            db_docs = [doc for doc in db_docs if hasattr(
                doc, 'page_content') and doc.page_content != "start"]

            if db_docs:
                all_source_docs.extend(db_docs)
                context_from_docs = "\n---\n".join(
                    [f"ä¾†æºï¼š{doc.metadata.get('source', 'æœªçŸ¥')}\nå…§å®¹ï¼š\n{doc.page_content}" for doc in db_docs])
                context_parts.append(f"[ç›¸é—œè³‡æ–™åº«å…§å®¹]:\n{context_from_docs}")

        final_context = "\n\n".join(
            context_parts) if context_parts else "æ²’æœ‰å¯ç”¨çš„ä¸Šä¸‹æ–‡è³‡æ–™ã€‚"
        return final_context, all_source_docs

    def ask(self, question: str, stream: bool = True):
        logging.info(f"\nğŸ¤” æ”¶åˆ°è«‹æ±‚ï¼Œå•é¡Œ: '{question}'")

        router_chain = self.router_prompt | self.llm | StrOutputParser()
        try:
            route = router_chain.invoke({"question": question}).strip().lower()
            logging.info(f"ğŸš¦ è·¯ç”±å™¨æ±ºç­–: {route}")
        except Exception as e:
            logging.error(f"ğŸš¦ è·¯ç”±å™¨æ±ºç­–å¤±æ•—: {e}, å°‡èµ°é è¨­ RAG è·¯å¾‘ã€‚")
            route = 'rag_query'

        if 'rag_query' in route:
            try:
                query_chain = self.query_expansion_prompt | self.llm | StrOutputParser()
                raw_expanded_output = query_chain.invoke(
                    {"original_query": question})
                match = re.search(r'\[æ“´å±•æŸ¥è©¢\]:\s*([\s\S]*)',
                                  raw_expanded_output, re.IGNORECASE)
                if match:
                    expanded_query = match.group(1).strip()
                else:
                    expanded_query = re.sub(
                        r'<think>.*?</think>', '', raw_expanded_output, flags=re.DOTALL).strip()
                if not expanded_query:
                    expanded_query = question
                retrieval_query = f"{question}\n{expanded_query}"
                logging.info(f"ğŸ’¡ æ¸…ç†å¾Œçš„æ“´å±•æŸ¥è©¢: {expanded_query}")
            except Exception as e:
                logging.error(f"ğŸ’¡ æŸ¥è©¢æ“´å±•å¤±æ•—: {e}, ä½¿ç”¨åŸå§‹æŸ¥è©¢ã€‚")
                retrieval_query = question

            final_context, all_source_docs = self._get_rag_context(
                question, retrieval_query)
            formatted_prompt = self.main_prompt.format(
                context=final_context, question=question)
            return self.stream_and_save(question, formatted_prompt, all_source_docs)
        else:
            logging.info("ğŸ’¬ èµ°é€šç”¨å°è©±è·¯å¾‘...")
            return self.stream_and_save(question, question, [])

    def stream_and_save(self, question, prompt, source_documents):
        full_answer = ""
        try:
            if source_documents:
                source_data = [{"page_content": doc.page_content,
                                "metadata": doc.metadata} for doc in source_documents]
                yield f"data: {json.dumps({'type': 'sources', 'data': source_data})}\n\n"

            for chunk in self.llm.stream(prompt):
                full_answer += chunk
                yield f"data: {json.dumps({'type': 'content', 'content': chunk})}\n\n"

            self.save_qa(question, full_answer)
        except Exception as e:
            error_msg = f"æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
            logging.error(f"âŒ åœ¨ä¸²æµç”Ÿæˆéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            yield f"data: {json.dumps({'type': 'error', 'error': error_msg})}\n\n"
        finally:
            yield f"data: [DONE]\n\n"

    def save_qa(self, question, answer):
        if not answer or not answer.strip():
            logging.info("   -> æª¢æ¸¬åˆ°ç©ºå›ç­”ï¼Œè·³éå­˜å„²ã€‚")
            return

        qa_pair_content = f"å•é¡Œ: {question}\nå›ç­”: {answer}"
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        metadata = {"source": "conversation", "timestamp": current_time}
        new_doc = Document(page_content=qa_pair_content, metadata=metadata)
        self.vector_db.add_documents([new_doc])
        self.update_ensemble_retriever(new_docs=[new_doc])
        logging.info("   -> å°è©±æ­·å²å­˜å„²ä¸¦åŒæ­¥è‡³æ··åˆç´¢å¼•å®Œç•¢ï¼")

