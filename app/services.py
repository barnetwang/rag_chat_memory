import os
import json
import re
import requests
import wikipedia
from datetime import datetime
from bs4 import BeautifulSoup
import logging
import urllib3
import time
from typing import Any

# LangChain Imports
from ddgs import DDGS
from langchain_ollama.llms import OllamaLLM
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain.prompts import PromptTemplate
from langchain_unstructured.document_loaders import UnstructuredLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain_core.output_parsers import StrOutputParser
from langchain_community.vectorstores.utils import filter_complex_metadata

try:
    from playwright.sync_api import sync_playwright, Playwright, Browser
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    logging.warning("Playwright æœªå®‰è£ã€‚çˆ¬èŸ²åŠŸèƒ½å°‡åƒ…ä½¿ç”¨ requestsã€‚å»ºè­°åŸ·è¡Œ 'pip install playwright && playwright install' ä¾†å¢å¼·çˆ¬èŸ²èƒ½åŠ›ã€‚")

try:
    import fitz
    PYMUPDF_AVAILABLE = True
except ImportError:
    PYMUPDF_AVAILABLE = False
    logging.warning("PyMuPDF æœªå®‰è£ã€‚PDF è™•ç†åŠŸèƒ½å°‡ä¸å¯ç”¨ã€‚å»ºè­°åŸ·è¡Œ 'pip install PyMuPDF'ã€‚")

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def get_ollama_models(ollama_base_url="http://localhost:11434"):
    try:
        response = requests.get(f"{ollama_base_url}/api/tags")
        response.raise_for_status()
        return [model["name"] for model in response.json().get("models", [])]
    except Exception as e:
        logging.error(f"å­˜å– Ollama æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return []

class ConversationalRAG:
    def __init__(self, config: dict):
        self.playwright = None
        self.browser = None
        self.config = config
        self.persist_directory = self.config["PERSIST_DIRECTORY"]
        self.use_web_search = True
        self.use_wikipedia = False
        self.use_history = True
        self.use_scraper = False
        self.ollama_base_url = self.config["OLLAMA_BASE_URL"]
        self.prompts = {}
        self.all_docs_for_bm25 = []
        self.bm25_retriever = None

        logging.info("æ­£åœ¨åˆå§‹åŒ– Embedding æ¨¡å‹...")
        try:
            self.embeddings = HuggingFaceEmbeddings(
                model_name=self.config["EMBEDDING_MODEL_NAME"],
                model_kwargs={"device": self.config["EMBEDDING_DEVICE"], "trust_remote_code": True},
            )
        except Exception as e:
            logging.error(f"åˆå§‹åŒ– Embedding æ¨¡å‹å¤±æ•—: {e}")
            raise

        logging.info("æ­£åœ¨åˆå§‹åŒ–/è¼‰å…¥å‘é‡æ•¸æ“šåŒ–...")
        try:
            self.vector_db = Chroma(
                persist_directory=self.persist_directory, embedding_function=self.embeddings
            )
        except Exception as e:
            logging.error(f"åˆå§‹åŒ–å‘é‡æ•¸æ“šåº«å¤±æ•—: {e}")
            raise

        logging.info("æ­£åœ¨è¨­ç½® LLM...")
        try:
            self.llm = OllamaLLM(model=self.config["llm_model"], base_url=self.ollama_base_url)
            self.current_llm_model = self.config["llm_model"]
        except Exception as e:
            logging.error(f"è¨­ç½® LLM å¤±æ•—: {e}")
            raise
            
        self._init_prompts()

        logging.info("æ­£åœ¨åˆå§‹åŒ–æ··åˆæª¢ç´¢å™¨...")
        try:
            self.vector_retriever = self.vector_db.as_retriever(search_kwargs={"k": self.config["VECTOR_SEARCH_K"]})
            self.ensemble_retriever = self.vector_retriever
            self.update_ensemble_retriever(full_rebuild=True)
        except Exception as e:
            logging.error(f"åˆå§‹åŒ–æ··åˆæª¢ç´¢å™¨å¤±æ•—: {e}")
            raise

        logging.info("âœ… ç³»çµ±å·²å°±ç·’ (ä½¿ç”¨ DDGS ä½œç‚ºæœå°‹å¼•æ“)ã€‚")

    def _init_playwright(self):
        if PLAYWRIGHT_AVAILABLE and not self.browser:
            try:
                logging.info("ğŸŒ æ­£åœ¨å•Ÿå‹•å…±äº«çš„ Playwright ç€è¦½å™¨å¯¦ä¾‹...")
                self.playwright = sync_playwright().start()
                self.browser = self.playwright.chromium.launch(headless=True)
                logging.info("âœ… Playwright å…±äº«ç€è¦½å™¨å¯¦ä¾‹å·²æˆåŠŸå•Ÿå‹•ã€‚")
            except Exception as e:
                logging.error(f"âŒ å•Ÿå‹• Playwright å¤±æ•—: {e}")
                self.playwright = None
                self.browser = None

    def _load_all_prompts(self, directory: str) -> dict:
        prompts = {}
        base_dir = os.path.dirname(os.path.abspath(__file__))
        prompts_dir = os.path.join(base_dir, directory)
        if not os.path.isdir(prompts_dir):
            return {}
        logging.info(f"ğŸ” æ­£åœ¨å¾ '{prompts_dir}' åŠ è¼‰ Prompt æ¨¡æ¿...")
        for filename in os.listdir(prompts_dir):
            if filename.endswith(".txt"):
                filepath = os.path.join(prompts_dir, filename)
                with open(filepath, 'r', encoding='utf-8') as f:
                    prompt_name = os.path.splitext(filename)[0]
                    prompts[prompt_name] = PromptTemplate.from_template(f.read(), template_format="f-string")
                    logging.info(f"   -> å·²åŠ è¼‰: {prompt_name}")
        return prompts

    def _init_prompts(self):
        self.prompts = self._load_all_prompts("prompts")
        aux_prompts = {
            "query_expansion": "ä½ æ˜¯ä¸€å€‹æŸ¥è©¢å„ªåŒ–åŠ©ç†ã€‚è«‹æ ¹æ“šä½¿ç”¨è€…æä¾›çš„åŸå§‹æŸ¥è©¢ï¼Œç”Ÿæˆä¸€å€‹æ›´å…·é«”ã€æ›´å¯èƒ½åœ¨æŠ€è¡“æª”ä¸­æ‰¾åˆ°ç›¸é—œå…§å®¹çš„æ“´å±•æŸ¥è©¢ã€‚è«‹åªè¿”å›æ“´å±•å¾Œçš„æŸ¥è©¢ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡‹ã€‚\n\n[åŸå§‹æŸ¥è©¢]: {original_query}\n\n[æ“´å±•æŸ¥è©¢]:",
            "web_search_generation": """ä½ æ˜¯ä¸€ä½ç²¾é€šç¶²è·¯æœå°‹çš„åŠ©ç†ã€‚ä½ çš„ä»»å‹™æ˜¯å°‡ä½¿ç”¨è€…æå‡ºçš„ã€Œç ”ç©¶å­ä»»å‹™ã€ï¼Œçµåˆã€Œæ ¸å¿ƒä¸»é¡Œã€ï¼Œè½‰æ›æˆä¸€çµ„ç°¡æ½”ã€é«˜æ•ˆçš„æœå°‹å¼•æ“é—œéµå­—ã€‚

è¦å‰‡ï¼š
- åªè¿”å›é—œéµå­—ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡‹æˆ–å¤šé¤˜çš„æ–‡å­—ã€‚
- å§‹çµ‚åŒ…å«æ ¸å¿ƒä¸»é¡Œï¼Œä»¥ç¢ºä¿æœå°‹çµæœçš„ç›¸é—œæ€§ã€‚
- ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚

ç¯„ä¾‹ï¼š
[æ ¸å¿ƒä¸»é¡Œ]: é›»å‹•è»Šå¸‚å ´åˆ†æ
[ç ”ç©¶å­ä»»å‹™]: åˆ†æé›»æ± æŠ€è¡“çš„ç™¼å±•ç“¶é ¸
[é—œéµå­—]: é›»å‹•è»Š é›»æ± æŠ€è¡“ç“¶é ¸ å›ºæ…‹é›»æ±  æˆæœ¬

[æ ¸å¿ƒä¸»é¡Œ]: {question}  <-- æˆ‘å€‘å¯ä»¥å°‡åŸå§‹å•é¡Œä½œç‚ºæ ¸å¿ƒä¸»é¡Œ
[ç ”ç©¶å­ä»»å‹™]: {task}
[é—œéµå­—]:"""
        }
        for name, template_str in aux_prompts.items():
            if name not in self.prompts:
                self.prompts[name] = PromptTemplate.from_template(
                    template_str, template_format="f-string"
                )
                logging.info(f"   -> å·²å‹•æ…‹åŠ è¼‰è¼”åŠ©æ¨¡æ¿: {name}")

        if 'router' not in self.prompts:
            logging.error("é—œéµçš„ 'router.txt' æ¨¡æ¿æœªæ‰¾åˆ°ï¼è«‹åœ¨ prompts ç›®éŒ„ä¸­å‰µå»ºå®ƒã€‚")
            fallback_router_template = 'ä½¿ç”¨è€…å•é¡Œ: {question}\n\n[JSON]: {{"path": "rag_query", "persona": "default_rag"}}'
            self.prompts['router'] = PromptTemplate.from_template(
                fallback_router_template, template_format="f-string"
            )

    def set_llm_model(self, model_name: str):
        logging.info(f"\nğŸ”„ æ­£åœ¨åˆ‡æ› LLM æ¨¡å‹è‡³: {model_name}")
        try:
            self.llm = OllamaLLM(model=model_name, base_url=self.ollama_base_url)
            self.llm.invoke("Hi", stop=["Hi"])
            self.current_llm_model = model_name
            logging.info(f"âœ… LLM æ¨¡å‹æˆåŠŸåˆ‡æ›ç‚º: {self.current_llm_model}")
            return True
        except Exception as e:
            logging.error(f"åˆ‡æ› LLM æ¨¡å‹å¤±æ•—: {e}")
            return False

    def set_web_search(self, enabled: bool):
        logging.info(f"ğŸ”„ å°‡ç¶²è·¯æœå°‹è¨­ç½®ç‚º: {'å•Ÿç”¨' if enabled else 'åœç”¨'}")
        self.use_web_search = enabled
        return True

    def set_history_retrieval(self, enabled: bool):
        logging.info(f"ğŸ”„ å°‡æ­·å²å°è©±æª¢ç´¢è¨­ç½®ç‚º: {'å•Ÿç”¨' if enabled else 'åœç”¨'}")
        self.use_history = enabled
        return True

    def set_wikipedia_search(self, enabled: bool):
        logging.info(f"ğŸ”„ å°‡ç¶­çªç™¾ç§‘æœç´¢è¨­ç½®ç‚º: {'å•Ÿç”¨' if enabled else 'åœç”¨'}")
        self.use_wikipedia = enabled
        return True

    def set_scraper_search(self, enabled: bool):
        logging.info(f"ğŸ”„ å°‡ç¶²é çˆ¬èŸ²è¨­ç½®ç‚º: {'å•Ÿç”¨' if enabled else 'åœç”¨'}")
        self.use_scraper = enabled
        return True

    def search_records(self, query: str = "", page: int = 1, per_page: int = 50):
        logging.info(f"ğŸ” æ­£åœ¨è³‡æ–™åº«ä¸­æœç´¢ '{query}'ï¼Œç¬¬ {page} é ...")
        offset = (page - 1) * per_page
        where_document_filter = {"$contains": query} if query and query.strip() else None
        try:
            all_matching_ids = self.vector_db._collection.get(
                where_document=where_document_filter, include=[]
            )["ids"]
            total_records = len(all_matching_ids)
            results = self.vector_db.get(
                limit=per_page,
                offset=offset,
                where_document=where_document_filter,
                include=["metadatas", "documents"],
            )
            records = [
                {
                    "id": results["ids"][i],
                    "content": results["documents"][i],
                    "metadata": results["metadatas"][i],
                }
                for i in range(len(results["ids"]))
            ]
            return {
                "records": sorted(records, key=lambda x: x["id"], reverse=True),
                "total_records": total_records,
                "current_page": page,
                "total_pages": (total_records + per_page - 1) // per_page,
            }
        except Exception as e:
            logging.error(f"åœ¨è³‡æ–™åº«æœç´¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {
                "records": [],
                "total_records": 0,
                "current_page": 1,
                "total_pages": 0,
            }

    def update_ensemble_retriever(
        self, new_docs: list = None, full_rebuild: bool = False
    ):
        logging.info("ğŸ”„ æ­£åœ¨æ›´æ–° Ensemble Retriever...")
        if full_rebuild:
            logging.info("   -> åŸ·è¡Œå®Œæ•´é‡å»º...")
            try:
                all_data = self.vector_db.get(include=["documents", "metadatas"])
                documents_content = all_data["documents"]
                metadatas = all_data["metadatas"]
                self.all_docs_for_bm25 = [
                    Document(page_content=documents_content[i], metadata=metadatas[i])
                    for i in range(len(documents_content))
                    if documents_content[i] != "start"
                ]
                logging.info(
                    f"   -> å¾è³‡æ–™åº«è¼‰å…¥ {len(self.all_docs_for_bm25)} ä»½æª”é€²è¡Œç´¢å¼•ã€‚"
                )
            except Exception as e:
                logging.error(f"å¾è³‡æ–™åº«ç²å–æ–‡æª”å¤±æ•—: {e}")
                self.all_docs_for_bm25 = []
        if new_docs:
            logging.info(f"   -> åŸ·è¡Œå¢é‡æ›´æ–°ï¼Œæ–°å¢ {len(new_docs)} ä»½æ–‡ä»¶...")
            self.all_docs_for_bm25.extend(new_docs)
        if not self.all_docs_for_bm25:
            logging.info("   -> è³‡æ–™åº«æ–‡æª”ä¸è¶³ï¼Œåƒ…ä½¿ç”¨å‘é‡æª¢ç´¢å™¨ã€‚")
            self.ensemble_retriever = self.vector_retriever
            self.bm25_retriever = None
            return
        try:
            logging.info(
                f"   -> æ­£åœ¨åŸºæ–¼ {len(self.all_docs_for_bm25)} ä»½æª”é‡å»º BM25 ç´¢å¼•..."
            )
            self.bm25_retriever = BM25Retriever.from_documents(
                self.all_docs_for_bm25, k=self.config["BM25_SEARCH_K"]
            )
            self.ensemble_retriever = EnsembleRetriever(
                retrievers=[self.bm25_retriever, self.vector_retriever],
                weights=self.config["ENSEMBLE_WEIGHTS"],
            )
            logging.info("âœ… æ··åˆæª¢ç´¢å™¨å·²æˆåŠŸæ›´æ–°ã€‚")
        except Exception as e:
            logging.error(f"æ›´æ–°æ··åˆæª¢ç´¢å™¨å¤±æ•—: {e}ã€‚å°‡é€€å›è‡³åƒ…ä½¿ç”¨å‘é‡æª¢ç´¢å™¨ã€‚")
            self.ensemble_retriever = self.vector_retriever

    def add_document(self, file_path: str):
        logging.info(f"ğŸ“„ æ­£åœ¨è™•ç†æ–°æ–‡ä»¶: {file_path}")
        try:
            loader = UnstructuredLoader(file_path, mode="elements", strategy="fast")
            docs = loader.load()
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=self.config.get("CHUNK_SIZE", 1000),
                chunk_overlap=self.config.get("CHUNK_OVERLAP", 200),
            )
            splits = text_splitter.split_documents(docs)
            logging.info(f"   -> æ–‡ä»¶è¢«åˆ‡å‰²æˆ {len(splits)} å€‹ç‰‡æ®µã€‚")
            file_name = os.path.basename(file_path)
            for split in splits:
                split.metadata["source"] = file_name
            final_splits = filter_complex_metadata(splits)
            logging.info(f"   -> å·²æ¸…ç† {len(final_splits)} å€‹ç‰‡æ®µçš„å…ƒæ•¸æ“šã€‚")
            if not final_splits:
                logging.warning("   -> æ–‡ä»¶è™•ç†å¾Œæ²’æœ‰ç”Ÿæˆä»»ä½•å¯ç”¨çš„æ–‡å­—ç‰‡æ®µï¼Œè™•ç†ä¸­æ­¢ã€‚")
                return
            batch_size = 32
            total_splits = len(final_splits)
            logging.info(f"   -> å°‡ä»¥æ¯æ‰¹ {batch_size} å€‹ç‰‡æ®µçš„å¤§å°ï¼Œåˆ†æ‰¹æ¬¡å­˜å…¥è³‡æ–™åº«...")
            for i in range(0, total_splits, batch_size):
                batch = final_splits[i : i + batch_size]
                self.vector_db.add_documents(batch)
                logging.info(
                    f"      -> å·²å­˜å…¥ {min(i + batch_size, total_splits)} / {total_splits} å€‹ç‰‡æ®µ..."
                )
            logging.info(f"âœ… æª” '{file_name}' å·²æˆåŠŸå­˜å…¥å‘é‡è³‡æ–™åº«ã€‚")
            self.update_ensemble_retriever(new_docs=final_splits)
        except Exception as e:
            logging.error(f"æ–‡ä»¶è™•ç†æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)
            raise
        finally:
            logging.info(f"ğŸ§¹ æ­£åœ¨æ¸…ç†æš«å­˜æª”: {file_path}")
            if os.path.exists(file_path):
                try:
                    os.remove(file_path)
                except OSError as e:
                    logging.error(f"æ¸…ç†æš«å­˜æª” {file_path} å¤±æ•—: {e}")

    def _agent_based_web_search(self, question: str):
        logging.info(f"ğŸ¤– (Agentæ¨¡å¼) å•Ÿå‹• DDGS é€šç”¨ç¶²è·¯æœå°‹: '{question}'")
        all_source_docs = []
        if not PLAYWRIGHT_AVAILABLE:
            logging.warning("Playwright æœªå®‰è£ï¼Œæœå°‹åŠŸèƒ½å—é™ã€‚")
            return []

        # --- ç¶²ç«™é»‘åå–® ---
        DOMAIN_BLACKLIST = [
            "zhihu.com",      # å¾ˆé›£çˆ¬å–ï¼Œç¶“å¸¸éœ€è¦ç™»å…¥
            "facebook.com",   # éœ€è¦ç™»å…¥
            "instagram.com",  # éœ€è¦ç™»å…¥
            "linkedin.com",   # éœ€è¦ç™»å…¥
            "medium.com",     # ç¶“å¸¸æœ‰ä»˜è²»ç‰†æˆ–ç™»å…¥æç¤º
        ]
        # ------------------------------------

        with sync_playwright() as playwright:
            browser: Browser = None
            try:
                browser = playwright.chromium.launch(headless=True)
                with DDGS() as ddgs:
                    # ç²å–æ›´å¤šçµæœä»¥ä¾¿éæ¿¾
                    search_results = ddgs.text(question, max_results=20, region="tw-zh")
                if not search_results: return []
                
                # --- [æ ¸å¿ƒä¿®æ”¹] éæ¿¾æ‰é»‘åå–®ä¸­çš„ç¶²å€ ---
                urls_to_browse = [
                    res["href"] for res in search_results 
                    if "href" in res and not any(blacklisted in res["href"] for blacklisted in DOMAIN_BLACKLIST)
                ][:10] # å–éæ¿¾å¾Œçš„å‰10å€‹
                # ------------------------------------------

                logging.info(f"   -> å·²éæ¿¾æ‰é»‘åå–®ç¶²ç«™ï¼Œæº–å‚™ç€è¦½ä»¥ä¸‹ {len(urls_to_browse)} å€‹ç¶²é ...")
                if not urls_to_browse:
                    logging.warning("   -> éæ¿¾å¾Œæ²’æœ‰å¯ç€è¦½çš„ç¶²é ã€‚")
                    return []

                for url in urls_to_browse:
                    if not url: continue
                    content = self._scrape_webpage_text(url, browser)
                    if "ç„¡æ³•çˆ¬å–" not in content and len(content) > 100: # æ–°å¢é•·åº¦åˆ¤æ–·ï¼Œéæ¿¾æ‰ç„¡ç”¨å…§å®¹
                        all_source_docs.append(Document(page_content=content, metadata={"source": f"ç¶²é ç€è¦½: {url}"}))
                
                return all_source_docs
            except Exception as e:
                logging.error(f"âŒ åœ¨ Agent æœå°‹éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
                return []
            finally:
                if browser: browser.close()
                logging.info("Agent æœå°‹ä»»å‹™å®Œæˆï¼ŒPlaywright è³‡æºå·²é‡‹æ”¾ã€‚")

    def _scrape_webpage_text(self, url: str, browser: Browser = None):
        if url.lower().endswith('.pdf'):
            if not PYMUPDF_AVAILABLE:
                return "ç„¡æ³•è™•ç† PDF æ–‡ä»¶ï¼Œå› ç‚º PyMuPDF æœªå®‰è£ã€‚"
            try:
                logging.info(f"ğŸ“„ (PyMuPDFæ¨¡å¼) æ­£åœ¨è™•ç† PDF ç¶²å€: {url}")
                headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'}
                response = requests.get(url, headers=headers, timeout=30, verify=False)
                response.raise_for_status()
                
                pdf_document = fitz.open(stream=response.content, filetype="pdf")
                text = "".join(page.get_text() for page in pdf_document)
                pdf_document.close()
                logging.info(f"âœ… (PyMuPDF) æˆåŠŸæå– PDF æ–‡å­—ï¼Œé•·åº¦: {len(text)} å­—å…ƒã€‚")
                return text
            except Exception as e:
                logging.error(f"âŒ (PyMuPDF) è™•ç† PDF æ™‚å¤±æ•—: {e}")
                return f"ç„¡æ³•çˆ¬å– PDFï¼ŒéŒ¯èª¤: {e}"

        if browser:
            logging.info(f"ğŸ•¸ï¸ (Playwrightæ¨¡å¼) æ­£åœ¨å˜—è©¦çˆ¬å–ç¶²å€: {url}")
            page = None
            try:
                if any(url.lower().endswith(ext) for ext in ['.doc', '.docx', '.zip', '.rar', '.xls', '.xlsx']):
                     raise Exception(f"æ–‡ä»¶é¡å‹ ({url})ï¼Œè·³é Playwrightã€‚")
                page = browser.new_page()
                page.goto(url, timeout=30000, wait_until='domcontentloaded')

                try:
                    page.wait_for_selector(
                        "main, article, #content, #main-content, .post-content, .article-body", 
                        timeout=10000
                    )
                    logging.info("   -> âœ… æˆåŠŸç­‰åˆ°é—œéµå…§å®¹å€å¡Šã€‚")
                except Exception as wait_e:
                    logging.warning(f"   -> âš ï¸ æœªèƒ½ç­‰åˆ°ç‰¹å®šå…§å®¹å€å¡Šï¼Œå°‡ç›´æ¥æŠ“å–ç¾æœ‰å…§å®¹ã€‚éŒ¯èª¤: {wait_e}")
                html_content = page.content()
                soup = BeautifulSoup(html_content, "html.parser")
                for element in soup(["script", "style", "nav", "footer", "aside", "header", "iframe", "form"]):
                    element.decompose()
                text = "\n".join(line.strip() for line in soup.get_text().splitlines() if line.strip())
                logging.info(f"âœ… (Playwright) æˆåŠŸç²å–ç¶²é æ–‡å­—ï¼Œé•·åº¦: {len(text)} å­—å…ƒã€‚")
                return text
            except Exception as e:
                logging.warning(f"âŒ (Playwright) å¤±æ•—: {e}ã€‚å°‡å›é€€è‡³ Requests æ¨¡å¼ã€‚")
            finally:
                if page and not page.is_closed():
                    page.close()
        
        logging.info(f"ğŸ•¸ï¸ (Requestsæ¨¡å¼) æ­£åœ¨å˜—è©¦çˆ¬å–ç¶²å€: {url}")
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'}
            response = requests.get(url, headers=headers, timeout=30, verify=False)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, "html.parser")
            for element in soup(["script", "style", "nav", "footer", "aside", "header", "iframe", "form"]):
                element.decompose()
            text = "\n".join(line.strip() for line in soup.get_text().splitlines() if line.strip())
            if not text:
                logging.warning(f"âš ï¸ (Requests) æœªèƒ½å¾ {url} æå–åˆ°ä»»ä½•æœ‰æ•ˆæ–‡å­—ã€‚")
            else:
                logging.info(f"âœ… (Requests) æˆåŠŸç²å–ç¶²é æ–‡å­—ï¼Œé•·åº¦: {len(text)} å­—å…ƒã€‚")
            return text
        except Exception as e:
            logging.error(f"âŒ (Requests) å¤±æ•—: {e}")
            return f"ç„¡æ³•çˆ¬å–ç¶²å€ï¼ŒéŒ¯èª¤: {e}"

    def _search_wikipedia(self, query: str):
        logging.info(f"ğŸ” (ç¶­çªç™¾ç§‘) æ­£åœ¨æœç´¢: '{query[:50].strip()}...'")
        try:
            wikipedia.set_lang("zh-tw")
            summary = wikipedia.summary(query, sentences=5, auto_suggest=False)
            return summary
        except Exception:
            return "ç„¡ç›¸é—œè³‡æ–™"

    def _get_rag_context(self, question: str, retrieval_query: str):
        all_source_docs, context_parts = [], []
        if self.use_scraper:
            if url_match := re.search(r"https?://[\S]+", question):
                url = url_match.group(0)
                web_content = self._scrape_webpage_text(url, browser=None)
                if "ç„¡æ³•çˆ¬å–" not in web_content:
                    doc = Document(page_content=web_content, metadata={"source": f"ç¶²é : {url}"})
                    all_source_docs.append(doc)
                    context_parts.append(f"ä¾†æºï¼š{doc.metadata['source']}\nå…§å®¹ï¼š\n{doc.page_content}")
        if self.use_wikipedia:
            wiki_content = self._search_wikipedia(question)
            if "ç„¡ç›¸é—œè³‡æ–™" not in wiki_content:
                doc = Document(page_content=wiki_content, metadata={"source": "ç¶­çªç™¾ç§‘"})
                all_source_docs.append(doc)
                context_parts.append(f"ä¾†æºï¼š{doc.metadata['source']}\nå…§å®¹ï¼š\n{doc.page_content}")
        if self.use_history:
            db_docs = self.ensemble_retriever.get_relevant_documents(retrieval_query)
            db_docs = [doc for doc in db_docs if doc.page_content != "start"]
            if db_docs:
                all_source_docs.extend(db_docs)
                context_parts.append("[ç›¸é—œè³‡æ–™åº«å…§å®¹]:\n" + "\n---\n".join([f"ä¾†æºï¼š{doc.metadata.get('source', 'æœªçŸ¥')}\nå…§å®¹ï¼š\n{doc.page_content}" for doc in db_docs]))
        return "\n\n".join(context_parts) if context_parts else "æ²’æœ‰å¯ç”¨çš„ä¸Šä¸‹æ–‡è³‡æ–™ã€‚", all_source_docs

    def ask(self, question: str, stream: bool = True):
        logging.info(f"\nğŸ¤” æ”¶åˆ°è«‹æ±‚ï¼Œå•é¡Œ: '{question}'")
        router_template = self.prompts.get('router')
        if not router_template:
            return self._stream_direct_message("ç³»çµ±éŒ¯èª¤ï¼šè·¯ç”±å™¨æœªé…ç½®ã€‚")
        router_prompt_string = router_template.template.replace('{question}', question)
        path, persona = "rag_query", "default_rag"
        try:
            raw_route_output = self.llm.invoke(router_prompt_string)
            logging.info(f"ğŸš¦ è·¯ç”±å™¨åŸå§‹æ±ºç­–: {raw_route_output}")
            if match := re.search(r'{\s*"path"\s*:\s*".*?",\s*"persona"\s*:\s*".*?"\s*}', raw_route_output, re.DOTALL):
                decision = json.loads(match.group(0))
                path, persona = decision.get("path", path), decision.get("persona", persona)
            else:
                logging.warning("ç„¡æ³•å¾è·¯ç”±å™¨è¼¸å‡ºè§£æ JSONï¼Œå°‡å˜—è©¦èˆŠçš„è§£ææ–¹æ³•ã€‚")
                decision_str = re.sub(r"<think>.*?</think>", "", raw_route_output, flags=re.DOTALL).strip().lower()
                if decision_str in ["rag_query", "web_search_query", "direct_answer_query", "general_conversation", "complex_project_query"]:
                    path = decision_str
            logging.info(f"ğŸš¦ è·¯ç”±å™¨æ¸…ç†å¾Œæ±ºç­– -> è·¯å¾‘: '{path}', è§’è‰²: '{persona}'")
        except Exception as e:
            logging.error(f"ğŸš¦ è·¯ç”±å™¨æ±ºç­–å¤±æ•—: {e}, å°‡èµ°é è¨­ RAG è·¯å¾‘ã€‚")
            path, persona = "rag_query", "default_rag"

        if path == "complex_project_query":
            return self._handle_complex_project(question, stream)
        
        main_prompt_template = self.prompts.get(persona, self.prompts.get('default_rag'))
        if not main_prompt_template:
            return self._stream_direct_message("ç³»çµ±éŒ¯èª¤ï¼šæ ¸å¿ƒæ¨¡æ¿ç¼ºå¤±ã€‚")

        if path in ["direct_answer_query", "general_conversation"]:
            logging.info(f"ğŸ’¬ èµ°ç›´æ¥/é€šç”¨å°è©±è·¯å¾‘ (ä½¿ç”¨è§’è‰²: {persona})...")
            prompt = main_prompt_template.format(context="æ²’æœ‰å¯ç”¨çš„ä¸Šä¸‹æ–‡è³‡æ–™ã€‚", question=question)
            return self.stream_and_save(question, prompt, [])
        
        if path == "web_search_query" and self.use_web_search:
            logging.info(f"ğŸŒ èµ°é€šç”¨ç¶²è·¯æœç´¢ RAG è·¯å¾‘ (ä½¿ç”¨è§’è‰²: {persona})...")
            keyword_chain = self.prompts['web_search_generation'] | self.llm | StrOutputParser()
            search_keywords = keyword_chain.invoke({"question": question}).strip()
            search_keywords = re.sub(r"<think>.*?</think>|\[.*?\]:", "", search_keywords, flags=re.DOTALL).strip()
            logging.info(f"ğŸ’¡ ç”Ÿæˆçš„ç¶²è·¯æœå°‹é—œéµå­—: '{search_keywords}'")
            search_docs = self._agent_based_web_search(search_keywords)
            if search_docs:
                context = "[ç¶²è·¯æœç´¢çµæœ]:\n" + "\n---\n".join([f"ä¾†æºï¼š{doc.metadata.get('source', 'ç¶²è·¯')}\nå…§å®¹ï¼š\n{doc.page_content}" for doc in search_docs])
                prompt = main_prompt_template.format(context=context, question=question)
                return self.stream_and_save(question, prompt, search_docs)
            else:
                return self._stream_direct_message("æŠ±æ­‰ï¼Œæˆ‘å˜—è©¦é€éç¶²è·¯æœå°‹ï¼Œä½†æœªèƒ½ç²å–åˆ°ç›¸é—œè³‡è¨Šã€‚")

        # Fallback to local RAG
        logging.info(f"ğŸ“š èµ°æœ¬åœ° RAG è·¯å¾‘ (ä½¿ç”¨è§’è‰²: {persona})...")
        query_chain = self.prompts['query_expansion'] | self.llm | StrOutputParser()
        expanded_query = query_chain.invoke({"original_query": question}).strip()
        retrieval_query = f"{question}\n{expanded_query}"
        context, source_docs = self._get_rag_context(question, retrieval_query)
        prompt = main_prompt_template.format(context=context, question=question)
        return self.stream_and_save(question, prompt, source_docs)

    def _write_report_from_blueprint(self, blueprint_json: dict, full_context: str) -> str:
        logging.info("â¡ï¸ é€²å…¥æ–¹æ¡ˆäºŒï¼šç« ç¯€æ’°å¯«å™¨å·¥ä½œæµ...")
        
        final_report_parts = []
        chapter_writer_template = self.prompts.get('chapter_writer')
        if not chapter_writer_template:
            raise ValueError("é—œéµçš„ 'chapter_writer.txt' æ¨¡æ¿æœªæ‰¾åˆ°ï¼")

        report_body = blueprint_json.get("report_blueprint", {})

        for chapter_key, chapter_data in report_body.items():
            try:
                title = chapter_data.get('title', chapter_key.replace('_', ' ').title()) if isinstance(chapter_data, dict) else chapter_key.replace('_', ' ').title()
                key_points = chapter_data.get('content', 'ç„¡ç‰¹å®šè¦é»ï¼Œè«‹æ ¹æ“šä¸»é¡Œè‡ªç”±ç™¼æ®ã€‚') if isinstance(chapter_data, dict) else chapter_data
                key_points_str = "\n".join([f"- {item}" if isinstance(item, str) else f"- {json.dumps(item, ensure_ascii=False)}" for item in key_points]) if isinstance(key_points, list) else str(key_points)

                logging.info(f"   -> æ­£åœ¨ç‚ºç« ç¯€ '{title}' æ’°å¯«å…§å®¹...")
                
                chapter_prompt = chapter_writer_template.format(
                    context=full_context,
                    chapter_title=title,
                    key_points=key_points_str
                )
                
                raw_chapter_content = self.llm.invoke(chapter_prompt)
                cleaned_content = re.sub(r"<think>.*?</think>", "", raw_chapter_content, flags=re.DOTALL).strip()
                match = re.search(r"(#+\s.*)", cleaned_content, re.DOTALL)
                if match:
                    final_chapter_content = match.group(1)
                else:
                    final_chapter_content = cleaned_content
                final_report_parts.append(f"{final_chapter_content.strip()}\n\n")

            except Exception as e:
                logging.error(f"âŒ åœ¨æ’°å¯«ç« ç¯€ '{chapter_key}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                final_report_parts.append(f"## {chapter_key.replace('_', ' ').title()}\n\n_{{æ­¤ç« ç¯€ç”Ÿæˆå¤±æ•—ï¼ŒéŒ¯èª¤è¨Šæ¯: {e}}}_ \n\n")
        
        logging.info("âœ… æ‰€æœ‰ç« ç¯€æ’°å¯«å®Œç•¢ï¼Œæ­£åœ¨çµ„è£æœ€çµ‚å ±å‘Šã€‚")
        return "".join(final_report_parts)

    def _handle_complex_project(self, question: str, stream: bool = True):
        try:
            logging.info("ğŸš€ å•Ÿå‹•å°ˆå®¶å°çµ„å·¥ä½œæµ (Blueprint + Chapter Writer ç‰ˆæœ¬)...")
            yield f"data: {json.dumps({'type': 'status', 'message': 'æ­¥é©Ÿ 1/4: æ­£åœ¨æ‹†è§£ä»»å‹™...'})}\n\n"
            
            task_decomp_template = self.prompts.get('task_decomposition')
            task_decomp_prompt_string = task_decomp_template.template.replace('{question}', question)
            sub_tasks_str = self.llm.invoke(task_decomp_prompt_string)
            matches = re.findall(r"^\s*\d+\.\s*(.*)", sub_tasks_str, re.MULTILINE)
            if not matches: raise ValueError("ç„¡æ³•å¾ LLM è¼¸å‡ºä¸­æå–æœ‰æ•ˆå­ä»»å‹™ã€‚")
            sub_tasks = [match.strip() for match in matches if match.strip()]
            logging.info(f"âœ… æ¸…ç†å¾Œåˆ†è§£å‡ºçš„å­ä»»å‹™: {sub_tasks}")

            research_memos = []
            all_source_documents = []
            analyst_template = self.prompts.get('market_analyst')
            keyword_gen_chain = self.prompts['web_search_generation'] | self.llm | StrOutputParser()

            for i, task in enumerate(sub_tasks):
                yield f"data: {json.dumps({'type': 'status', 'message': f'æ­¥é©Ÿ 2.{i+1}/{len(sub_tasks)}: æ­£åœ¨ç ”ç©¶ \"{task[:20]}...\"'})}\n\n"
                search_keywords_raw = keyword_gen_chain.invoke({"question": question, "task": task})
                search_keywords = re.sub(r"<think>.*?</think>|\[.*?\]:", "", search_keywords_raw, flags=re.DOTALL).strip()
                logging.info(f"   -> ç‚ºå­ä»»å‹™ '{task}' ç”Ÿæˆçš„æ¸…ç†å¾Œé—œéµè©: {search_keywords}")
                
                search_docs = self._agent_based_web_search(search_keywords)
                if search_docs:
                    all_source_documents.extend(search_docs)
                context = "\n---\n".join([f"ä¾†æºï¼š{doc.metadata.get('source', 'ç¶²è·¯')}\nå…§å®¹ï¼š\n{doc.page_content}" for doc in search_docs]) if search_docs else "æ³¨æ„ï¼šæœªèƒ½å¾ç¶²è·¯æ‰¾åˆ°ç›¸é—œè³‡æ–™ã€‚"              
                analyst_prompt = analyst_template.template.replace('{context}', context).replace('{question}', task)
                memo = self.llm.invoke(analyst_prompt)
                research_memos.append(f"--- ç ”ç©¶å‚™å¿˜éŒ„ for '{task}' ---\n{memo}\n")
                logging.info(f"   -> âœ… å®Œæˆäº†å­ä»»å‹™ '{task}' çš„ç ”ç©¶å‚™å¿˜éŒ„ã€‚")

            yield f"data: {json.dumps({'type': 'status', 'message': 'æ­¥é©Ÿ 3/4: æ­£åœ¨ç”Ÿæˆå ±å‘Šå¤§ç¶±...'})}\n\n"
            final_context = "\n".join(research_memos)
            blueprint_gen_template = self.prompts.get('answer_blueprint_generator')
            base_blueprint_prompt = blueprint_gen_template.template.replace('{context}', final_context).replace('{question}', question)
            blueprint_json = None
            max_retries = 3; last_error = ""
            for attempt in range(max_retries):
                blueprint_str = self.llm.invoke(base_blueprint_prompt if attempt == 0 else base_blueprint_prompt + f"\n[ä¿®æ­£æŒ‡ä»¤]: ä¸Šæ¬¡å¤±æ•—: '{last_error}'. è«‹ä¿®æ­£ä¸¦åªè¼¸å‡º JSONã€‚")
                try:
                    match = re.search(r"```json\s*(\{.*?\})\s*```", blueprint_str, re.DOTALL) or re.search(r"(\{.*\})", blueprint_str, re.DOTALL)
                    if not match: raise json.JSONDecodeError("è¼¸å‡ºä¸­æ‰¾ä¸åˆ° JSONã€‚", blueprint_str, 0)
                    blueprint_json = json.loads(match.group(1))
                    logging.info(f"âœ… æˆåŠŸè§£æå›ç­”å¤§ç¶± JSONã€‚")
                    break
                except json.JSONDecodeError as e:
                    last_error = str(e); logging.warning(f"âŒ è§£æ JSON å¤±æ•— (å˜—è©¦ {attempt + 1}): {e}")
            if blueprint_json is None: raise ValueError(f"åœ¨ {max_retries} æ¬¡å˜—è©¦å¾Œä»ç„¡æ³•è§£æ JSONã€‚")
            
            yield f"data: {json.dumps({'type': 'status', 'message': 'æ­¥é©Ÿ 4/4: æ­£åœ¨æ ¹æ“šå¤§ç¶±é€ç« æ’°å¯«å ±å‘Š...'})}\n\n"            
            final_answer = self._write_report_from_blueprint(blueprint_json, final_context)
            
            yield f"data: {json.dumps({'type': 'status', 'message': 'å ±å‘Šç”Ÿæˆå®Œç•¢ï¼'})}\n\n"
            for chunk in self._stream_and_save_final_answer(question, final_answer, all_source_documents):
                yield chunk

        except Exception as e:
            logging.error(f"âŒ åœ¨å°ˆå®¶å°çµ„å·¥ä½œæµä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)
            yield f"data: {json.dumps({'type': 'error', 'error': f'æŠ±æ­‰ï¼ŒåŸ·è¡Œè¤‡é›œå°ˆæ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}'})}\n\n"
        finally:
            yield f"data: [DONE]\n\n"

    def _generate_markdown_from_blueprint(self, blueprint_node: Any, level: int = 1) -> str:
        """
        [V5]
        èƒ½ç†è§£ 'title', 'content', 'recommendation', 'tool_type' ç­‰ç‰¹å®šéµçš„èªç¾©ï¼Œ
        ç”Ÿæˆæ›´ç²¾ç¾ã€æ›´ç¬¦åˆäººé¡é–±è®€ç¿’æ…£çš„ Markdownã€‚
        """
        markdown_parts = []
        if isinstance(blueprint_node, dict):
            if 'title' in blueprint_node:
                markdown_parts.append(f"{'#' * level} {blueprint_node['title']}\n\n")
                for key, value in blueprint_node.items():
                    if key != 'title':
                        markdown_parts.append(self._generate_markdown_from_blueprint(value, level + 1))
            elif 'tool_type' in blueprint_node and 'recommendation' in blueprint_node:
                tool_type = blueprint_node.get('tool_type', 'å·¥å…·')
                recommendation = blueprint_node.get('recommendation', '')
                markdown_parts.append(f"- **{tool_type}:** {recommendation}\n")
            else:
                for key, value in blueprint_node.items():
                    title = key.replace('_', ' ').title()
                    markdown_parts.append(f"{'#' * level} {title}\n\n")
                    markdown_parts.append(self._generate_markdown_from_blueprint(value, level + 1))

        elif isinstance(blueprint_node, list):
            for item in blueprint_node:
                markdown_parts.append(self._generate_markdown_from_blueprint(item, level))
            markdown_parts.append("\n")

        else:
            if level > 2:
                 markdown_parts.append(f"- {str(blueprint_node)}\n")
            else: 
                 markdown_parts.append(f"{str(blueprint_node)}\n\n")
            
        return "".join(markdown_parts)

    def _stream_direct_message(self, message: str):
        try:
            yield f"data: {json.dumps({'type': 'content', 'content': message})}\n\n"
        except Exception as e:
            yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"
        finally:
            yield f"data: [DONE]\n\n"

    def _stream_and_save_final_answer(self, question, final_answer, source_documents):
        try:
            if source_documents:
                yield f"data: {json.dumps({'type': 'sources', 'data': [{'page_content': doc.page_content, 'metadata': doc.metadata} for doc in source_documents]})}\n\n"
            for i in range(0, len(final_answer), 30):
                chunk = final_answer[i:i+30]
                yield f"data: {json.dumps({'type': 'content', 'content': chunk})}\n\n"
                time.sleep(0.02)
            if final_answer.strip():
                self.save_qa(question, final_answer)
        except Exception as e:
            logging.error(f"âŒ åœ¨æœ€çµ‚ç­”æ¡ˆä¸²æµéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"
        finally:
            yield f"data: [DONE]\n\n"

    def stream_and_save(self, question, prompt, source_documents):
        full_answer = ""
        try:
            if source_documents:
                yield f"data: {json.dumps({'type': 'sources', 'data': [{'page_content': doc.page_content, 'metadata': doc.metadata} for doc in source_documents]})}\n\n"
            for chunk in self.llm.stream(prompt):
                full_answer += chunk
                yield f"data: {json.dumps({'type': 'content', 'content': chunk})}\n\n"
            if full_answer.strip():
                self.save_qa(question, full_answer)
        except Exception as e:
            logging.error(f"âŒ åœ¨ä¸²æµç”Ÿæˆéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"
        finally:
            yield f"data: [DONE]\n\n"

    def save_qa(self, question, answer):
        if not answer or not answer.strip():
            return
        qa_pair_content = f"å•é¡Œ: {question}\nå›ç­”: {answer}"
        metadata = {"source": "conversation", "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
        new_doc = Document(page_content=qa_pair_content, metadata=metadata)
        self.vector_db.add_documents([new_doc])
        self.update_ensemble_retriever(new_docs=[new_doc])
        logging.info("   -> å°è©±æ­·å²å­˜å„²ä¸¦åŒæ­¥è‡³æ··åˆç´¢å¼•å®Œç•¢ï¼")
