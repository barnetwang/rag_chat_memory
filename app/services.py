import os
import json
import re
import requests
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import logging
import urllib3
import time
from typing import Any, Generator
from pathlib import Path
import sqlite3
import threading
from pydantic import BaseModel, Field
from typing import List, Literal
from concurrent.futures import ThreadPoolExecutor, as_completed

# LangChain Imports
from ddgs import DDGS
from langchain_ollama.llms import OllamaLLM
from langchain_core.documents import Document
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import PydanticOutputParser, StrOutputParser
from langchain_core.exceptions import OutputParserException


# Pydantic Models for structured output
class QueryAssessment(BaseModel):
    assessment: Literal["specific_topic", "broad_concept"] = Field(
        description="è©•ä¼°ä½¿ç”¨è€…å•é¡Œæ˜¯ 'specific_topic' (å…·é«”ä¸»é¡Œ) é‚„æ˜¯ 'broad_concept' (å»£æ³›æ¦‚å¿µ)ã€‚"
    )


class QueryClarification(BaseModel):
    intro_sentence: str = Field(description="å¼•å°ä½¿ç”¨è€…é¸æ“‡çš„é–‹å ´ç™½ã€‚")
    options: List[str] = Field(
        description="æä¾›çµ¦ä½¿ç”¨è€…çš„ 3 åˆ° 4 å€‹å…·é«”ç ”ç©¶æ–¹å‘é¸é …ï¼Œæœ€å¾Œä¸€å€‹é¸é …å¿…é ˆæ˜¯ã€ä»¥ä¸Šå…¨éƒ¨ï¼Œç‚ºæˆ‘ç”Ÿæˆä¸€ä»½å…¨é¢çš„ç¶œåˆå ±å‘Šã€ã€‚")


class TaskDecomposition(BaseModel):
    sub_tasks: List[str] = Field(
        description="å°‡è¤‡é›œç ”ç©¶ç›®æ¨™åˆ†è§£ç‚º 2 åˆ° 4 å€‹æ¸…æ™°ã€å¯åŸ·è¡Œçš„ç ”ç©¶å­å•é¡Œåˆ—è¡¨ã€‚")


class SearchStrategy(BaseModel):
    search_queries: List[str] = Field(
        description="é‡å°ç ”ç©¶å­ä»»å‹™ç”Ÿæˆçš„ 3 åˆ° 5 å€‹å…·é«”ã€å¤šæ¨£åŒ–çš„æœå°‹å¼•æ“é—œéµå­—åˆ—è¡¨ã€‚")


class SearchResult(BaseModel):
    id: int
    title: str
    url: str


class AssessedSearchResult(BaseModel):
    id: int
    is_relevant: bool


class SearchResultAssessmentList(BaseModel):
    results: List[AssessedSearchResult]


class ContentRelevanceResult(BaseModel):
    is_truly_relevant: bool
    summary: str = Field(
        description="A brief summary of the content if relevant, or a reason for irrelevance.")


class ClaimList(BaseModel):
    claims: List[str] = Field(
        description="A list of verifiable, factual claims extracted from a report.")


class FactCheckResult(BaseModel):
    is_verified: bool = Field(
        description="Whether the statement is verified by the source material.")
    supporting_evidence: str = Field(
        description="The snippet from the source material that supports the statement, or 'null' if not verifiable.")


class Chapter(BaseModel):
    title: str = Field(description="ç« ç¯€æ¨™é¡Œï¼Œå¿…é ˆä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚")
    description: str = Field(description="ç« ç¯€å…§å®¹çš„ç°¡è¦æè¿°ï¼Œå¦‚æœåŒ…å«è¡¨æ ¼æ•¸æ“šï¼Œè«‹è¨»æ˜ã€ä½¿ç”¨è¡¨æ ¼å‘ˆç¾ã€ã€‚")
    key_points: List[str] = Field(description="ä¸€å€‹åŒ…å«æœ¬ç« ç¯€éœ€è¦é—¡è¿°çš„ 3 åˆ° 5 å€‹æ ¸å¿ƒè¦é»çš„åˆ—è¡¨ã€‚")


class ReportBlueprint(BaseModel):
    title: str = Field(description="å ±å‘Šçš„ç¸½æ¨™é¡Œï¼Œå¿…é ˆä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚")
    chapters: List[Chapter] = Field(description="å ±å‘Šçš„ç« ç¯€åˆ—è¡¨ï¼Œæ¯å€‹ç« ç¯€åŒ…å«æ¨™é¡Œå’Œæè¿°ã€‚")


try:
    from playwright.sync_api import sync_playwright, Browser
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    logging.warning(
        "Playwright æœªå®‰è£ã€‚çˆ¬èŸ²åŠŸèƒ½å°‡åƒ…ä½¿ç”¨ requestsã€‚å»ºè­°åŸ·è¡Œ 'pip install playwright && playwright install' ä¾†å¢å¼·çˆ¬èŸ²èƒ½åŠ›ã€‚")

try:
    import fitz
    PYMUPDF_AVAILABLE = True
except ImportError:
    PYMUPDF_AVAILABLE = False
    logging.warning("PyMuPDF æœªå®‰è£ã€‚PDF è™•ç†åŠŸèƒ½å°‡ä¸å¯ç”¨ã€‚å»ºè­°åŸ·è¡Œ 'pip install PyMuPDF'ã€‚")

logging.basicConfig(level=logging.INFO,
                    format="%(asctime)s - %(levelname)s - %(message)s")
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


class ClientDisconnectedError(Exception):
    """Custom exception for client disconnects."""
    pass


def get_ollama_models(ollama_base_url="http://localhost:11434"):
    try:
        response = requests.get(f"{ollama_base_url}/api/tags")
        response.raise_for_status()
        return [model["name"] for model in response.json().get("models", [])]
    except Exception as e:
        logging.error(f"å­˜å– Ollama æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return []


class ConversationalRAG:
    def __init__(self, config: dict):
        self.config = config
        self.use_web_search = True
        self.ollama_base_url = self.config["OLLAMA_BASE_URL"]
        self.prompts = {}
        self.active_tasks = {}
        self.blueprint_cache = {}
        self.db_path = "database.db"
        self._init_database()

        logging.info("æ­£åœ¨è¨­ç½® LLM...")
        try:
            self.llm = OllamaLLM(
                model=self.config["llm_model"],
                base_url=self.ollama_base_url,
                request_timeout=self.config.get('OLLAMA_REQUEST_TIMEOUT')
            )
            self.llm.invoke("Hi", stop=["Hi"])
            self.current_llm_model = self.config["llm_model"]
        except Exception as e:
            logging.error(f"è¨­ç½® LLM å¤±æ•—: {e}")
            raise

        self._init_prompts()
        logging.info("âœ… ç³»çµ±å·²å°±ç·’ (ä½¿ç”¨ DDGS ä½œç‚ºæœå°‹å¼•æ“)ã€‚")

    def close(self):
        pass

    def _init_database(self):
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    title TEXT NOT NULL,
                    report TEXT NOT NULL,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            conn.commit()
            conn.close()
            logging.info(f"âœ… è³‡æ–™åº« '{self.db_path}' å·²æˆåŠŸåˆå§‹åŒ–ã€‚")
        except Exception as e:
            logging.error(f"âŒ åˆå§‹åŒ–è³‡æ–™åº«å¤±æ•—: {e}")
            raise

    def add_history_entry(self, title: str, report: str) -> int:
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute(
            "INSERT INTO history (title, report) VALUES (?, ?)", (title, report))
        entry_id = cursor.lastrowid
        conn.commit()
        conn.close()
        logging.info(f"ğŸ’¾ å·²å°‡å ±å‘Š '{title[:20]}...' å„²å­˜è‡³æ­·å²ç´€éŒ„ (ID: {entry_id})ã€‚")
        return entry_id

    def get_history_list(self) -> list:
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        cursor.execute(
            "SELECT id, title, timestamp FROM history ORDER BY timestamp DESC")
        entries = [dict(row) for row in cursor.fetchall()]
        conn.close()
        return entries

    def get_history_entry(self, entry_id: int) -> dict:
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        cursor.execute("SELECT * FROM history WHERE id = ?", (entry_id,))
        entry = cursor.fetchone()
        conn.close()
        return dict(entry) if entry else None

    def delete_history_entry(self, entry_id: int) -> bool:
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("DELETE FROM history WHERE id = ?", (entry_id,))
        conn.commit()
        deleted_rows = cursor.rowcount
        conn.close()
        if deleted_rows > 0:
            logging.info(f"ğŸ—‘ï¸ å·²å¾æ­·å²ç´€éŒ„ä¸­åˆªé™¤ ID: {entry_id}ã€‚")
            return True
        return False

    def _load_all_prompts(self, directory: str) -> dict:
        prompts = {}
        base_dir = os.path.dirname(os.path.abspath(__file__))
        prompts_dir = os.path.join(base_dir, directory)
        if not os.path.isdir(prompts_dir):
            return {}
        logging.info(f"ğŸ” æ­£åœ¨å¾ '{prompts_dir}' åŠ è¼‰ Prompt æ¨¡æ¿...")
        for filename in os.listdir(prompts_dir):
            if filename.endswith(".txt"):
                filepath = os.path.join(prompts_dir, filename)
                with open(filepath, 'r', encoding='utf-8') as f:
                    prompt_name = os.path.splitext(filename)[0]
                    prompts[prompt_name] = PromptTemplate.from_template(
                        f.read(), template_format="f-string")
                    logging.info(f"   -> å·²åŠ è¼‰: {prompt_name}")
        return prompts

    def _init_prompts(self):
        base_prompts = self._load_all_prompts("prompts")
        self.prompts = {}

        self.parsers = {
            "query_assessor": PydanticOutputParser(pydantic_object=QueryAssessment),
            "query_clarifier": PydanticOutputParser(pydantic_object=QueryClarification),
            "task_decomposition": PydanticOutputParser(pydantic_object=TaskDecomposition),
            "search_strategist": PydanticOutputParser(pydantic_object=SearchStrategy),
            "answer_blueprint_generator": PydanticOutputParser(pydantic_object=ReportBlueprint),
            "search_result_assessor": PydanticOutputParser(pydantic_object=SearchResultAssessmentList),
            "content_relevance_filter": PydanticOutputParser(pydantic_object=ContentRelevanceResult),
            "claim_extractor": PydanticOutputParser(pydantic_object=ClaimList),
            "fact_checker": PydanticOutputParser(pydantic_object=FactCheckResult),
        }

        for prompt_name, parser in self.parsers.items():
            if prompt_name in base_prompts:
                template_string = base_prompts[prompt_name].template
                self.prompts[prompt_name] = PromptTemplate(
                    template=template_string + "\n{format_instructions}",
                    input_variables=base_prompts[prompt_name].input_variables,
                    partial_variables={
                        "format_instructions": parser.get_format_instructions()},
                    template_format="f-string"
                )
                logging.info(f"   -> å·²ç‚º {prompt_name} æ³¨å…¥ Pydantic æ ¼å¼æŒ‡ä»¤ã€‚")
            else:
                logging.warning(f"âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ° {prompt_name} çš„åŸºç¤ Prompt æ¨¡æ¿ã€‚")

        for prompt_name, prompt_template in base_prompts.items():
            if prompt_name not in self.prompts:
                self.prompts[prompt_name] = prompt_template
                logging.info(f"   -> å·²åŠ è¼‰é Pydantic Prompt: {prompt_name}")

    def set_llm_model(self, model_name: str):
        logging.info(f"\nğŸ”„ æ­£åœ¨åˆ‡æ› LLM æ¨¡å‹è‡³: {model_name}")
        try:
            self.llm = OllamaLLM(
                model=model_name, base_url=self.ollama_base_url)
            self.llm.invoke("Hi", stop=["Hi"])
            self.current_llm_model = model_name
            logging.info(f"âœ… LLM æ¨¡å‹æˆåŠŸåˆ‡æ›ç‚º: {self.current_llm_model}")
            return True
        except Exception as e:
            logging.error(f"åˆ‡æ› LLM æ¨¡å‹å¤±æ•—: {e}")
            return False

    def set_web_search(self, enabled: bool):
        logging.info(f"ğŸ”„ å°‡ç¶²è·¯æœå°‹è¨­ç½®ç‚º: {'å•Ÿç”¨' if enabled else 'åœç”¨'}")
        self.use_web_search = enabled
        return True

    def _agent_based_web_search(self, question: str, task: str):
        logging.info(f"ğŸ¤– (Agentæ¨¡å¼) å•Ÿå‹• DDGS é€šç”¨ç¶²è·¯æœå°‹: '{question}'")
        all_source_docs = []

        assessor_chain = self.prompts.get(
            "search_result_assessor") | self.llm | self.parsers["search_result_assessor"]
        content_filter_chain = self.prompts.get(
            "content_relevance_filter") | self.llm | self.parsers["content_relevance_filter"]

        DOMAIN_BLACKLIST = ["zhihu.com", "facebook.com",
                            "instagram.com", "linkedin.com", "medium.com"]

        try:
            with DDGS() as ddgs:
                search_results = list(
                    ddgs.text(question, max_results=30, region="tw-zh"))

            if not search_results:
                logging.warning(
                    f"   -> DDGS for query '{question}' returned no results.")
                return []

            logging.info(
                f"   -> DDGS for query '{question}' returned {len(search_results)} results. Now performing Layer 1 (Batch Relevance Scan)...")

            filtered_results = [res for res in search_results if "href" in res and res.get(
                "title") and not any(blacklisted in res["href"] for blacklisted in DOMAIN_BLACKLIST)]
            input_list = [SearchResult(id=i, title=res["title"], url=res["href"])
                          for i, res in enumerate(filtered_results)]

            try:
                assessment_list_obj = assessor_chain.invoke(
                    {"task": task, "search_results_json": json.dumps([r.dict() for r in input_list])})
                relevant_ids = {
                    res.id for res in assessment_list_obj.results if res.is_relevant}
                urls_to_browse = [
                    res.url for res in input_list if res.id in relevant_ids]
                logging.info(
                    f"   -> Layer 1 Scan complete. {len(urls_to_browse)} URLs passed initial relevance filter.")
            except Exception as e:
                logging.error(
                    f"   -> âš ï¸ Layer 1 (Batch Relevance Scan) failed: {e}. Skipping filtering.")
                urls_to_browse = [res.url for res in input_list][:10]

            def scrape_and_verify_url(url):
                try:
                    with sync_playwright() as p:
                        browser = p.chromium.launch(headless=True)
                        try:
                            content = self._scrape_webpage_text(url, browser)
                            if "ç„¡æ³•çˆ¬å–" in content or len(content) < 100:
                                logging.warning(
                                    f"   -> Skipping {url} due to insufficient content.")
                                return None

                            filter_result = content_filter_chain.invoke(
                                {"task": task, "content": content})
                            if filter_result.is_truly_relevant:
                                logging.info(f"   -> âœ… DEEPLY RELEVANT: {url}")
                                return Document(page_content=content, metadata={"source": f"ç¶²é ç€è¦½: {url}"})
                            else:
                                logging.info(
                                    f"   -> âŒ CONTENT IRRELEVANT: {url}. Reason: {filter_result.summary}")
                                return None
                        finally:
                            browser.close()
                except Exception as e:
                    logging.error(
                        f"   -> âš ï¸ Error during scrape_and_verify_url for {url}: {e}")
                    return None

            logging.info(
                f"   -> Now performing Layer 2 (Deep Content Verification) on {len(urls_to_browse)} URLs in parallel...")
            with ThreadPoolExecutor(max_workers=5) as executor:
                future_to_url = {executor.submit(
                    scrape_and_verify_url, url): url for url in urls_to_browse}
                for future in as_completed(future_to_url):
                    result_doc = future.result()
                    if result_doc:
                        all_source_docs.append(result_doc)

            logging.info(
                f"   -> Agent æœå°‹ '{question}' å®Œæˆï¼Œå…±æ‰¾åˆ° {len(all_source_docs)} ä»½é«˜å“è³ªæ–‡ä»¶ã€‚")
            return all_source_docs
        except Exception as e:
            logging.error(f"âŒ åœ¨ Agent æœå°‹éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return []

    def _scrape_webpage_text(self, url: str, browser: Browser):
        if url.lower().endswith('.pdf'):
            if not PYMUPDF_AVAILABLE:
                return "ç„¡æ³•è™•ç† PDF æ–‡ä»¶ï¼Œå› ç‚º PyMuPDF æœªå®‰è£ã€‚"
            try:
                logging.info(f"ğŸ“„ (PyMuPDFæ¨¡å¼) æ­£åœ¨è™•ç† PDF ç¶²å€: {url}")
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'}
                response = requests.get(
                    url, headers=headers, timeout=30, verify=False)
                response.raise_for_status()
                with fitz.open(stream=response.content, filetype="pdf") as pdf_document:
                    text = "".join(page.get_text() for page in pdf_document)
                logging.info(f"âœ… (PyMuPDF) æˆåŠŸæå– PDF æ–‡å­—ï¼Œé•·åº¦: {len(text)} å­—å…ƒã€‚")
                return text
            except Exception as e:
                logging.error(f"âŒ (PyMuPDF) è™•ç† PDF æ™‚å¤±æ•—: {e}")
                return f"ç„¡æ³•çˆ¬å– PDFï¼ŒéŒ¯èª¤: {e}"

        logging.info(f"ğŸ•¸ï¸ (Playwrightæ¨¡å¼) æ­£åœ¨å˜—è©¦çˆ¬å–ç¶²å€: {url}")
        page = None
        try:
            if any(url.lower().endswith(ext) for ext in ['.doc', '.docx', '.zip', '.rar', '.xls', '.xlsx']):
                return "ç„¡æ³•çˆ¬å–ç¶²ç«™ï¼ŒéŒ¯èª¤: ä¸æ”¯æŒçš„æ–‡ä»¶é¡å‹"
            page = browser.new_page()
            page.goto(url, timeout=30000, wait_until='domcontentloaded')
            try:
                page.wait_for_selector(
                    "main, article, #content, #main-content, .post-content, .article-body", timeout=10000)
                logging.info("   -> âœ… æˆåŠŸç­‰åˆ°é—œéµå…§å®¹å€å¡Šã€‚")
            except Exception as wait_e:
                logging.warning(f"   -> âš ï¸ æœªèƒ½ç­‰åˆ°ç‰¹å®šå…§å®¹å€å¡Šï¼Œå°‡ç›´æ¥æŠ“å–ç¾æœ‰å…§å®¹ã€‚éŒ¯èª¤: {wait_e}")
            html_content = page.content()
            soup = BeautifulSoup(html_content, "html.parser")
            for element in soup(["script", "style", "nav", "footer", "aside", "header", "iframe", "form"]):
                element.decompose()
            text = "\n".join(line.strip()
                             for line in soup.get_text().splitlines() if line.strip())
            if len(text) < 300:
                logging.warning(
                    f"âš ï¸ (Playwright) ä» {url} æå–çš„æœ‰æ•ˆæ–‡å­—éå°‘ ({len(text)} å­—å…ƒ)ï¼Œå¯èƒ½ä¸æ˜¯ä¸»è¦å…§å®¹ã€‚")
            logging.info(f"âœ… (Playwright) æˆåŠŸç²å–ç¶²é æ–‡å­—ï¼Œé•·åº¦: {len(text)} å­—å…ƒã€‚")
            return text
        except Exception as e:
            logging.warning(f"âŒ (Playwright) å¤±æ•—: {e}ã€‚å°‡å›é€€è‡³ Requests æ¨¡å¼ã€‚")
            return "ç„¡æ³•çˆ¬å–ç¶²ç«™ï¼ŒéŒ¯èª¤: Playwright é€£æ¥å¤±æ•—"
        finally:
            if page and not page.is_closed():
                page.close()

    def _perform_final_cleanup(self, task_id: str):
        if task_id and task_id in self.active_tasks:
            logging.info(f"ğŸ§¹ ä»»å‹™ {task_id} æ­£åœ¨æœ€çµ‚è¨»éŠ·ã€‚")
            del self.active_tasks[task_id]

    def _send_done_signal(self) -> Generator[str, None, None]:
        yield f"data: {json.dumps({'type': 'done', 'content': '[DONE]'})}\n\n"
        logging.info("âœ… è³‡æ–™æµå·²æ­£å¸¸é—œé–‰ã€‚")

    def _handle_clarification(self, question: str, task_id: str) -> Generator[str, None, None]:
        logging.info(f"ğŸ’¬ KAIZEN è·¯ç”±ï¼šå•é¡Œéæ–¼å¯¬æ³›ï¼Œå•Ÿå‹•å•é¡Œæ¾„æ¸…æµç¨‹ã€‚")
        clarifier_template = self.prompts.get("query_clarifier")
        if not clarifier_template:
            raise ValueError("query_clarifier.txt æ¨¡æ¿æœªæ‰¾åˆ°ï¼")

        clarifier_chain = clarifier_template | self.llm | self.parsers["query_clarifier"]
        clarification_obj = clarifier_chain.invoke({"question": question})

        clarification_json = clarification_obj.dict()
        yield f"data: {json.dumps({'type': 'clarification', 'data': clarification_json})}\n\n"

    def _stream_research_and_blueprint(self, question: str, stream: bool, task_id: str) -> Generator[str, None, None]:
        logging.info(f"ğŸš€ å•Ÿå‹• [ç¬¬ä¸€éšæ®µ] å°ˆå®¶å°çµ„å·¥ä½œæµ (Task ID: {task_id})...")
        start_time = time.time()

        def send_event(event_data):
            try:
                yield event_data
            except GeneratorExit:
                raise ClientDisconnectedError(
                    f"Client disconnected during task {task_id}")

        try:
            # --- å¿ƒè·³æ©Ÿåˆ¶åˆå§‹åŒ– ---
            last_heartbeat = time.time()
            HEARTBEAT_INTERVAL = 15  # seconds

            def yield_heartbeat_if_needed():
                nonlocal last_heartbeat
                if time.time() - last_heartbeat > HEARTBEAT_INTERVAL:
                    # SSE è¨»è§£æ ¼å¼ï¼Œå®¢æˆ¶ç«¯ JS æœƒå¿½ç•¥å®ƒï¼Œä½†èƒ½ä¿æŒ TCP é€£æ¥
                    yield f"data: {json.dumps({'type': 'heartbeat'})}\n\n"
                    last_heartbeat = time.time()
                    logging.debug(f"Sent heartbeat for task {task_id}")
            # --- å¿ƒè·³æ©Ÿåˆ¶åˆå§‹åŒ–çµæŸ ---

            def check_cancellation():
                if task_id and self.active_tasks.get(task_id, {}).get("is_cancelled"):
                    raise InterruptedError(
                        f"Task {task_id} cancelled by user.")

            check_cancellation()
            yield from send_event(f"data: {json.dumps({'type': 'status', 'message': 'æ­¥é©Ÿ 1/3: æ­£åœ¨æ‹†è§£ç ”ç©¶ä»»å‹™...'})}\n\n")
            yield from yield_heartbeat_if_needed() # <<-- åŠ å…¥å¿ƒè·³æª¢æŸ¥é»

            task_decomp_template = self.prompts.get("task_decomposition")
            task_decomp_chain = task_decomp_template | self.llm | self.parsers[
                "task_decomposition"]
            task_decomp_obj = task_decomp_chain.invoke({"question": question})
            sub_tasks = list(dict.fromkeys(
                [t.strip() for t in task_decomp_obj.sub_tasks]))[:4]
            logging.info(f"âœ… æ¸…ç†èˆ‡é©—è­‰å¾Œçš„å­ä»»å‹™ ({len(sub_tasks)} æ¢): {sub_tasks}")

            research_memos = []
            all_source_documents = []
            analyst_template = self.prompts.get("research_synthesizer")
            search_strategist_template = self.prompts.get("search_strategist")
            if not all([analyst_template, search_strategist_template]):
                raise ValueError("ä¸€å€‹æˆ–å¤šå€‹é—œéµçš„ Prompt æ¨¡æ¿æœªæ‰¾åˆ°ï¼")
            strategist_chain = search_strategist_template | self.llm | self.parsers[
                "search_strategist"]

            # ä¸»è¦çš„å¾ªç’°ï¼Œæœ€é©åˆåŠ å…¥å¿ƒè·³
            for i, task in enumerate(sub_tasks):
                check_cancellation()
                yield from yield_heartbeat_if_needed() # <<-- æ¯æ¬¡å¾ªç’°é–‹å§‹æ™‚æª¢æŸ¥
                yield from send_event(f"data: {json.dumps({'type': 'status', 'message': f'æ­¥é©Ÿ 2.{i+1}/{len(sub_tasks)}: æ­£åœ¨ç¶œåˆç ”ç©¶ "{task[:20]}..."'})}\n\n")

                search_strategy_obj = strategist_chain.invoke(
                    {"question": question, "task": task})
                search_queries = list(dict.fromkeys(
                    [q.strip() for q in search_strategy_obj.search_queries])) or [task]
                logging.info(
                    f"   -> ç­–ç•¥å¸«ç‚º '{task}' ç”Ÿæˆäº† {len(search_queries)} å€‹æœå°‹å‘é‡: {search_queries}")
                task_specific_docs = []
                
                # å…§éƒ¨å¾ªç’°ï¼Œé€™è£¡çš„ç­‰å¾…æ™‚é–“æœ€é•·
                for query in search_queries:
                    check_cancellation()
                    yield from yield_heartbeat_if_needed() # <<-- æ¯æ¬¡æŸ¥è©¢å‰æª¢æŸ¥
                    docs_for_query = self._agent_based_web_search(query, task) # <== é•·æ™‚é–“æ“ä½œ
                    if docs_for_query:
                        task_specific_docs.extend(docs_for_query)
                
                context = "æ³¨æ„ï¼šæœªèƒ½å¾ç¶²è·¯æ‰¾åˆ°ç›¸é—œè³‡æ–™ã€‚"
                if task_specific_docs:
                    all_source_documents.extend(task_specific_docs)
                    unique_contents, unique_docs_for_synthesis = set(), []
                    for doc in task_specific_docs:
                        if doc.page_content not in unique_contents:
                            unique_contents.add(doc.page_content)
                            unique_docs_for_synthesis.append(doc)
                    logging.info(
                        f"   -> ç‚ºå­ä»»å‹™ '{task}' åŒ¯ç¸½äº† {len(unique_docs_for_synthesis)} ä»½ä¸é‡è¤‡çš„æ–‡ä»¶é€²è¡Œç¶œåˆåˆ†æã€‚")
                    context = "\n---\n".join(
                        [f"ä¾†æºï¼š{doc.metadata.get('source', 'ç¶²è·¯')}\nå…§å®¹:\n{doc.page_content}" for doc in unique_docs_for_synthesis])
                else:
                    logging.warning(f"   -> æœªèƒ½ç‚ºå­ä»»å‹™ '{task}' æ‰¾åˆ°ä»»ä½•ç¶²è·¯è³‡æ–™ã€‚")
                
                yield from yield_heartbeat_if_needed() # <<-- LLMèª¿ç”¨å‰æª¢æŸ¥
                detailed_memo = self.llm.invoke(
                    analyst_template.format(context=context, question=task))

                research_memos.append(f"### ç ”ç©¶ä¸»é¡Œ: {task}\n\n{detailed_memo}")
                logging.info(f"   -> å·²ç‚º '{task}' ç”Ÿæˆè©³ç´°ç ”ç©¶å‚™å¿˜éŒ„ã€‚")

            final_context = "\n\n---\n\n".join(research_memos)

            check_cancellation()
            yield from send_event(f"data: {json.dumps({'type': 'status', 'message': 'æ­¥é©Ÿ 3/3: æ­£åœ¨ç”Ÿæˆå ±å‘Šå¤§ç¶±...'})}\n\n")
            yield from yield_heartbeat_if_needed() # <<-- æœ€å¾Œä¸€æ­¥å‰æª¢æŸ¥
            blueprint_gen_template = self.prompts.get(
                "answer_blueprint_generator")
            blueprint_chain = blueprint_gen_template | self.llm | self.parsers[
                "answer_blueprint_generator"]
            blueprint_obj = blueprint_chain.invoke(
                {"context": final_context, "question": question})

            blueprint_json = blueprint_obj.dict()
            logging.info("âœ… æˆåŠŸè§£æå›ç­”å¤§ç¶± JSONã€‚")

            serializable_sources = [{'page_content': doc.page_content,
                                     'metadata': doc.metadata} for doc in all_source_documents]

            end_time = time.time()
            duration_seconds = end_time - start_time
            duration_str = str(timedelta(seconds=int(duration_seconds)))

            self.blueprint_cache[task_id] = {
                'question': question,
                'context': final_context,
                'blueprint': blueprint_json,
                'sources': serializable_sources
            }
            logging.info(f"âœ… å·²ç‚ºä»»å‹™ {task_id} å¿«å–è—åœ–è³‡æ–™ã€‚")

            yield from send_event(f"data: {json.dumps({'type': 'blueprint_generated', 'data': {'blueprint': blueprint_json, 'duration': duration_str}})}\n\n")
            logging.info(f"âœ… å·²ç‚ºä»»å‹™ {task_id} ç”Ÿæˆè—åœ–ä¸¦ç™¼é€è‡³å‰ç«¯ï¼Œç­‰å¾…ä½¿ç”¨è€…ç¢ºèªã€‚")

        except ClientDisconnectedError as e:
            logging.warning(f"[WORKFLOW CANCELLATION] {e}")
            self.active_tasks[task_id]['is_cancelled'] = True

    def _fact_check_report(self, report_text: str, context: str) -> Generator[str, None, None]:
        logging.info("ğŸ•µï¸ å•Ÿå‹•äº‹å¾Œäº‹å¯¦æ ¡é©—æµç¨‹...")
        yield f"data: {json.dumps({'type': 'status', 'message': 'æ­¥é©Ÿ 5/5: æ­£åœ¨é€²è¡Œäº‹å¾Œäº‹å¯¦æ ¡é©—...'})}\n\n"

        try:
            claim_extractor_chain = self.prompts["claim_extractor"] | self.llm | self.parsers["claim_extractor"]
            fact_checker_chain = self.prompts["fact_checker"] | self.llm | self.parsers["fact_checker"]

            extracted_claims_obj = claim_extractor_chain.invoke(
                {"report_text": report_text})
            claims = extracted_claims_obj.claims
            logging.info(f"   -> å·²å¾å ±å‘Šä¸­æå– {len(claims)} æ¢é—œéµè«–è¿°é€²è¡Œæ ¡é©—ã€‚")

            if not claims:
                yield f"data: {json.dumps({'type': 'content', 'content': '\n\n---\n\n## ğŸ¤– è‡ªå‹•äº‹å¯¦æ ¡é©—\n\næœªèƒ½å¾å ±å‘Šä¸­æå–å‡ºå¯ä¾›æ ¡é©—çš„é—œéµè«–è¿°ã€‚'})}\n\n"
                return

            fact_check_results = []
            for claim in claims:
                try:
                    check_result = fact_checker_chain.invoke(
                        {"context": context, "statement": claim})
                    fact_check_results.append((claim, check_result))
                except Exception as e:
                    logging.warning(f"æ ¡é©—è«–è¿°æ™‚å‡ºéŒ¯: '{claim}'. éŒ¯èª¤: {e}")

            result_markdown = "\n\n---\n\n## ğŸ¤– è‡ªå‹•äº‹å¯¦æ ¡é©—\n\næœ¬å·¥å…·å·²è‡ªå‹•æ ¡é©—å ±å‘Šä¸­çš„é—œéµè«–è¿°ï¼Œçµæœå¦‚ä¸‹ï¼š\n\n"
            for claim, result in fact_check_results:
                icon = "âœ…" if result.is_verified else "âŒ"
                evidence = result.supporting_evidence if result.is_verified else "æœªåœ¨åŸå§‹è³‡æ–™ä¸­æ‰¾åˆ°ç›´æ¥è­‰æ“šã€‚"
                result_markdown += f"- **è«–è¿°**: \"{claim}\"\n"
                result_markdown += f"  - **æ ¡é©—çµæœ**: {icon} {evidence}\n"

            yield f"data: {json.dumps({'type': 'content', 'content': result_markdown})}\n\n"
            logging.info("âœ… äº‹å¯¦æ ¡é©—æµç¨‹å®Œæˆã€‚")

        except Exception as e:
            logging.error(f"âŒ åœ¨äº‹å¯¦æ ¡é©—æµç¨‹ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)
            yield f"data: {json.dumps({'type': 'content', 'content': '\n\n> **äº‹å¯¦æ ¡é©—å¤±æ•—**: åœ¨åŸ·è¡Œè‡ªå‹•æ ¡é©—æ™‚ç™¼ç”Ÿå…§éƒ¨éŒ¯èª¤ã€‚'})}\n\n"

    def stream_write_report(self, task_id: str) -> Generator[str, None, None]:
        def check_cancellation():
            if task_id and self.active_tasks.get(task_id, {}).get("is_cancelled"):
                raise InterruptedError(f"Task {task_id} cancelled by user.")

        full_report_text = ""
        try:
            cached_data = self.blueprint_cache.get(task_id)
            if not cached_data:
                raise ValueError(f"ç„¡æ³•åœ¨å¿«å–ä¸­æ‰¾åˆ°ä»»å‹™ {task_id} çš„è—åœ–è³‡æ–™ã€‚")

            # --- å¿ƒè·³æ©Ÿåˆ¶åˆå§‹åŒ– ---
            last_heartbeat = time.time()
            HEARTBEAT_INTERVAL = 15 # seconds

            def yield_heartbeat_if_needed():
                nonlocal last_heartbeat
                if time.time() - last_heartbeat > HEARTBEAT_INTERVAL:
                    yield f"data: {json.dumps({'type': 'heartbeat'})}\n\n"
                    last_heartbeat = time.time()
                    logging.debug(f"Sent heartbeat during report writing for task {task_id}")
            # --- å¿ƒè·³æ©Ÿåˆ¶åˆå§‹åŒ–çµæŸ ---

            question = cached_data['question']
            context = cached_data['context']
            blueprint = cached_data['blueprint']
            sources_json = cached_data['sources']

            all_source_documents = [Document(
                page_content=doc['page_content'], metadata=doc['metadata']) for doc in sources_json]

            check_cancellation()
            yield f"data: {json.dumps({'type': 'status', 'message': 'æ­¥é©Ÿ 4/4: æ­£åœ¨æ’°å¯«æœ€çµ‚å ±å‘Š...'})}\n\n"
            chapter_writer_template = self.prompts.get("chapter_writer")
            if not chapter_writer_template:
                raise ValueError("chapter_writer.txt æ¨¡æ¿æœªæ‰¾åˆ°ï¼")

            chapter_writer_chain = chapter_writer_template | self.llm | StrOutputParser()

            for chapter in blueprint.get('chapters', []):
                check_cancellation()
                yield from yield_heartbeat_if_needed() # <<-- æ¯å€‹ç« ç¯€é–‹å§‹å‰æª¢æŸ¥

                chapter_title = chapter.get('title', 'ç„¡æ¨™é¡Œç« ç¯€')
                # Yield chapter title as a header
                chapter_header = f"## {chapter_title}\n\n"
                full_report_text += chapter_header
                yield f"data: {json.dumps({'type': 'content', 'content': chapter_header})}\n\n"
                key_points_list = chapter.get('key_points', [])
                formatted_key_points = "\n".join(
                    f"- {point}" for point in key_points_list)

                for chunk in chapter_writer_chain.stream({
                    "question": question,
                    "context": context,
                    "chapter_title": chapter_title,
                    "key_points": formatted_key_points
                }):
                    check_cancellation()
                    # é‡ç½®å¿ƒè·³è¨ˆæ™‚å™¨ï¼Œå› ç‚ºæˆ‘å€‘å‰›å‰›ç™¼é€äº†çœŸå¯¦æ•¸æ“š
                    last_heartbeat = time.time() 
                    full_report_text += chunk
                    yield f"data: {json.dumps({'type': 'content', 'content': chunk})}\n\n"

            logging.info("å ±å‘Šæ­£æ–‡ä¸²æµç”Ÿæˆå®Œç•¢ï¼æ­£åœ¨é€²è¡Œæœ€çµ‚çµ„è£...")
            unique_urls = {doc.metadata.get("source", "").replace("ç¶²é ç€è¦½: ", "").strip(
            ) for doc in all_source_documents if doc.metadata.get("source", "").startswith("ç¶²é ç€è¦½: ")}
            reference_list_str = "\n".join(
                [f"- {url}" for url in sorted(list(unique_urls)) if url])
            if reference_list_str:
                reference_section_str = f"\n\n---\n\n## åƒè€ƒæ–‡ç»\n{reference_list_str}"
                full_report_text += reference_section_str
                logging.info(f"âœ… å·²æå– {len(unique_urls)} æ¢ç¨ç‰¹çš„åƒè€ƒæ–‡ç»ã€‚")
                yield f"data: {json.dumps({'type': 'content', 'content': reference_section_str})}\n\n"

            # --- Fact-Checking Stage ---
            yield from yield_heartbeat_if_needed()
            yield from self._fact_check_report(full_report_text, context)

            yield f"data: {json.dumps({'type': 'status', 'message': 'å ±å‘Šç”Ÿæˆå®Œç•¢ï¼'})}\n\n"
            yield from self._send_done_signal()

        except InterruptedError:
            logging.info(f"ğŸ›‘ å ±å‘Šæ’°å¯«ä»»å‹™ {task_id} å·²è¢«ä½¿ç”¨è€…æˆåŠŸä¸­æ­¢ã€‚")
            yield f"data: {json.dumps({'type': 'status', 'message': 'ä»»å‹™å·²å–æ¶ˆã€‚'})}\n\n"
            yield from self._send_done_signal()
        except Exception as e:
            logging.error(f"âŒ åœ¨å ±å‘Šæ’°å¯«æµç¨‹ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)
            error_message = f'æŠ±æ­‰ï¼ŒåŸ·è¡Œå ±å‘Šæ’°å¯«æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}'
            yield f"data: {json.dumps({'type': 'error', 'content': error_message})}\n\n"
            yield from self._send_done_signal()
        finally:
            if task_id in self.blueprint_cache:
                final_report_with_fact_check = self.blueprint_cache[task_id].get(
                    'final_report', full_report_text)
                if final_report_with_fact_check:
                    self.add_history_entry(
                        self.blueprint_cache[task_id]['question'], final_report_with_fact_check)
                self._perform_final_cleanup(task_id)

    def ask(self, question: str, stream: bool = True, task_id: str = None, bypass_assessment: bool = False) -> Generator[str, None, None]:
        if not task_id:
            task_id = f"task_{int(time.time() * 1000)}_{os.urandom(4).hex()}"
        self.active_tasks[task_id] = {'is_cancelled': False}
        logging.info(f"\nğŸ¤” æ”¶åˆ°è«‹æ±‚ä¸¦è¨»å†Šä»»å‹™ ID: {task_id}ï¼Œå•é¡Œ: '{question}'")

        if not self.use_web_search:
            logging.info(" KAIZEN è·¯ç”±ï¼šç¶²è·¯ç ”ç©¶å·²åœç”¨ï¼Œå¼·åˆ¶åŸ·è¡Œç›´æ¥å•ç­”ã€‚ ")
            try:
                direct_answer_template = self.prompts.get("direct_answer")
                prompt = direct_answer_template.format(
                    question=question) if direct_answer_template else question
                for chunk in self.llm.stream(prompt):
                    yield f"data: {json.dumps({'type': 'content', 'content': chunk})}\n\n"
                yield from self._send_done_signal()
            finally:
                self._perform_final_cleanup(task_id)
            return

        is_complex_flow = False
        try:
            if bypass_assessment or re.search(r"https?://[\S]+", question):
                is_complex_flow = True
                yield from self._stream_research_and_blueprint(question, stream, task_id)
            else:
                assessor_template = self.prompts.get("query_assessor")
                assessor_chain = assessor_template | self.llm | self.parsers["query_assessor"]
                assessment_obj = assessor_chain.invoke({"question": question})
                assessment = assessment_obj.assessment

                if assessment == "specific_topic":
                    is_complex_flow = True
                    yield from self._stream_research_and_blueprint(question, stream, task_id)
                elif assessment == "broad_concept":
                    is_complex_flow = True
                    yield from self._handle_clarification(question, task_id)
                else:
                    is_complex_flow = True
                    yield from self._stream_research_and_blueprint(question, stream, task_id)

        except OutputParserException as e:
            logging.error(
                f"âŒ åœ¨ ASK æŒ‡æ®å®˜æ¨¡å¼ä¸­ç™¼ç”Ÿ OutputParserException: {e}", exc_info=True)
            error_message = 'å ±å‘Šç”Ÿæˆå¤±æ•—ï¼šAI æ¨¡å‹æœªèƒ½è¿”å›é æœŸæ ¼å¼çš„æ•¸æ“šï¼Œè«‹å˜—è©¦èª¿æ•´å•é¡Œæˆ–ç¨å¾Œé‡è©¦ã€‚'
            yield f"data: {json.dumps({'type': 'error', 'content': error_message})}\n\n"
            yield from self._send_done_signal()
        except InterruptedError:
            logging.info(f" ä»»å‹™ {task_id} å·²è¢«ä½¿ç”¨è€…æˆåŠŸä¸­æ­¢ã€‚")
            yield f"data: {json.dumps({'type': 'status', 'message': 'ä»»å‹™å·²å–æ¶ˆã€‚'})}\n\n"
            yield from self._send_done_signal()
        except Exception as e:
            logging.error(f"âŒ åœ¨ ASK æŒ‡æ®å®˜æ¨¡å¼ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)
            error_message = f'æŠ±æ­‰ï¼ŒåŸ·è¡Œæ™‚ç™¼ç”Ÿäº†ç„¡æ³•é æœŸçš„éŒ¯èª¤: {str(e)}'
            yield f"data: {json.dumps({'type': 'error', 'content': error_message})}\n\n"
            yield from self._send_done_signal()
        
