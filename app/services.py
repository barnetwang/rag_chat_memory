import os
import json
import re
import requests
from datetime import datetime
from bs4 import BeautifulSoup
import logging
import urllib3
import time
from typing import Any, Generator

# LangChain Imports
from ddgs import DDGS
from langchain_ollama.llms import OllamaLLM
from langchain_core.documents import Document
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

try:
    from playwright.sync_api import sync_playwright, Browser
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    logging.warning(
        "Playwright æœªå®‰è£ã€‚çˆ¬èŸ²åŠŸèƒ½å°‡åƒ…ä½¿ç”¨ requestsã€‚å»ºè­°åŸ·è¡Œ 'pip install playwright && playwright install' ä¾†å¢å¼·çˆ¬èŸ²èƒ½åŠ›ã€‚")

try:
    import fitz
    PYMUPDF_AVAILABLE = True
except ImportError:
    PYMUPDF_AVAILABLE = False
    logging.warning("PyMuPDF æœªå®‰è£ã€‚PDF è™•ç†åŠŸèƒ½å°‡ä¸å¯ç”¨ã€‚å»ºè­°åŸ·è¡Œ 'pip install PyMuPDF'ã€‚")

logging.basicConfig(level=logging.INFO,
                    format="%(asctime)s - %(levelname)s - %(message)s")
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


def get_ollama_models(ollama_base_url="http://localhost:11434"):
    try:
        response = requests.get(f"{ollama_base_url}/api/tags")
        response.raise_for_status()
        return [model["name"] for model in response.json().get("models", [])]
    except Exception as e:
        logging.error(f"å­˜å– Ollama æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return []


class ConversationalRAG:
    def __init__(self, config: dict):
        self.playwright = None
        self.browser = None
        self.config = config
        self.use_web_search = True
        self.ollama_base_url = self.config["OLLAMA_BASE_URL"]
        self.prompts = {}
        self.active_tasks = {}

        logging.info("æ­£åœ¨è¨­ç½® LLM...")
        try:
            self.llm = OllamaLLM(
                model=self.config["llm_model"], base_url=self.ollama_base_url)
            self.current_llm_model = self.config["llm_model"]
        except Exception as e:
            logging.error(f"è¨­ç½® LLM å¤±æ•—: {e}")
            raise

        self._init_prompts()
        logging.info("âœ… ç³»çµ±å·²å°±ç·’ (ä½¿ç”¨ DDGS ä½œç‚ºæœå°‹å¼•æ“)ã€‚")

    def _init_playwright(self):
        if PLAYWRIGHT_AVAILABLE and not self.browser:
            try:
                logging.info("ğŸŒ æ­£åœ¨å•Ÿå‹•å…±äº«çš„ Playwright ç€è¦½å™¨å¯¦ä¾‹...")
                self.playwright = sync_playwright().start()
                self.browser = self.playwright.chromium.launch(headless=True)
                logging.info("âœ… Playwright å…±äº«ç€è¦½å™¨å¯¦ä¾‹å·²æˆåŠŸå•Ÿå‹•ã€‚")
            except Exception as e:
                logging.error(f"âŒ å•Ÿå‹• Playwright å¤±æ•—: {e}")
                self.playwright = None
                self.browser = None

    def _load_all_prompts(self, directory: str) -> dict:
        prompts = {}
        base_dir = os.path.dirname(os.path.abspath(__file__))
        prompts_dir = os.path.join(base_dir, directory)
        if not os.path.isdir(prompts_dir):
            return {}
        logging.info(f"ğŸ” æ­£åœ¨å¾ '{prompts_dir}' åŠ è¼‰ Prompt æ¨¡æ¿...")
        for filename in os.listdir(prompts_dir):
            if filename.endswith(".txt"):
                filepath = os.path.join(prompts_dir, filename)
                with open(filepath, 'r', encoding='utf-8') as f:
                    prompt_name = os.path.splitext(filename)[0]
                    prompts[prompt_name] = PromptTemplate.from_template(
                        f.read(), template_format="f-string")
                    logging.info(f"   -> å·²åŠ è¼‰: {prompt_name}")
        return prompts

    def _init_prompts(self):
        self.prompts = self._load_all_prompts("prompts")

    def set_llm_model(self, model_name: str):
        logging.info(f"\nğŸ”„ æ­£åœ¨åˆ‡æ› LLM æ¨¡å‹è‡³: {model_name}")
        try:
            self.llm = OllamaLLM(
                model=model_name, base_url=self.ollama_base_url)
            self.llm.invoke("Hi", stop=["Hi"])
            self.current_llm_model = model_name
            logging.info(f"âœ… LLM æ¨¡å‹æˆåŠŸåˆ‡æ›ç‚º: {self.current_llm_model}")
            return True
        except Exception as e:
            logging.error(f"åˆ‡æ› LLM æ¨¡å‹å¤±æ•—: {e}")
            return False

    def set_web_search(self, enabled: bool):
        logging.info(f"ğŸ”„ å°‡ç¶²è·¯æœå°‹è¨­ç½®ç‚º: {'å•Ÿç”¨' if enabled else 'åœç”¨'}")
        self.use_web_search = enabled
        return True

    def _agent_based_web_search(self, question: str):
        logging.info(f"ğŸ¤– (Agentæ¨¡å¼) å•Ÿå‹• DDGS é€šç”¨ç¶²è·¯æœå°‹: '{question}'")
        all_source_docs = []
        if not PLAYWRIGHT_AVAILABLE:
            logging.warning("Playwright æœªå®‰è£ï¼Œæœå°‹åŠŸèƒ½å—é™ã€‚")
            return []
        # --- ç¶²ç«™é»‘åå–® ---
        DOMAIN_BLACKLIST = [
            "zhihu.com",      # å¾ˆé›£çˆ¬å–ï¼Œç¶“å¸¸éœ€è¦ç™»å…¥
            "facebook.com",   # éœ€è¦ç™»å…¥
            "instagram.com",  # éœ€è¦ç™»å…¥
            "linkedin.com",   # éœ€è¦ç™»å…¥
            "medium.com",     # ç¶“å¸¸æœ‰ä»˜è²»ç‰†æˆ–ç™»å…¥æç¤º
        ]
        # ------------------------------------
        with sync_playwright() as playwright:
            browser: Browser = None
            try:
                browser = playwright.chromium.launch(headless=True)
                with DDGS() as ddgs:
                    search_results = ddgs.text(
                        question, max_results=20, region="tw-zh")
                if not search_results:
                    return []
                urls_to_browse = [
                    res["href"] for res in search_results
                    if "href" in res and not any(blacklisted in res["href"] for blacklisted in DOMAIN_BLACKLIST)
                ][:10]
                logging.info(f"   -> å·²éæ¿¾æ‰é»‘åå–®ç¶²ç«™ï¼Œæº–å‚™ç€è¦½ä»¥ä¸‹ {len(urls_to_browse)} å€‹ç¶²é ...")
                if not urls_to_browse:
                    logging.warning("   -> éæ¿¾å¾Œæ²’æœ‰å¯ç€è¦½çš„ç¶²é ã€‚")
                    return []

                for url in urls_to_browse:
                    if not url:
                        continue
                    content = self._scrape_webpage_text(url, browser)
                    if "ç„¡æ³•çˆ¬å–" not in content and len(content) > 100:
                        all_source_docs.append(
                            Document(page_content=content, metadata={"source": f"ç¶²é ç€è¦½: {url}"}))
                logging.info(f"   -> Agent æœå°‹ '{question}' å®Œæˆï¼Œå…±æ‰¾åˆ° {len(all_source_docs)} ä»½æœ‰æ•ˆæ–‡ä»¶ã€‚")
                return all_source_docs
            except Exception as e:
                logging.error(f"âŒ åœ¨ Agent æœå°‹éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
                return []
            finally:
                if browser:
                    browser.close()

    def _scrape_webpage_text(self, url: str, browser: Browser = None):
        if url.lower().endswith('.pdf'):
            if not PYMUPDF_AVAILABLE: return "ç„¡æ³•è™•ç† PDF æ–‡ä»¶ï¼Œå› ç‚º PyMuPDF æœªå®‰è£ã€‚"
            try:
                logging.info(f"ğŸ“„ (PyMuPDFæ¨¡å¼) æ­£åœ¨è™•ç† PDF ç¶²å€: {url}")
                headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'}
                response = requests.get(url, headers=headers, timeout=30, verify=False)
                response.raise_for_status()
                with fitz.open(stream=response.content, filetype="pdf") as pdf_document:
                    text = "".join(page.get_text() for page in pdf_document)
                logging.info(f"âœ… (PyMuPDF) æˆåŠŸæå– PDF æ–‡å­—ï¼Œé•·åº¦: {len(text)} å­—å…ƒã€‚")
                return text
            except Exception as e:
                logging.error(f"âŒ (PyMuPDF) è™•ç† PDF æ™‚å¤±æ•—: {e}")
                return f"ç„¡æ³•çˆ¬å– PDFï¼ŒéŒ¯èª¤: {e}"
        if browser:
            logging.info(f"ğŸ•¸ï¸ (Playwrightæ¨¡å¼) æ­£åœ¨å˜—è©¦çˆ¬å–ç¶²å€: {url}")
            page = None
            try:
                if any(url.lower().endswith(ext) for ext in ['.doc', '.docx', '.zip', '.rar', '.xls', '.xlsx']): return "ç„¡æ³•çˆ¬å–ç¶²ç«™ï¼ŒéŒ¯èª¤: ä¸æ”¯æŒçš„æ–‡ä»¶é¡å‹"
                page = browser.new_page()
                page.goto(url, timeout=30000, wait_until='domcontentloaded')
                try:
                    page.wait_for_selector("main, article, #content, #main-content, .post-content, .article-body", timeout=10000)
                    logging.info("   -> âœ… æˆåŠŸç­‰åˆ°é—œéµå…§å®¹å€å¡Šã€‚")
                except Exception as wait_e:
                    logging.warning(f"   -> âš ï¸ æœªèƒ½ç­‰åˆ°ç‰¹å®šå…§å®¹å€å¡Šï¼Œå°‡ç›´æ¥æŠ“å–ç¾æœ‰å…§å®¹ã€‚éŒ¯èª¤: {wait_e}")
                html_content = page.content()
                soup = BeautifulSoup(html_content, "html.parser")
                for element in soup(["script", "style", "nav", "footer", "aside", "header", "iframe", "form"]): element.decompose()
                text = "\n".join(line.strip() for line in soup.get_text().splitlines() if line.strip())
                if len(text) < 300: logging.warning(f"âš ï¸ (Playwright) ä» {url} æå–çš„æœ‰æ•ˆæ–‡å­—è¿‡å°‘ ({len(text)} å­—å…ƒ)ï¼Œå¯èƒ½ä¸æ˜¯ä¸»è¦å†…å®¹ã€‚")
                logging.info(f"âœ… (Playwright) æˆåŠŸç²å–ç¶²é æ–‡å­—ï¼Œé•·åº¦: {len(text)} å­—å…ƒã€‚")
                return text
            except Exception as e:
                logging.warning(f"âŒ (Playwright) å¤±æ•—: {e}ã€‚å°‡å›é€€è‡³ Requests æ¨¡å¼ã€‚")
                return "ç„¡æ³•çˆ¬å–ç¶²ç«™ï¼ŒéŒ¯èª¤: Playwright é€£æ¥å¤±æ•—"
            finally:
                if page and not page.is_closed(): page.close()
        logging.info(f"ğŸ•¸ï¸ (Requestsæ¨¡å¼) æ­£åœ¨å˜—è©¦çˆ¬å–ç¶²å€: {url}")
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'}
            response = requests.get(url, headers=headers, timeout=30, verify=False)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, "html.parser")
            for element in soup(["script", "style", "nav", "footer", "aside", "header", "iframe", "form"]): element.decompose()
            text = "\n".join(line.strip() for line in soup.get_text().splitlines() if line.strip())
            if not text or len(text) < 300:
                logging.warning(f"âš ï¸ (Requests) æœªèƒ½å¾ {url} æå–åˆ°ä»»ä½•æœ‰æ•ˆæ–‡å­—ã€‚")
                return "ç„¡æ³•çˆ¬å–ç¶²ç«™ï¼ŒéŒ¯èª¤: ç¶²é å…§å®¹éå°‘æˆ–ç„¡æ•ˆ"
            logging.info(f"âœ… (Requests) æˆåŠŸç²å–ç¶²é æ–‡å­—ï¼Œé•·åº¦: {len(text)} å­—å…ƒã€‚")
            return text
        except Exception as e:
            logging.error(f"âŒ (Requests) å¤±æ•—: {e}")
            return f"ç„¡æ³•çˆ¬å–ç¶²å€ï¼ŒéŒ¯èª¤: {e}"

    def _perform_final_cleanup(self, task_id: str):
        if task_id:
            logging.info(f"ğŸ§¹ ä»»å‹™ {task_id} æ­£åœ¨æœ€çµ‚è¨»éŠ·ã€‚")
            if task_id in self.active_tasks:
                del self.active_tasks[task_id]

    def _send_done_signal(self) -> Generator[str, None, None]:
        yield f"data: {json.dumps({'type': 'done', 'content': '[DONE]'})}\n\n"
        logging.info("âœ… è³‡æ–™æµå·²æ­£å¸¸é—œé–‰ã€‚")

    def _handle_clarification(self, question: str, task_id: str) -> Generator[str, None, None]:
        logging.info(f"ğŸ’¬ KAIZEN è·¯ç”±ï¼šå•é¡Œéæ–¼å¯¬æ³›ï¼Œå•Ÿå‹•å•é¡Œæ¾„æ¸…æµç¨‹ã€‚")
        clarifier_template = self.prompts.get("query_clarifier")
        if not clarifier_template: raise ValueError("query_clarifier.txt æ¨¡æ¿æœªæ‰¾åˆ°ï¼")
        clarifier_chain = clarifier_template | self.llm | StrOutputParser()
        clarification_str = clarifier_chain.invoke({"question": question})
        match = re.search(r"\{.*\}", clarification_str, re.DOTALL)
        if not match: raise ValueError(f"å•é¡Œæ¾„æ¸…å¸«çš„å›æ‡‰ä¸­ä¸åŒ…å«æœ‰æ•ˆçš„ JSON çµæ§‹: {clarification_str}")
        clarification_json = json.loads(match.group(0))
        yield f"data: {json.dumps({'type': 'clarification', 'data': clarification_json})}\n\n"

    def _handle_complex_project(self, question: str, stream: bool, task_id: str) -> Generator[str, None, None]:
        if not self.use_web_search:
            logging.info("ğŸŒ ç¶²è·¯ç ”ç©¶åŠŸèƒ½å·²åœç”¨ã€‚è½‰å‘ç›´æ¥å•ç­”æ¨¡å¼ (Invoke)ã€‚")
            direct_answer_template = self.prompts.get("direct_answer")
            direct_answer_prompt = direct_answer_template.format(question=question) if direct_answer_template else f"å•é¡Œï¼š{question}\n\nå›ç­”ï¼š"
            logging.info("æ­£åœ¨å‘¼å« LLM (invoke)...")
            full_response = self.llm.invoke(direct_answer_prompt)
            logging.info(f"LLM (invoke) å·²å›è¦†ï¼Œå…§å®¹é•·åº¦: {len(str(full_response))}")
            response_content = full_response.content if hasattr(full_response, 'content') else str(full_response)
            if response_content and response_content.strip():
                yield f"data: {json.dumps({'type': 'content', 'content': response_content})}\n\n"
            else:
                logging.warning("âš ï¸ LLM åœ¨ç›´æ¥å•ç­”æ¨¡å¼ä¸‹æ²’æœ‰ç”Ÿæˆä»»ä½•å…§å®¹ã€‚")
                fallback_message = "æŠ±æ­‰ï¼Œæˆ‘ä¼¼ä¹å°é€™å€‹å•é¡Œæ²’æœ‰æƒ³æ³•ã€‚èƒ½å¦è«‹æ‚¨æ›å€‹æ–¹å¼æå•ï¼Ÿ"
                yield f"data: {json.dumps({'type': 'content', 'content': fallback_message})}\n\n"
            return

        logging.info(f"ğŸš€ å•Ÿå‹• [KAIZEN æœ€çµ‚æ¶æ§‹] å°ˆå®¶å°çµ„å·¥ä½œæµ (Task ID: {task_id})...")
        def check_cancellation():
            if task_id and self.active_tasks.get(task_id, {}).get("is_cancelled"):
                logging.warning(f"ğŸ›‘ ä»»å‹™ {task_id} å·²è¢«ä½¿ç”¨è€…å–æ¶ˆã€‚")
                raise InterruptedError(f"Task {task_id} cancelled by user.")
        check_cancellation()
        yield f"data: {json.dumps({'type': 'status', 'message': 'æ­¥é©Ÿ 1/4: æ­£åœ¨æ‹†è§£ç ”ç©¶ä»»å‹™...'})}\n\n"

        task_decomp_template = self.prompts.get("task_decomposition")
        if not task_decomp_template: raise ValueError("task_decomposition.txt æ¨¡æ¿æœªæ‰¾åˆ°ï¼")
        task_decomp_prompt_string = task_decomp_template.format(question=question)
        sub_tasks_str = self.llm.invoke(task_decomp_prompt_string)
        matches = re.findall(r"^\s*\d+\.\s*(.*)", sub_tasks_str, re.MULTILINE)
        if not matches: raise ValueError("ç„¡æ³•å¾ LLM è¼¸å‡ºä¸­æå–æœ‰æ•ˆå­ä»»å‹™ã€‚")
        validated_tasks = list(dict.fromkeys([t.strip() for t in matches]))[:4]

        if len(validated_tasks) < 2:
            logging.warning(f"ä»»å‹™æ‹†è§£å¾Œæœ‰æ•ˆä»»å‹™ä¸è¶³2å€‹ï¼Œè½‰ç‚ºç›´æ¥å›ç­”æ¨¡å¼ã€‚")
            direct_answer_prompt = self.prompts.get("direct_answer").format(question=question)
            full_response = self.llm.invoke(direct_answer_prompt)
            response_content = full_response.content if hasattr(full_response, 'content') else full_response
            yield f"data: {json.dumps({'type': 'content', 'content': response_content})}\n\n"
            return
        sub_tasks = validated_tasks
        logging.info(f"âœ… æ¸…ç†èˆ‡é©—è­‰å¾Œçš„å­ä»»å‹™ ({len(sub_tasks)} æ¢): {sub_tasks}")

        executive_summaries, all_source_documents = [], []
        analyst_template = self.prompts.get("research_synthesizer")
        summarizer_template = self.prompts.get("memo_summarizer")
        search_strategist_template = self.prompts.get("search_strategist")
        if not all([analyst_template, summarizer_template, search_strategist_template]): raise ValueError("ä¸€å€‹æˆ–å¤šå€‹é—œéµçš„ Prompt æ¨¡æ¿æœªæ‰¾åˆ°ï¼")
        strategist_chain = search_strategist_template | self.llm | StrOutputParser()
        memo_summarizer_chain = summarizer_template | self.llm | StrOutputParser()

        for i, task in enumerate(sub_tasks):
            check_cancellation()
            yield f"data: {json.dumps({'type': 'status', 'message': f'æ­¥é©Ÿ 2.{i+1}/{len(sub_tasks)}: æ­£åœ¨æ·±åº¦ç ”ç©¶ \"{task[:20]}...\"'})}\n\n"
            search_queries_str = strategist_chain.invoke({"question": question, "task": task})
            search_queries = [line.strip() for line in re.findall(r"^\s*\d+\.\s*(.*)", search_queries_str, re.MULTILINE) if line.strip()] or [task]
            logging.info(f"   -> ç­–ç•¥å¸«ç‚º '{task}' ç”Ÿæˆäº† {len(search_queries)} å€‹æœå°‹å‘é‡: {search_queries}")
            task_specific_docs = []
            for query in search_queries:
                check_cancellation()
                docs_for_query = self._agent_based_web_search(query)
                if docs_for_query: task_specific_docs.extend(docs_for_query)
            context = "æ³¨æ„ï¼šæœªèƒ½å¾ç¶²è·¯æ‰¾åˆ°ç›¸é—œè³‡æ–™ã€‚"
            if task_specific_docs:
                all_source_documents.extend(task_specific_docs)
                unique_contents, unique_docs_for_synthesis = set(), []
                for doc in task_specific_docs:
                    if doc.page_content not in unique_contents:
                        unique_contents.add(doc.page_content)
                        unique_docs_for_synthesis.append(doc)
                logging.info(f"   -> ç‚ºå­ä»»å‹™ '{task}' åŒ¯ç¸½äº† {len(unique_docs_for_synthesis)} ä»½ä¸é‡è¤‡çš„æ–‡ä»¶é€²è¡Œç¶œåˆåˆ†æã€‚")
                context = "\n---\n".join([f"ä¾†æºï¼š{doc.metadata.get('source', 'ç¶²è·¯')}\nå…§å®¹ï¼š\n{doc.page_content}" for doc in unique_docs_for_synthesis])
            else:
                logging.warning(f"   -> æœªèƒ½ç‚ºå­ä»»å‹™ '{task}' æ‰¾åˆ°ä»»ä½•ç¶²è·¯è³‡æ–™ã€‚")
            detailed_memo = self.llm.invoke(analyst_template.format(context=context, question=task))
            check_cancellation()
            yield f"data: {json.dumps({'type': 'status', 'message': f'æ­¥é©Ÿ 2.{i+1}/{len(sub_tasks)}: æ­£åœ¨ç²¾ç…‰ \"{task[:20]}...\" çš„ç ”ç©¶æˆæœ...'})}\n\n"
            summary = memo_summarizer_chain.invoke({"memo": detailed_memo})
            executive_summaries.append(f"### ç ”ç©¶ä¸»é¡Œ: {task}\n{summary}")
            logging.info(f"   -> å·²ç‚º '{task}' ç”ŸæˆåŸ·è¡Œæ‘˜è¦:\n{summary[:100]}...")
        final_context = "\n\n---\n\n".join(executive_summaries)
        
        check_cancellation()
        yield f"data: {json.dumps({'type': 'status', 'message': 'æ­¥é©Ÿ 3/4: æ­£åœ¨åŸºæ–¼ç ”ç©¶æˆæœç”Ÿæˆå ±å‘Šå¤§ç¶±...'})}\n\n"
        blueprint_gen_template = self.prompts.get("answer_blueprint_generator")
        if not blueprint_gen_template: raise ValueError("answer_blueprint_generator.txt æ¨¡æ¿æœªæ‰¾åˆ°ï¼")
        base_blueprint_prompt = blueprint_gen_template.format(context=final_context, question=question)
        blueprint_json = None
        for attempt in range(3):
            check_cancellation()
            prompt_to_use = base_blueprint_prompt if attempt == 0 else f"{base_blueprint_prompt}\n\n[ä¿®æ­£æŒ‡ä»¤]: ä¸Šæ¬¡è§£æå¤±æ•—ï¼Œè«‹åš´æ ¼åªè¼¸å‡ºåŒ…è£¹åœ¨ ```json ``` ä¸­çš„ä»£ç¢¼å¡Šï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€‚"
            blueprint_str = self.llm.invoke(prompt_to_use)
            logging.info(f"--- å¤§ç¶±ç”Ÿæˆå™¨å›æ‡‰ (å˜—è©¦ {attempt + 1}) ---\n{blueprint_str}\n--------------------")
            try:
                match = re.search(r"```json\s*(\{.*?\})\s*```", blueprint_str, re.DOTALL) or re.search(r"(\{.*\})", blueprint_str, re.DOTALL)
                if not match: raise json.JSONDecodeError("è¼¸å‡ºä¸­æ‰¾ä¸åˆ° JSON çµæ§‹ã€‚", blueprint_str, 0)
                blueprint_json = json.loads(match.group(1))
                logging.info("âœ… æˆåŠŸè§£æå›ç­”å¤§ç¶± JSONã€‚")
                break
            except json.JSONDecodeError as e:
                logging.warning(f"âŒ è§£æ JSON å¤±æ•— (å˜—è©¦ {attempt + 1}): {e}")
        if blueprint_json is None: raise ValueError("åœ¨ 3 æ¬¡å˜—è©¦å¾Œä»ç„¡æ³•è§£æ JSONã€‚")
        
        check_cancellation()
        yield f"data: {json.dumps({'type': 'status', 'message': 'æ­¥é©Ÿ 4/4: æ­£åœ¨æ’°å¯«æœ€çµ‚å ±å‘Š...'})}\n\n"
        final_writer_template = self.prompts.get("final_report_writer")
        if not final_writer_template: raise ValueError("é—œéµçš„ 'final_report_writer.txt' æ¨¡æ¿æœªæ‰¾åˆ°ï¼")
        final_report_prompt = final_writer_template.format(question=question, context=final_context, blueprint=json.dumps(blueprint_json, indent=2, ensure_ascii=False))
        stream_iterator = self.llm.stream(final_report_prompt)
        for chunk in stream_iterator:
            check_cancellation()
            content_chunk = chunk.content if hasattr(chunk, 'content') else chunk
            yield f"data: {json.dumps({'type': 'content', 'content': content_chunk})}\n\n"
        logging.info("å ±å‘Šæ­£æ–‡ä¸²æµç”Ÿæˆå®Œç•¢ï¼æ­£åœ¨é€²è¡Œæœ€çµ‚çµ„è£...")
        unique_urls = {doc.metadata.get("source", "").replace("ç¶²é ç€è¦½: ", "").strip() for doc in all_source_documents if doc.metadata.get("source", "").startswith("ç¶²é ç€è¦½: ")}
        reference_list_str = "\n".join([f"- {url}" for url in sorted(list(unique_urls)) if url])
        if reference_list_str:
            reference_section_str = f"\n\n---\n\n## åƒè€ƒæ–‡ç»\n{reference_list_str}"
            logging.info(f"âœ… å·²æå– {len(unique_urls)} æ¢ç¨ç‰¹çš„åƒè€ƒæ–‡ç»ã€‚")
            yield f"data: {json.dumps({'type': 'content', 'content': reference_section_str})}\n\n"
        yield f"data: {json.dumps({'type': 'status', 'message': 'å ±å‘Šç”Ÿæˆå®Œç•¢ï¼'})}\n\n"

    def ask(self, question: str, stream: bool = True, task_id: str = None, bypass_assessment: bool = False) -> Generator[str, None, None]:
        if not task_id:
            task_id = f"task_{int(time.time() * 1000)}_{os.urandom(4).hex()}"
        self.active_tasks[task_id] = {'is_cancelled': False}
        logging.info(f"\nğŸ¤” æ”¶åˆ°è«‹æ±‚ä¸¦è¨»å†Šä»»å‹™ ID: {task_id}ï¼Œå•é¡Œ: '{question}'")

        try:
            # --- è·¯ç”±æ±ºç­– ---
            if bypass_assessment:
                logging.info(f"ğŸ›‚ KAIZEN è·¯ç”±ï¼šæ”¶åˆ°ç¹éæŒ‡ä»¤ï¼Œç›´æ¥å•Ÿå‹•æ·±åº¦ç ”ç©¶å·¥ä½œæµã€‚")
                yield from self._handle_complex_project(question, stream, task_id)

            elif not self.use_web_search:
                logging.info("ğŸŒ KAIZEN è·¯ç”±ï¼šç¶²è·¯ç ”ç©¶å·²åœç”¨ï¼Œå¼·åˆ¶åŸ·è¡Œç›´æ¥å•ç­”ã€‚")
                yield from self._handle_complex_project(question, stream, task_id)

            elif re.search(r"https?://[\S]+", question):
                url = re.search(r"https?://[\S]+", question).group(0)
                logging.info(f"ğŸ”— KAIZEN è·¯ç”±ï¼šæª¢æ¸¬åˆ° URLï¼Œè‡ªå‹•å‡æ ¼ç‚ºæ·±åº¦ç ”ç©¶ä»»å‹™ã€‚")
                rewritten_question = f"è«‹ç‚ºæˆ‘æ’°å¯«ä¸€ä»½é—œæ–¼ä»¥ä¸‹ç¶²å€å…§å®¹çš„æ·±åº¦ç¸½çµå ±å‘Šï¼š{url}ã€‚å ±å‘Šéœ€è¦æç…‰å‡ºå…¶æ ¸å¿ƒè§€é»ã€é—œéµä¿¡æ¯å’Œä¸»è¦è«–æ“šã€‚"
                yield from self._handle_complex_project(rewritten_question, stream, task_id)
            
            else:
                assessor_template = self.prompts.get("query_assessor")
                if not assessor_template: raise ValueError("query_assessor.txt æ¨¡æ¿æœªæ‰¾åˆ°ï¼")
                assessor_chain = assessor_template | self.llm | StrOutputParser()
                assessment_str = assessor_chain.invoke({"question": question})
                match = re.search(r"\{.*\}", assessment_str, re.DOTALL)
                if not match: raise ValueError(f"å•é¡Œè©•ä¼°å¸«çš„å›æ‡‰ä¸­ä¸åŒ…å«æœ‰æ•ˆçš„ JSON çµæ§‹: {assessment_str}")
                assessment_json = json.loads(match.group(0))
                assessment = assessment_json.get("assessment")
                logging.info(f"ğŸ§ å•é¡Œè©•ä¼°å¸«çµè«–: {assessment}")

                if assessment == "specific_topic":
                    yield from self._handle_complex_project(question, stream, task_id)
                elif assessment == "broad_concept":
                    yield from self._handle_clarification(question, task_id)
                else:
                    logging.warning(f"âš ï¸ æœªçŸ¥çš„è©•ä¼°çµæœ: {assessment}ã€‚å°‡ç›´æ¥åŸ·è¡Œæ·±åº¦ç ”ç©¶ã€‚")
                    yield from self._handle_complex_project(question, stream, task_id)
            
            yield from self._send_done_signal()

        except GeneratorExit:
            logging.warning(f"ğŸ”Œ å®¢æˆ¶ç«¯åœ¨ä»»å‹™ {task_id} åŸ·è¡ŒæœŸé–“æ–·é–‹é€£æ¥ã€‚æ­£åœ¨éœé»˜æ¸…ç†ã€‚")
        except InterruptedError:
            logging.info(f"ğŸ›‘ ä»»å‹™ {task_id} å·²è¢«ä½¿ç”¨è€…æˆåŠŸä¸­æ­¢ã€‚")
            yield f"data: {json.dumps({'type': 'status', 'message': 'ä»»å‹™å·²å–æ¶ˆã€‚'})}\n\n"
            yield from self._send_done_signal()
        except Exception as e:
            logging.error(f"âŒ åœ¨ ASK æŒ‡æ®å®˜æ¨¡å¼ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)
            error_message = f'æŠ±æ­‰ï¼ŒåŸ·è¡Œæ™‚ç™¼ç”Ÿäº†ç„¡æ³•é æœŸçš„éŒ¯èª¤: {str(e)}'
            yield f"data: {json.dumps({'type': 'error', 'content': error_message})}\n\n"
            yield from self._send_done_signal()
        finally:
            self._perform_final_cleanup(task_id)