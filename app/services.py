import os
import json
import re
import requests
import wikipedia
from datetime import datetime
from bs4 import BeautifulSoup

# LangChain Imports
from langchain_ollama.llms import OllamaLLM
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain.prompts import PromptTemplate
from langchain_unstructured.document_loaders import UnstructuredLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain_core.output_parsers import StrOutputParser

def get_ollama_models(ollama_base_url="http://localhost:11434"):
    try:
        response = requests.get(f"{ollama_base_url}/api/tags")
        response.raise_for_status()
        return [model["name"] for model in response.json().get("models", [])]
    except Exception as e:
        print(f"âŒ ç²å– Ollama æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return []

class ConversationalRAG:
    def __init__(self, config: dict):
        self.config = config
        self.persist_directory = self.config['PERSIST_DIRECTORY']
        self.use_wikipedia = True
        self.use_history = True
        self.use_scraper = True
        self.ollama_base_url = self.config['OLLAMA_BASE_URL']
        self.history_summary_threshold = 2000

        # CORRECTED: Use spaces for indentation
        self.all_docs_for_bm25 = []
        self.bm25_retriever = None

        print("æ­£åœ¨åˆå§‹åŒ– Embedding æ¨¡å‹...")
        print(f"   -> ä½¿ç”¨æ¨¡å‹: {self.config['EMBEDDING_MODEL_NAME']}")
        print(f"   -> ä½¿ç”¨è¨­å‚™: {self.config['EMBEDDING_DEVICE']}")
        self.embeddings = HuggingFaceEmbeddings(
            model_name=self.config['EMBEDDING_MODEL_NAME'], 
            model_kwargs={'device': self.config['EMBEDDING_DEVICE'], 'trust_remote_code': True}
        )
        print("æ­£åœ¨åˆå§‹åŒ–/è¼‰å…¥å‘é‡è³‡æ–™åº«...")
        self.vector_db = Chroma(
            persist_directory=self.persist_directory, 
            embedding_function=self.embeddings
        )
        print("æ­£åœ¨è¨­å®š LLM...")
        self.llm = OllamaLLM(model=self.config['llm_model'], base_url=self.ollama_base_url)
        self.current_llm_model = self.config['llm_model']

        # --- Hybrid retriever initialization ---
        print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–æ··åˆæª¢ç´¢å™¨...")
        self.vector_retriever = self.vector_db.as_retriever(
            search_kwargs={'k': self.config['VECTOR_SEARCH_K']}
        )
        self.ensemble_retriever = self.vector_retriever # Start with vector retriever only
        # The full ensemble retriever will be built by update_ensemble_retriever
        self.update_ensemble_retriever(full_rebuild=True)
        
        # --- Prompt Templates åˆå§‹åŒ– ---
        self._init_prompts()
    
    def _init_prompts(self):
        self.main_prompt = PromptTemplate.from_template(
            """ä½ æ˜¯ä¸€ä½é ‚ç´šçš„ AI æŠ€è¡“æ–‡ä»¶åˆ†æå¸«ã€‚ä½ çš„ä»»å‹™æ˜¯åš´æ ¼åŸºæ–¼ä¸‹é¢æä¾›çš„ã€Œä¸Šä¸‹æ–‡è³‡æ–™ã€ï¼Œæ·±å…¥åœ°å›ç­”ã€Œä½¿ç”¨è€…ç•¶å‰å•é¡Œã€ã€‚

**åŸ·è¡Œæµç¨‹èˆ‡è¦å‰‡ï¼š**
1.  **æ·±å…¥åˆ†æä¸Šä¸‹æ–‡**ï¼šé¦–å…ˆï¼Œä»”ç´°é–±è®€ä¸¦å®Œå…¨ç†è§£ã€Œä¸Šä¸‹æ–‡è³‡æ–™ã€ä¸­èˆ‡ã€Œä½¿ç”¨è€…ç•¶å‰å•é¡Œã€ç›¸é—œçš„æ‰€æœ‰ç‰‡æ®µã€‚
2.  **ç»„ç»‡ä¸å›ç­”**ï¼š
    *   å¦‚æœã€Œä¸Šä¸‹æ–‡è³‡æ–™ã€ä¸­åŒ…å«å›ç­”å•é¡Œæ‰€éœ€çš„è³‡è¨Šï¼Œä½ çš„ä»»å‹™æ˜¯ã€æˆæ¬Šä¸¦é¼“å‹µã€‘ä½ ä½¿ç”¨è‡ªå·±çš„èªè¨€èƒ½åŠ›ï¼Œå°é€™äº›ç¢ç‰‡åŒ–çš„è³‡è¨Šé€²è¡Œ**ç¸½çµã€æ¨ç†ã€å’Œé‡æ–°çµ„ç¹”**ï¼Œä»¥å½¢æˆä¸€å€‹é€£è²«ã€æ¸…æ™°ã€å°ˆæ¥­çš„å›ç­”ã€‚
    *   å¦‚æœã€Œä¸Šä¸‹æ–‡è³‡æ–™ã€å®Œå…¨æ²’æœ‰æåŠå•é¡Œçš„æ ¸å¿ƒä¸»é¡Œï¼Œé‚£éº¼ä½ ã€å¿…é ˆã€‘åªå›ç­”ï¼šã€Œæ ¹æ“šæˆ‘æ‰€æŒæ¡çš„è³‡æ–™ï¼Œæˆ‘æ‰¾ä¸åˆ°é—œæ–¼ '{question}' çš„ç¢ºåˆ‡è³‡è¨Šã€‚ã€
3.  **å®šä¹‰â€œå¹»è§‰â€ç¦åŒº**ï¼šä½ ã€çµ•å°ç¦æ­¢ã€‘å¼•å…¥ä»»ä½•**åœ¨ã€Œä¸Šä¸‹æ–‡è³‡æ–™ã€ä¸­å®Œå…¨ä¸å­˜åœ¨çš„ã€æ†‘ç©ºæé€ çš„äº‹å¯¦æˆ–æ•¸æ“š**ã€‚
4.  **æ€è€ƒéç¨‹**ï¼šåœ¨æœ€ç»ˆç­”æ¡ˆå‰ï¼Œä½ å¯ä»¥ä½¿ç”¨ <think>...</think> æ¨™ç±¤æ¥å†™ä¸‹ä½ çš„åˆ†æã€æ¨ç†å’Œåˆ¤æ–­è¿‡ç¨‹ã€‚
5.  **ã€æ ¼å¼è¦æ±‚ã€‘**: `<think>` å€å¡Šå¿…é ˆä»¥ `</think>` çµæŸï¼Œä¸” `</think>` æ¨™ç±¤ä¹‹å¾Œã€å¿…é ˆç«‹åˆ»ã€‘å¼€å§‹ä½ çš„æœ€ç»ˆå›ç­”ï¼Œä¸­é—´ä¸èƒ½æœ‰ä»»ä½•æ¢è¡Œæˆ–å¤šä½™çš„ç©ºæ ¼ã€‚

---
[ä¸Šä¸‹æ–‡è³‡æ–™]:
{context}
---
[ä½¿ç”¨è€…ç•¶å‰å•é¡Œ]: {question}
ä½ çš„å›ç­”:"""
        )
        self.query_expansion_prompt = PromptTemplate.from_template(
            "ä½ æ˜¯ä¸€å€‹æŸ¥è©¢å„ªåŒ–åŠ©ç†ã€‚è«‹æ ¹æ“šä½¿ç”¨è€…æä¾›çš„åŸå§‹æŸ¥è©¢ï¼Œç”Ÿæˆä¸€å€‹æ›´å…·é«”ã€æ›´å¯èƒ½åœ¨æŠ€è¡“æ–‡ä»¶ä¸­æ‰¾åˆ°ç›¸é—œå…§å®¹çš„æ“´å……æŸ¥è©¢ã€‚è«‹åªè¿”å›æ“´å……å¾Œçš„æŸ¥è©¢ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡‹ã€‚\n\n[åŸå§‹æŸ¥è©¢]: {original_query}\n\n[æ“´å……æŸ¥è©¢]:"
        )
        self.router_prompt = PromptTemplate.from_template(
            """ä½ æ˜¯ä¸€å€‹ä»»å‹™è·¯ç”±å™¨ã€‚æ ¹æ“šä½¿ç”¨è€…çš„å•é¡Œï¼Œåˆ¤æ–·å®ƒå±¬æ–¼å“ªä¸€ç¨®é¡å‹ã€‚è«‹åªå›ç­”ä»¥ä¸‹åˆ†é¡ä¸­çš„ä¸€å€‹ï¼š'rag_query' æˆ– 'general_conversation'ã€‚

- å¦‚æœå•é¡Œéœ€è¦æŸ¥æ‰¾ã€è§£é‡‹ã€æ¯”è¼ƒæˆ–å®šç¾©ç‰¹å®šè³‡è¨Šï¼Œç‰¹åˆ¥æ˜¯æŠ€è¡“è¡“èªï¼Œè«‹å›ç­” 'rag_query'ã€‚
- å¦‚æœå•é¡Œæ˜¯ç°¡å–®çš„å•å€™ã€é–’èŠæˆ–èˆ‡çŸ¥è­˜åº«ç„¡é—œçš„å°è©±ï¼Œè«‹å›ç­” 'general_conversation'ã€‚

[ä½¿ç”¨è€…å•é¡Œ]: {question}
[åˆ†é¡]:"""
        )

    def set_llm_model(self, model_name: str):
        print(f"\nğŸ”„ æ­£åœ¨åˆ‡æ› LLM æ¨¡å‹è‡³: {model_name}")
        try:
            self.llm = OllamaLLM(model=model_name, base_url=self.ollama_base_url)
            self.llm.invoke("Hi", stop=["Hi"]) 
            self.current_llm_model = model_name
            print(f"âœ… LLM æ¨¡å‹æˆåŠŸåˆ‡æ›ç‚º: {self.current_llm_model}")
            return True
        except Exception as e:
            print(f"âŒ åˆ‡æ› LLM æ¨¡å‹å¤±æ•—: {e}")
            return False

    def set_history_retrieval(self, enabled: bool):
        print(f"ğŸ”„ å°‡æ­·å²å°è©±æª¢ç´¢è¨­å®šç‚º: {'å•Ÿç”¨' if enabled else 'åœç”¨'}")
        self.use_history = enabled
        return True

    def set_wikipedia_search(self, enabled: bool):
        print(f"ğŸ”„ å°‡ç¶­åŸºç™¾ç§‘æœå°‹è¨­å®šç‚º: {'å•Ÿç”¨' if enabled else 'åœç”¨'}")
        self.use_wikipedia = enabled
        return True
    
    def set_scraper_search(self, enabled: bool):
        print(f"ğŸ”„ å°‡ç¶²é çˆ¬èŸ²è¨­å®šç‚º: {'å•Ÿç”¨' if enabled else 'åœç”¨'}")
        self.use_scraper = enabled
        return True

    # --- MAJOR REFACTORING of update_ensemble_retriever ---
    def update_ensemble_retriever(self, new_docs: list = None, full_rebuild: bool = False):
        print("ğŸ”„ æ­£åœ¨æ›´æ–° Ensemble Retriever...")
        if full_rebuild:
            print("   -> åŸ·è¡Œå®Œæ•´é‡å»º...")
            try:
                all_data = self.vector_db.get(include=["documents", "metadatas"])
                documents_content = all_data['documents']
                metadatas = all_data['metadatas']
                self.all_docs_for_bm25 = [
                    Document(page_content=documents_content[i], metadata=metadatas[i])
                    for i in range(len(documents_content))
                    if documents_content[i] != "start"
                ]
                print(f"   -> å¾è³‡æ–™åº«è¼‰å…¥ {len(self.all_docs_for_bm25)} ä»½æ–‡ä»¶é€²è¡Œç´¢å¼•ã€‚")
            except Exception as e:
                print(f"âŒ å¾ DB ç²å–æ–‡æª”å¤±æ•—: {e}")
                self.all_docs_for_bm25 = []

        if new_docs:
            print(f"   -> åŸ·è¡Œå¢é‡æ›´æ–°ï¼Œæ–°å¢ {len(new_docs)} ä»½æ–‡ä»¶...")
            self.all_docs_for_bm25.extend(new_docs)

        if not self.all_docs_for_bm25:
            print("   -> è³‡æ–™åº«æ–‡æª”ä¸è¶³ï¼Œåƒ…ä½¿ç”¨å‘é‡æª¢ç´¢å™¨ã€‚")
            self.ensemble_retriever = self.vector_retriever
            self.bm25_retriever = None
            return

        try:
            print(f"   -> æ­£åœ¨åŸºæ–¼ {len(self.all_docs_for_bm25)} ä»½æ–‡ä»¶é‡å»º BM25 ç´¢å¼•...")
            self.bm25_retriever = BM25Retriever.from_documents(
                self.all_docs_for_bm25,
                k=self.config['BM25_SEARCH_K']
            )
            self.ensemble_retriever = EnsembleRetriever(
                retrievers=[self.bm25_retriever, self.vector_retriever],
                weights=self.config['ENSEMBLE_WEIGHTS']
            )
            print("âœ… æ··åˆæª¢ç´¢å™¨å·²æˆåŠŸæ›´æ–°ã€‚")
        except Exception as e:
            print(f"âŒ æ›´æ–°æ··åˆæª¢ç´¢å™¨å¤±æ•—: {e}ã€‚å°‡é€€å›è‡³åƒ…ä½¿ç”¨å‘é‡æª¢ç´¢å™¨ã€‚")
            self.ensemble_retriever = self.vector_retriever
            
    def add_document(self, file_path: str):
        print(f"ğŸ“„ æ­£åœ¨è™•ç†æ–°æ–‡ä»¶: {file_path}")
        try:
            loader = UnstructuredLoader(file_path)
            docs = loader.load()
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=self.config['CHUNK_SIZE'], 
                chunk_overlap=self.config['CHUNK_OVERLAP']
            )
            splits = text_splitter.split_documents(docs)
            self.vector_db.add_documents(splits)
            print(f"âœ… æ–‡ä»¶ '{os.path.basename(file_path)}' çš„åˆ†å¡Šå·²æˆåŠŸåŠ å…¥å‘é‡è³‡æ–™åº«ã€‚")
            self.update_ensemble_retriever(new_docs=splits)
        finally:
            # --- ç„¡è«– try å€å¡ŠæˆåŠŸæˆ–å¤±æ•—ï¼Œé€™è£¡éƒ½ä¿è­‰æœƒåŸ·è¡Œ ---
            print(f"ğŸ§¹ æ­£åœ¨æ¸…ç†æš«å­˜æ–‡ä»¶: {file_path}")
            if os.path.exists(file_path):
                os.remove(file_path)
            
    def _scrape_webpage_text(self, url: str):
        print(f"ğŸ•¸ï¸ (çˆ¬èŸ²) æ­£åœ¨å˜—è©¦çˆ¬å–ç¶²å€: {url}")
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            for element in soup(['script', 'style', 'nav', 'footer', 'aside']): element.decompose()
            text = '\n'.join(line.strip() for line in soup.get_text().splitlines() if line.strip())
            print(f"âœ… (çˆ¬èŸ²) æˆåŠŸç²å–ç¶²é æ–‡å­—ï¼Œé•·åº¦: {len(text)} å­—å…ƒã€‚")
            return text[:4000]
        except Exception as e:
            print(f"âŒ (çˆ¬èŸ²) å¤±æ•—: {e}")
            return f"ç„¡æ³•çˆ¬å–ç¶²å€ï¼ŒéŒ¯èª¤: {e}"
            
    def _search_wikipedia(self, query: str):
        print(f"ğŸ” (ç¶­åŸºç™¾ç§‘) æ­£åœ¨æœå°‹: '{query[:50].strip()}...'")
        try:
            wikipedia.set_lang("zh-tw")

            summary = wikipedia.summary(query, sentences=5, auto_suggest=False)
            print(f"âœ… (ç¶­åŸºç™¾ç§‘) æ‰¾åˆ°æ‘˜è¦ã€‚")
            return summary
        except wikipedia.exceptions.PageError:
            return "ç„¡ç›¸é—œè³‡æ–™"
        except Exception as e:
            print(f"âŒ (ç¶­åŸºç™¾ç§‘) æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return "æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤"

    def _get_rag_context(self, question: str, retrieval_query: str):
        all_source_docs = []
        context_parts = []

        # 1. ç¶²é çˆ¬èŸ²
        if self.use_scraper:
            url_match = re.search(r'https?://[\S]+', question)
            if url_match:
                url = url_match.group(0)
                web_content = self._scrape_webpage_text(url)
                if "ç„¡æ³•çˆ¬å–" not in web_content:
                    web_doc = Document(page_content=web_content, metadata={"source": f"ç¶²é : {url}"})
                    all_source_docs.append(web_doc)
                    context_parts.append(f"ä¾†æºï¼š{web_doc.metadata['source']}\nå…§å®¹ï¼š\n{web_doc.page_content}")
        if self.use_wikipedia:
            short_query = question
            wiki_content = self._search_wikipedia(short_query)
            if wiki_content not in ["ç„¡ç›¸é—œè³‡æ–™", "æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤"]:
                wiki_doc = Document(page_content=wiki_content, metadata={"source": "ç¶­åŸºç™¾ç§‘"})
                all_source_docs.append(wiki_doc)
                context_parts.append(f"ä¾†æºï¼š{wiki_doc.metadata['source']}\nå…§å®¹ï¼š\n{wiki_doc.page_content}")

        if self.use_history:
            print(f"ğŸ” (æ··åˆæª¢ç´¢) æ­£åœ¨æª¢ç´¢: '{retrieval_query[:80].replace('\n', ' ')}...'")
            db_docs = self.ensemble_retriever.get_relevant_documents(retrieval_query)
            db_docs = [doc for doc in db_docs if hasattr(doc, 'page_content') and doc.page_content != "start"]
            
            if db_docs:
                all_source_docs.extend(db_docs)
                context_from_docs = "\n---\n".join([f"ä¾†æºï¼š{doc.metadata.get('source', 'æœªçŸ¥')}\nå…§å®¹ï¼š\n{doc.page_content}" for doc in db_docs])
                context_parts.append(f"[ç›¸é—œè³‡æ–™åº«å…§å®¹]:\n{context_from_docs}")
        
        final_context = "\n\n".join(context_parts) if context_parts else "æ²’æœ‰å¯ç”¨çš„ä¸Šä¸‹æ–‡è³‡æ–™ã€‚"
        return final_context, all_source_docs

    def ask(self, question: str, stream: bool = True):
        print(f"\nğŸ¤” æ”¶åˆ°è«‹æ±‚ï¼Œå•é¡Œ: '{question}'")

        router_chain = self.router_prompt | self.llm | StrOutputParser()
        try:
            route = router_chain.invoke({"question": question}).strip().lower()
            print(f"ğŸš¦ è·¯ç”±å™¨æ±ºç­–: {route}")
        except Exception as e:
            print(f"ğŸš¦ è·¯ç”±å™¨æ±ºç­–å¤±æ•—: {e}, å°‡èµ°é è¨­ RAG è·¯å¾‘ã€‚")
            route = 'rag_query'

        if 'rag_query' in route:
            try:
                query_chain = self.query_expansion_prompt | self.llm | StrOutputParser()
                raw_expanded_output = query_chain.invoke({"original_query": question})
                match = re.search(r'\[æ“´å……æŸ¥è©¢\]:\s*([\s\S]*)', raw_expanded_output, re.IGNORECASE)
                if match:
                    expanded_query = match.group(1).strip()
                else:
                    expanded_query = re.sub(r'<think>.*?</think>', '', raw_expanded_output, flags=re.DOTALL).strip()
                if not expanded_query:
                    expanded_query = question
                retrieval_query = f"{question}\n{expanded_query}"
                print(f"ğŸ’¡ æ¸…ç†å¾Œçš„æ“´å±•æŸ¥è©¢: {expanded_query}")
            except Exception as e:
                print(f"ğŸ’¡ æŸ¥è©¢æ“´å±•å¤±æ•—: {e}, ä½¿ç”¨åŸå§‹æŸ¥è©¢ã€‚")
                retrieval_query = question

            final_context, all_source_docs = self._get_rag_context(question, retrieval_query)
            formatted_prompt = self.main_prompt.format(context=final_context, question=question)
            return self.stream_and_save(question, formatted_prompt, all_source_docs)
        else:
            print("ğŸ’¬ èµ°é€šç”¨å°è©±è·¯å¾‘...")
            return self.stream_and_save(question, question, [])

    def stream_and_save(self, question, prompt, source_documents):
        full_answer = ""
        try:
            if source_documents:
                source_data = [{"page_content": doc.page_content, "metadata": doc.metadata} for doc in source_documents]
                yield f"data: {json.dumps({'type': 'sources', 'data': source_data})}\n\n"

            for chunk in self.llm.stream(prompt):
                full_answer += chunk
                yield f"data: {json.dumps({'type': 'content', 'content': chunk})}\n\n"
            
            self.save_qa(question, full_answer)
        except Exception as e:
            error_msg = f"æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
            print(f"âŒ åœ¨ä¸²æµç”Ÿæˆéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            yield f"data: {json.dumps({'type': 'error', 'error': error_msg})}\n\n"
        
        finally:
            # Ensure the temp file is always removed
            #if os.path.exists(file_path):
            #    os.remove(file_path)
                yield f"data: [DONE]\n\n"

    def save_qa(self, question, answer):
        if not answer or not answer.strip():
            print("   -> åµæ¸¬åˆ°ç©ºå›ç­”ï¼Œè·³éå„²å­˜ã€‚")
            return
            
        qa_pair_content = f"å•é¡Œ: {question}\nå›ç­”: {answer}"
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        metadata = { "source": "conversation", "timestamp": current_time }
        new_doc = Document(page_content=qa_pair_content, metadata=metadata)
        self.vector_db.add_documents([new_doc])
        self.update_ensemble_retriever(new_docs=[new_doc])
        print("   -> å°è©±æ­·å²å„²å­˜ä¸¦åŒæ­¥è‡³æ··åˆç´¢å¼•å®Œç•¢ï¼")
