<div align="right">
  <b><a href="#-english-readme">English</a></b> | <b><a href="#-ä¸­æ–‡èªªæ˜">ä¸­æ–‡</a></b>
</div>

<a name="-english-readme"></a>

# Local LLM RAG Web UI - v2.0 (High-Performance Edition)

**A high-performance, scalable, 100% local, and private Retrieval-Augmented Generation (RAG) web interface powered by Ollama and LangChain.**

This project has evolved into an advanced solution for running a conversational AI that can process large documents, learn from its own memory, search Wikipedia, and browse the web, all on your local machine.

<img width="1807" height="1179" alt="image" src="https://github.com/user-attachments/assets/b0f520a6-6422-46d1-aebb-2a4e308ab83c" />


---

## ğŸŒŸ Key Features

*   **ğŸ’» 100% Local & Private:** Runs entirely on your machine using [Ollama](https://ollama.com/). Your models and data stay with you.
*   **ğŸš€ High-Performance Hybrid Search:** Combines keyword search (BM25) and semantic search (vectors) for superior retrieval accuracy. The index updates **incrementally and efficiently**, allowing for the ingestion of large documents without server slowdowns.
*   **ğŸ“„ Multi-Format Document Uploads:** Upload and process various document formats (PDF, DOCX, TXT, etc.) directly through the UI, powered by the `unstructured` library.
*   **ğŸ§  Scalable & Persistent Memory:** Uses [Chroma](https://www.trychroma.com/) as a local vector database. The system is architected to handle tens of thousands of document chunks gracefully.
*   **ğŸŒ Multi-Source RAG Engine:** The AI can augment its responses using:
    *   **Uploaded Documents:** Your private knowledge base.
    *   **Dialogue History:** Remembers past conversations for context.
    *   **Web Scraper:** Can read content from URLs provided in your questions.
    *   **Wikipedia Search:** Looks up information on Wikipedia to answer factual questions.
*   **ğŸ”„ Live Model Switching:** Change the underlying LLM model (any model in Ollama) directly from the UI without restarting the server.
*   **ğŸ› ï¸ Database & Index Management:** View, search, and delete individual records. A dedicated API endpoint allows for manually rebuilding the search index to ensure data consistency.

## ğŸ› ï¸ Technology Stack

*   **Backend:** Flask
*   **LLM Serving:** Ollama
*   **Orchestration:** LangChain (v0.2+ compatible)
*   **Document Processing:** Unstructured
*   **Search & Retrieval:**
    *   **Vector Database:** ChromaDB
    *   **Keyword Search:** In-memory BM25
    *   **Hybrid Search:** LangChain's EnsembleRetriever
*   **Embeddings:** HuggingFace Sentence Transformers

## âš™ï¸ Installation & Setup

### Prerequisites

*   Python 3.8+
*   Git
*   [Ollama](https://ollama.com/) installed and running.

### Step-by-Step Guide

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/barnetwang/rag-chat-memory.git
    cd rag-chat-memory
    ```

2.  **Install Python dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

3.  **Pull an Ollama model:**
    The default model is configured in `config.py`. Make sure you have it downloaded.
    ```bash
    # Example for the default model "deepseek-r1:8b"
    ollama pull deepseek-r1:8b
    ```

4.  **Launch the application:**
    ```bash
    python run.py
    ```

5.  **Open the Web UI:**
    Open your browser and navigate to **http://127.0.0.1:5000**.

## ğŸ”§ Configuration

You can customize the application's behavior by editing the `config.py` file:

*   **Core Settings:**
    *   `DEFAULT_MODEL`: The default LLM model to use on startup.
    *   `PERSIST_DIRECTORY`: Folder where the vector database is stored.
    *   `EMBEDDING_MODEL_NAME`: The sentence-transformer model for embeddings.
    *   `EMBEDDING_DEVICE`: The device to run embeddings on (`"cpu"`, `"cuda"`).

*   **RAG Tuning Parameters:**
    *   `CHUNK_SIZE` & `CHUNK_OVERLAP`: Control how documents are split.
    *   `VECTOR_SEARCH_K` & `BM25_SEARCH_K`: The number of results to retrieve from each search method.
    *   `ENSEMBLE_WEIGHTS`: The weights to assign to vector search vs. keyword search (e.g., `[0.5, 0.5]`).

## ğŸ“‚ Project Structure
```
/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py      # Initializes the Flask app
â”‚   â”œâ”€â”€ routes.py        # Contains all Flask routes and API endpoints
â”‚   â”œâ”€â”€ services.py      # Contains the core ConversationalRAG class
â”‚   â”œâ”€â”€ static/          # For CSS/JS files
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ index.html   # The single-page HTML for the frontend UI
â”œâ”€â”€ config.py            # All user-configurable settings
â”œâ”€â”€ run.py               # The main entry point to start the application
â”œâ”€â”€ requirements.txt     # Python package dependencies
â””â”€â”€ README.md            # This file
```

## ğŸ¤ Contributing

Contributions are welcome! If you have a suggestion or want to fix a bug, please follow the standard Fork and Pull Request workflow.

## âš–ï¸ License & Acknowledgements

This project is open-source and relies on several third-party packages. Please review their licenses before using this project for commercial purposes. Key dependencies include: LangChain, ChromaDB, Unstructured, HuggingFace, and Wikipedia. This project is for educational and research purposes. The user is solely responsible for compliance with all licenses.

---
<br>

<details>
<summary><b>ä¸­æ–‡èªªæ˜ (é»æ“Šå±•é–‹)</b></summary>

<a name="-ä¸­æ–‡èªªæ˜"></a>

# æœ¬åœ°ç«¯ LLM RAG æ•´åˆä»‹é¢ - v2.0 (é«˜æ€§èƒ½ç‰ˆ)

**ä¸€å€‹é«˜æ€§èƒ½ã€å¯æ“´å±•ã€100% æœ¬åœ°é‹è¡Œã€æ³¨é‡éš±ç§çš„æª¢ç´¢å¢å¼·ç”Ÿæˆ (RAG) ç¶²é æ‡‰ç”¨ç¨‹å¼ï¼Œç”± Ollama èˆ‡ LangChain é©…å‹•ã€‚**

æœ¬å°ˆæ¡ˆå·²é€²åŒ–ç‚ºä¸€å€‹å…ˆé€²çš„è§£æ±ºæ–¹æ¡ˆï¼Œè®“æ‚¨åœ¨è‡ªå·±çš„é›»è…¦ä¸Šé‹è¡Œä¸€å€‹å¼·å¤§çš„å°è©±å¼ AIã€‚å®ƒèƒ½è™•ç†å¤§å‹æŠ€è¡“æ–‡ä»¶ã€å¾è‡ªèº«è¨˜æ†¶ä¸­å­¸ç¿’ã€æœå°‹ç¶­åŸºç™¾ç§‘ä¸¦ç€è¦½ç¶²é ï¼Œç¢ºä¿å®Œå…¨çš„éš±ç§ã€‚

<img width="1807" height="1179" alt="image" src="https://github.com/user-attachments/assets/b0f520a6-6422-46d1-aebb-2a4e308ab83c" />

---

## ğŸŒŸ æ ¸å¿ƒåŠŸèƒ½

*   **ğŸ’» 100% æœ¬åœ°åŒ–èˆ‡éš±ç§:** å®Œå…¨åœ¨æ‚¨çš„æœ¬æ©Ÿä¸Šé€é [Ollama](https://ollama.com/) é‹è¡Œã€‚æ‚¨çš„æ¨¡å‹å’Œå°è©±è³‡æ–™çµ•ä¸æœƒé›¢é–‹æ‚¨çš„é›»è…¦ã€‚
*   **ğŸš€ é«˜æ€§èƒ½æ··åˆå¼æœå°‹:** çµåˆé—œéµè©æœå°‹ (BM25) èˆ‡èªç¾©æœå°‹ (å‘é‡)ï¼Œæä¾›å“è¶Šçš„æª¢ç´¢æº–ç¢ºåº¦ã€‚æœå°‹ç´¢å¼•æ¡ç”¨**é«˜æ•ˆçš„å¢é‡æ›´æ–°**æ©Ÿåˆ¶ï¼Œå…è¨±è¼‰å…¥å¤§å‹æ–‡ä»¶è€Œä¸æœƒæ‹–æ…¢ä¼ºæœå™¨ã€‚
*   **ğŸ“„ å¤šæ ¼å¼æ–‡ä»¶ä¸Šå‚³:** é€é UI ç›´æ¥ä¸Šå‚³ä¸¦è™•ç†å¤šç¨®æ–‡ä»¶æ ¼å¼ï¼ˆPDF, DOCX, TXT ç­‰ï¼‰ï¼Œç”± `unstructured` å‡½å¼åº«å¼·åŠ›é©…å‹•ã€‚
*   **ğŸ§  å¯æ“´å±•çš„æŒä¹…åŒ–è¨˜æ†¶:** ä½¿ç”¨ [Chroma](https://www.trychroma.com/) ä½œç‚ºæœ¬åœ°å‘é‡è³‡æ–™åº«ï¼Œæ•´é«”æ¶æ§‹ç¶“éå„ªåŒ–ï¼Œèƒ½å¤ è¼•é¬†è™•ç†æ•¸è¬ç´šåˆ¥çš„æ–‡ä»¶åˆ†å¡Šã€‚
*   **ğŸŒ å¤šæº RAG å¼•æ“:** AI å¯ä»¥å¾å¤šç¨®ä¾†æºç²å–è³‡è¨Šä»¥å¢å¼·å…¶å›ç­”ï¼š
    *   **ä¸Šå‚³çš„æ–‡ä»¶:** æ‚¨çš„ç§æœ‰çŸ¥è­˜åº«ã€‚
    *   **å°è©±æ­·å²:** è¨˜å¾—éå»çš„å°è©±ï¼Œç†è§£ä¸Šä¸‹æ–‡ã€‚
    *   **ç¶²é çˆ¬èŸ²:** èƒ½å¤ è®€å–æ‚¨åœ¨å•é¡Œä¸­æä¾›çš„ç¶²å€å…§å®¹ã€‚
    *   **ç¶­åŸºç™¾ç§‘æœå°‹:** æŸ¥æ‰¾ç¶­åŸºç™¾ç§‘ä¾†å›ç­”äº‹å¯¦æ€§å•é¡Œã€‚
*   **ğŸ”„ å³æ™‚æ¨¡å‹åˆ‡æ›:** ç›´æ¥å¾ç¶²é ä»‹é¢æ›´æ›åº•å±¤çš„ LLM æ¨¡å‹ï¼ˆä»»ä½•æ‚¨åœ¨ Ollama ä¸­å·²å®‰è£çš„æ¨¡å‹ï¼‰ï¼Œä¼ºæœå™¨ä¸ä¸­æ–·ã€‚
*   **ğŸ› ï¸ è¨˜æ†¶åº«èˆ‡ç´¢å¼•ç®¡ç†:** å¯ç€è¦½ã€æœå°‹å’Œåˆªé™¤å–®ç­†ç´€éŒ„ã€‚æä¾›å°ˆç”¨çš„ API ç«¯é»ï¼Œç”¨æ–¼æ‰‹å‹•å®Œæ•´é‡å»ºæœå°‹ç´¢å¼•ï¼Œç¢ºä¿è³‡æ–™ä¸€è‡´æ€§ã€‚

## ğŸ› ï¸ æŠ€è¡“æ£§

*   **å¾Œç«¯æ¡†æ¶:** Flask
*   **LLM æœå‹™:** Ollama
*   **AI æ¡†æ¶:** LangChain (å…¼å®¹ v0.2+)
*   **æ–‡ä»¶è™•ç†:** Unstructured
*   **æœå°‹èˆ‡æª¢ç´¢:**
    *   **å‘é‡è³‡æ–™åº«:** ChromaDB
    *   **é—œéµè©æœå°‹:** In-memory BM25
    *   **æ··åˆå¼æœå°‹:** LangChain EnsembleRetriever
*   **åµŒå…¥æ¨¡å‹:** HuggingFace Sentence Transformers

## âš™ï¸ å®‰è£èˆ‡å•Ÿå‹•

### å‰ç½®éœ€æ±‚

*   Python 3.8+
*   Git
*   [Ollama](https://ollama.com/) å·²å®‰è£ä¸¦æ­£åœ¨é‹è¡Œã€‚

### æ­¥é©ŸæŒ‡å—

1.  **å…‹éš†å°ˆæ¡ˆå€‰åº«ï¼š**
    ```bash
    git clone https://github.com/barnetwang/rag-chat-memory.git
    cd rag-chat-memory
    ```

2.  **å®‰è£ Python ä¾è³´å¥—ä»¶ï¼š**
    ```bash
    pip install -r requirements.txt
    ```

3.  **ä¸‹è¼‰ Ollama æ¨¡å‹ï¼š**
    å°ˆæ¡ˆçš„é è¨­æ¨¡å‹å¯åœ¨ `config.py` ä¸­è¨­å®šï¼Œè«‹ç¢ºä¿æ‚¨å·²ä¸‹è¼‰è©²æ¨¡å‹ã€‚
    ```bash
    # ä»¥é è¨­æ¨¡å‹ "deepseek-r1:8b" ç‚ºä¾‹
    ollama pull deepseek-r1:8b
    ```

4.  **å•Ÿå‹•æ‡‰ç”¨ç¨‹å¼ï¼š**
    ```bash
    python run.py
    ```

5.  **æ‰“é–‹ Web UIï¼š**
    æ‰“é–‹æ‚¨çš„ç€è¦½å™¨ï¼Œä¸¦è¨ªå• **http://127.0.0.1:5000**ã€‚

## ğŸ”§ å°ˆæ¡ˆè¨­å®š

æ‚¨å¯ä»¥é€éç·¨è¼¯ `config.py` æª”æ¡ˆä¾†è‡ªè¨‚æ‡‰ç”¨ç¨‹å¼çš„è¡Œç‚ºï¼š

*   **æ ¸å¿ƒè¨­å®š:**
    *   `DEFAULT_MODEL`: å•Ÿå‹•æ™‚é è¨­ä½¿ç”¨çš„ LLM æ¨¡å‹ã€‚
    *   `PERSIST_DIRECTORY`: å‘é‡è³‡æ–™åº«çš„å„²å­˜è³‡æ–™å¤¾ã€‚
    *   `EMBEDDING_MODEL_NAME`: ç”¨æ–¼ç”ŸæˆåµŒå…¥å‘é‡çš„å¥å­è½‰æ›å™¨æ¨¡å‹ã€‚
    *   `EMBEDDING_DEVICE`: é‹è¡ŒåµŒå…¥æ¨¡å‹çš„è¨­å‚™ (`"cpu"`, `"cuda"`)ã€‚

*   **RAG èª¿å„ªåƒæ•¸:**
    *   `CHUNK_SIZE` & `CHUNK_OVERLAP`: æ§åˆ¶æ–‡ä»¶åˆ†å‰²çš„æ–¹å¼ã€‚
    *   `VECTOR_SEARCH_K` & `BM25_SEARCH_K`: å¾æ¯ç¨®æœå°‹æ–¹æ³•ä¸­æª¢ç´¢çš„çµæœæ•¸é‡ã€‚
    *   `ENSEMBLE_WEIGHTS`: åˆ†é…çµ¦å‘é‡æœå°‹èˆ‡é—œéµè©æœå°‹çš„æ¬Šé‡ (ä¾‹å¦‚ `[0.5, 0.5]`)ã€‚

## ğŸ“‚ å°ˆæ¡ˆçµæ§‹
```
/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py      # åˆå§‹åŒ– Flask App
â”‚   â”œâ”€â”€ routes.py        # åŒ…å«æ‰€æœ‰ Flask è·¯ç”±å’Œ API ç«¯é»
â”‚   â”œâ”€â”€ services.py      # åŒ…å«æ ¸å¿ƒçš„ ConversationalRAG é¡åˆ¥
â”‚   â”œâ”€â”€ static/          # ç”¨æ–¼å­˜æ”¾ CSS/JS æª”æ¡ˆ
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ index.html   # å‰ç«¯ UI çš„å–®é  HTML æª”æ¡ˆ
â”œâ”€â”€ config.py            # æ‰€æœ‰ä½¿ç”¨è€…å¯é…ç½®çš„è¨­å®š
â”œâ”€â”€ run.py               # å•Ÿå‹•æ‡‰ç”¨ç¨‹å¼çš„ä¸»è¦é€²å…¥é»
â”œâ”€â”€ requirements.txt     # Python ä¾è³´å¥—ä»¶åˆ—è¡¨
â””â”€â”€ README.md            # æœ¬èªªæ˜æª”æ¡ˆ
```

## ğŸ¤ è²¢ç»æŒ‡å—

æ­¡è¿ä»»ä½•å½¢å¼çš„è²¢ç»ï¼å¦‚æœæ‚¨æœ‰å»ºè­°æˆ–æƒ³è¦ä¿®å¾©éŒ¯èª¤ï¼Œè«‹éµå¾ªæ¨™æº–çš„ Fork å’Œ Pull Request å·¥ä½œæµç¨‹ã€‚

## âš–ï¸ æˆæ¬Šèˆ‡è‡´è¬

æœ¬å°ˆæ¡ˆç‚ºé–‹æºå°ˆæ¡ˆï¼Œå…¶ä¾è³´çš„å¤šå€‹ç¬¬ä¸‰æ–¹å¥—ä»¶æ“æœ‰å„è‡ªçš„æˆæ¬Šæ¢æ¬¾ã€‚åœ¨å°‡æœ¬å°ˆæ¡ˆç”¨æ–¼å•†æ¥­ç›®çš„å‰ï¼Œè«‹å‹™å¿…è©³ç´°é–±è®€ä¸¦éµå®ˆã€‚ä¸»è¦ä¾è³´åŒ…æ‹¬ï¼šLangChainã€ChromaDBã€Unstructuredã€HuggingFace èˆ‡ Wikipediaã€‚æœ¬å°ˆæ¡ˆåƒ…ä¾›æ•™è‚²å’Œç ”ç©¶ç›®çš„ï¼Œä½¿ç”¨è€…éœ€è‡ªè¡Œè² è²¬ç¢ºä¿æ‰€æœ‰æˆæ¬Šå’Œæœå‹™æ¢æ¬¾çš„åˆè¦æ€§ã€‚

</details>
