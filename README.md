<div align="right">
  <b><a href="#-english-readme">English</a></b> | <b><a href="#-ä¸­æ–‡èªªæ˜">ä¸­æ–‡</a></b>
</div>

<a name="-english-readme"></a>

# Local LLM RAG Web UI - v2.5 (Intelligent Engine Edition)

**An intelligent, high-performance, 100% local, and private Retrieval-Augmented Generation (RAG) web interface powered by Ollama and LangChain.**

This project has evolved into a production-ready solution for running a conversational AI. It features an intelligent routing system, advanced hybrid search, and a robust document processing pipeline, all on your local machine.

![image](https://github.com/user-attachments/assets/06047960-b8c0-46f0-9447-c6810934a076)

---

## ğŸŒŸ Key Features

*   **ğŸ’» 100% Local & Private:** Runs entirely on your machine using [Ollama](https://ollama.com/). Your models and data stay with you.
*   **ğŸ§  Intelligent Query Routing:** Employs a multi-path RAG architecture. An LLM-powered router analyzes incoming questions to distinguish between complex RAG queries and general conversation, directing them to the optimal processing pipeline.
*   **ğŸš€ High-Performance Hybrid Search:** Combines keyword search (BM25) and semantic search (vectors) for superior retrieval accuracy on both technical terms and conceptual questions. The keyword index is efficiently updated in memory after new documents are added.
*   **ğŸ“„ Robust Multi-Format Document Ingestion:** Upload and process various document formats (PDF, DOCX, TXT, etc.). The pipeline uses specialized parsers like `PyMuPDF` for high-quality text extraction from complex layouts and includes an advanced pre-processing step to clean headers, footers, and other noise.
*   **ğŸ’¡ AI-Powered Query Expansion:** Automatically refines vague user queries into more specific, detailed search terms to significantly improve retrieval "hit rates".
*   **ğŸ“ Trustworthy Answers with Source Citation:** Every answer generated from the knowledge base is accompanied by clickable source links, allowing users to trace information back to the original document snippets.
*   **ğŸŒ Multi-Source RAG Engine:** Augments responses using:
    *   **Uploaded Documents:** Your private knowledge base.
    *   **Dialogue History:** Remembers past conversations for context.
    *   **Web Scraper & Wikipedia Search:** (Optional) Can be enabled for real-time information.
*   **ğŸ”„ Live Model Switching:** Change the underlying LLM (any model in Ollama) directly from the UI.
*   **ğŸ› ï¸ Database & Index Management:** View, search, and delete individual records. The system is architected to handle tens of thousands of document chunks gracefully with batch processing.

## ğŸ› ï¸ Technology Stack

*   **Backend:** Flask
*   **LLM Serving:** Ollama
*   **Orchestration:** LangChain (v0.2+)
*   **Document Processing:** PyMuPDF, Unstructured
*   **Search & Retrieval:**
    *   **Vector Database:** ChromaDB
    *   **Keyword Search:** rank_bm25
    *   **Hybrid Search:** LangChain EnsembleRetriever
*   **Embeddings:** HuggingFace Sentence Transformers (e.g., `nomic-embed-text`)

## âš™ï¸ Installation & Setup

### Prerequisites

*   Python 3.9+
*   Git
*   [Ollama](https://ollama.com/) installed and running.

### Step-by-Step Guide

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/barnetwang/rag-chat-memory.git
    cd rag-chat-memory
    ```

2.  **Install Python dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

3.  **Pull an Ollama model:**
    The default LLM and Embedding models are configured in `config.py`. Make sure you have them downloaded.
    ```bash
    # Example for the default LLM
    ollama pull gemma:2b

    # Example for the default Embedding model (if you choose to use Ollama for embeddings)
    ollama pull nomic-embed-text
    ```

4.  **Launch the application:**
    ```bash
    python run.py
    ```

5.  **Open the Web UI:**
    Open your browser and navigate to **http://127.0.0.1:5000**.

## ğŸ”§ Configuration

You can customize the application's behavior by editing the `config.py` file:

*   **Core Settings:**
    *   `DEFAULT_MODEL`: The default LLM model to use on startup.
    *   `EMBEDDING_MODEL_NAME`: The sentence-transformer model for embeddings from Hugging Face.
    *   `PERSIST_DIRECTORY`: Folder where the vector database is stored.

*   **RAG Tuning Parameters:**
    *   `VECTOR_SEARCH_K` & `BM25_SEARCH_K`: The number of results to retrieve from each search method.
    *   `ENSEMBLE_WEIGHTS`: The weights to assign to vector search vs. keyword search.

## ğŸ“‚ Project Structure
```
/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py      # Initializes the Flask app
â”‚   â”œâ”€â”€ routes.py        # Contains all Flask routes and API endpoints
â”‚   â”œâ”€â”€ services.py      # Contains the core ConversationalRAG class
â”‚   â”œâ”€â”€ static/          # For CSS/JS files
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ index.html   # The single-page HTML for the frontend UI
â”œâ”€â”€ config.py            # All user-configurable settings
â”œâ”€â”€ run.py               # The main entry point to start the application
â”œâ”€â”€ requirements.txt     # Python package dependencies
â””â”€â”€ README.md            # This file
```

## ğŸ¤ Contributing

Contributions are welcome! Please follow the standard Fork and Pull Request workflow.

## âš–ï¸ License & Acknowledgements

This project is open-source and relies on several third-party packages. Please review their licenses before using this project for commercial purposes.

---
<br>

<details>
<summary><b>ä¸­æ–‡èªªæ˜ (é»æ“Šå±•é–‹)</b></summary>

<a name="-ä¸­æ–‡èªªæ˜"></a>

# æœ¬åœ°ç«¯ LLM RAG æ•´åˆä»‹é¢ - v2.5 (æ™ºèƒ½å¼•æ“ç‰ˆ)

**ä¸€å€‹æ™ºèƒ½ã€é«˜æ€§èƒ½ã€100% æœ¬åœ°é‹è¡Œã€æ³¨é‡éš±ç§çš„æª¢ç´¢å¢å¼·ç”Ÿæˆ (RAG) ç¶²é æ‡‰ç”¨ç¨‹å¼ï¼Œç”± Ollama èˆ‡ LangChain é©…å‹•ã€‚**

æœ¬å°ˆæ¡ˆå·²é€²åŒ–ç‚ºä¸€å€‹ç”Ÿç”¢ç´šçš„è§£æ±ºæ–¹æ¡ˆï¼Œè®“æ‚¨åœ¨è‡ªå·±çš„é›»è…¦ä¸Šé‹è¡Œä¸€å€‹å¼·å¤§çš„å°è©±å¼ AIã€‚å®ƒå…·å‚™æ™ºèƒ½è·¯ç”±ç³»çµ±ã€å…ˆé€²çš„æ··åˆå¼æœå°‹ã€ä»¥åŠç©©å¥çš„æ–‡ä»¶è™•ç†æµç¨‹ï¼Œç¢ºä¿å®Œå…¨çš„éš±ç§ã€‚

![image](https://github.com/user-attachments/assets/06047960-b8c0-46f0-9447-c6810934a076)

---

## ğŸŒŸ æ ¸å¿ƒåŠŸèƒ½

*   **ğŸ’» 100% æœ¬åœ°åŒ–èˆ‡éš±ç§:** å®Œå…¨åœ¨æ‚¨çš„æœ¬æ©Ÿä¸Šé€é [Ollama](https://ollama.com/) é‹è¡Œã€‚
*   **ğŸ§  æ™ºèƒ½æŸ¥è©¢è·¯ç”±:** æ¡ç”¨å¤šè·¯å¾‘ RAG æ¶æ§‹ã€‚ç”± LLM é©…å‹•çš„è·¯ç”±å™¨æœƒåˆ†æå‚³å…¥çš„å•é¡Œï¼Œå€åˆ†éœ€è¦æ·±åº¦æª¢ç´¢çš„è¤‡é›œæŸ¥è©¢å’Œä¸€èˆ¬å°è©±ï¼Œä¸¦å°‡å®ƒå€‘å°å‘æœ€ä½³çš„è™•ç†æµç¨‹ã€‚
*   **ğŸš€ é«˜æ€§èƒ½æ··åˆå¼æœå°‹:** çµåˆé—œéµè©æœå°‹ (BM25) èˆ‡èªç¾©æœå°‹ (å‘é‡)ï¼Œåœ¨è™•ç†æŠ€è¡“è¡“èªå’Œæ¦‚å¿µæ€§å•é¡Œæ™‚éƒ½èƒ½é”åˆ°å“è¶Šçš„æª¢ç´¢æº–ç¢ºåº¦ã€‚é—œéµè©ç´¢å¼•åœ¨æ–°å¢æ–‡ä»¶å¾Œæœƒé«˜æ•ˆåœ°åœ¨è¨˜æ†¶é«”ä¸­é€²è¡Œæ›´æ–°ã€‚
*   **ğŸ“„ ç©©å¥çš„å¤šæ ¼å¼æ–‡ä»¶è™•ç†:** å¯ä¸Šå‚³ä¸¦è™•ç†å¤šç¨®æ–‡ä»¶æ ¼å¼ï¼ˆPDF, DOCX ç­‰ï¼‰ã€‚è™•ç†æµç¨‹ä½¿ç”¨å¦‚ `PyMuPDF` ç­‰å°ˆæ¥­è§£æå™¨ï¼Œä»¥å¾è¤‡é›œä½ˆå±€ä¸­é€²è¡Œé«˜è³ªé‡çš„æ–‡æœ¬æå–ï¼Œä¸¦åŒ…å«å…ˆé€²çš„é è™•ç†æ­¥é©Ÿä¾†æ¸…ç†é çœ‰ã€é è…³ç­‰å™ªéŸ³ã€‚
*   **ğŸ’¡ AI é©…å‹•çš„æŸ¥è©¢æ“´å±•:** è‡ªå‹•å°‡ä½¿ç”¨è€…æ¨¡ç³Šçš„æŸ¥è©¢ï¼Œç´°åŒ–ç‚ºæ›´å…·é«”ã€æ›´å°ˆæ¥­çš„æœç´¢è©ï¼Œé¡¯è‘—æå‡æª¢ç´¢â€œå‘½ä¸­ç‡â€ã€‚
*   **ğŸ“ å¯ä¿¡è³´çš„ç­”æ¡ˆèˆ‡ä¾†æºå¼•ç”¨:** æ¯å€‹å¾çŸ¥è­˜åº«ç”Ÿæˆçš„å›ç­”éƒ½æœƒé™„å¸¶å¯é»æ“Šçš„ä¾†æºé€£çµï¼Œè®“ä½¿ç”¨è€…èƒ½è¿½æº¯è³‡è¨Šè‡³åŸå§‹çš„æ–‡ä»¶ç‰‡æ®µã€‚
*   **ğŸŒ å¤šæº RAG å¼•æ“:** å¯æ•´åˆä¾†è‡ª**ä¸Šå‚³çš„æ–‡ä»¶**ã€**å°è©±æ­·å²**ã€**ç¶²é çˆ¬èŸ²**å’Œ**ç¶­åŸºç™¾ç§‘**çš„å¤šç¨®è³‡è¨Šæºã€‚
*   **ğŸ”„ å³æ™‚æ¨¡å‹åˆ‡æ›:** ç›´æ¥å¾ UI æ›´æ›åº•å±¤çš„ LLM æ¨¡å‹ã€‚
*   **ğŸ› ï¸ è¨˜æ†¶åº«èˆ‡ç´¢å¼•ç®¡ç†:** å¯ç€è¦½ã€æœå°‹å’Œåˆªé™¤å–®ç­†ç´€éŒ„ã€‚ç³»çµ±é€éåˆ†æ‰¹è™•ç†ï¼Œèƒ½å¤ è¼•é¬†æ‡‰å°åŒ…å«æ•¸è¬ç‰‡æ®µçš„å¤§å‹æ–‡ä»¶ã€‚

## ğŸ› ï¸ æŠ€è¡“æ£§

*   **å¾Œç«¯æ¡†æ¶:** Flask
*   **LLM æœå‹™:** Ollama
*   **AI æ¡†æ¶:** LangChain (v0.2+)
*   **æ–‡ä»¶è™•ç†:** PyMuPDF, Unstructured
*   **æœå°‹èˆ‡æª¢ç´¢:**
    *   **å‘é‡è³‡æ–™åº«:** ChromaDB
    *   **é—œéµè©æœå°‹:** rank_bm25
    *   **æ··åˆå¼æœå°‹:** LangChain EnsembleRetriever
*   **åµŒå…¥æ¨¡å‹:** HuggingFace Sentence Transformers (ä¾‹å¦‚ `nomic-embed-text`)

## âš™ï¸ å®‰è£èˆ‡å•Ÿå‹•

### å‰ç½®éœ€æ±‚

*   Python 3.9+
*   Git
*   [Ollama](https://ollama.com/) å·²å®‰è£ä¸¦æ­£åœ¨é‹è¡Œã€‚

### æ­¥é©ŸæŒ‡å—

1.  **å…‹éš†å°ˆæ¡ˆå€‰åº«ï¼š**
    ```bash
    git clone https://github.com/barnetwang/rag-chat-memory.git
    cd rag-chat-memory
    ```

2.  **å®‰è£ Python ä¾è³´å¥—ä»¶ï¼š**
    ```bash
    pip install -r requirements.txt
    ```

3.  **ä¸‹è¼‰ Ollama æ¨¡å‹ï¼š**
    å°ˆæ¡ˆçš„é è¨­æ¨¡å‹å¯åœ¨ `config.py` ä¸­è¨­å®šï¼Œè«‹ç¢ºä¿æ‚¨å·²ä¸‹è¼‰ã€‚
    ```bash
    # ä»¥ gemma:2b ç‚ºä¾‹
    ollama pull gemma:2b
    ```

4.  **å•Ÿå‹•æ‡‰ç”¨ç¨‹å¼ï¼š**
    ```bash
    python run.py
    ```

5.  **æ‰“é–‹ Web UIï¼š**
    æ‰“é–‹æ‚¨çš„ç€è¦½å™¨ï¼Œä¸¦è¨ªå• **http://127.0.0.1:5000**ã€‚

## ğŸ”§ å°ˆæ¡ˆè¨­å®š

æ‚¨å¯ä»¥é€éç·¨è¼¯ `config.py` æª”æ¡ˆä¾†è‡ªè¨‚æ‡‰ç”¨ç¨‹å¼çš„è¡Œç‚ºï¼š

*   **æ ¸å¿ƒè¨­å®š:**
    *   `DEFAULT_MODEL`: å•Ÿå‹•æ™‚é è¨­ä½¿ç”¨çš„ LLM æ¨¡å‹ã€‚
    *   `EMBEDDING_MODEL_NAME`: å¾ Hugging Face ä¸‹è¼‰çš„åµŒå…¥æ¨¡å‹ã€‚
    *   `PERSIST_DIRECTORY`: å‘é‡è³‡æ–™åº«çš„å„²å­˜è³‡æ–™å¤¾ã€‚

*   **RAG èª¿å„ªåƒæ•¸:**
    *   `VECTOR_SEARCH_K` & `BM25_SEARCH_K`: å¾æ¯ç¨®æœå°‹æ–¹æ³•ä¸­æª¢ç´¢çš„çµæœæ•¸é‡ã€‚
    *   `ENSEMBLE_WEIGHTS`: åˆ†é…çµ¦å‘é‡æœå°‹èˆ‡é—œéµè©æœå°‹çš„æ¬Šé‡ã€‚

## ğŸ“‚ å°ˆæ¡ˆçµæ§‹
```
/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py      # åˆå§‹åŒ– Flask App
â”‚   â”œâ”€â”€ routes.py        # åŒ…å«æ‰€æœ‰ Flask è·¯ç”±å’Œ API ç«¯é»
â”‚   â”œâ”€â”€ services.py      # åŒ…å«æ ¸å¿ƒçš„ ConversationalRAG é¡åˆ¥
â”‚   â”œâ”€â”€ static/          # ç”¨æ–¼å­˜æ”¾ CSS/JS æª”æ¡ˆ
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ index.html   # å‰ç«¯ UI çš„å–®é  HTML æª”æ¡ˆ
â”œâ”€â”€ config.py            # æ‰€æœ‰ä½¿ç”¨è€…å¯é…ç½®çš„è¨­å®š
â”œâ”€â”€ run.py               # å•Ÿå‹•æ‡‰ç”¨ç¨‹å¼çš„ä¸»è¦é€²å…¥é»
â”œâ”€â”€ requirements.txt     # Python ä¾è³´å¥—ä»¶åˆ—è¡¨
â””â”€â”€ README.md            # æœ¬èªªæ˜æª”æ¡ˆ
```

## ğŸ¤ è²¢ç»æŒ‡å—

æ­¡è¿ä»»ä½•å½¢å¼çš„è²¢ç»ï¼è«‹éµå¾ªæ¨™æº–çš„ Fork å’Œ Pull Request å·¥ä½œæµç¨‹ã€‚

## âš–ï¸ æˆæ¬Šèˆ‡è‡´è¬

æœ¬å°ˆæ¡ˆç‚ºé–‹æºå°ˆæ¡ˆï¼Œå…¶ä¾è³´çš„å¤šå€‹ç¬¬ä¸‰æ–¹å¥—ä»¶æ“æœ‰å„è‡ªçš„æˆæ¬Šæ¢æ¬¾ã€‚åœ¨å°‡æœ¬å°ˆæ¡ˆç”¨æ–¼å•†æ¥­ç›®çš„å‰ï¼Œè«‹å‹™å¿…è©³ç´°é–±è®€ä¸¦éµå®ˆã€‚

</details>
